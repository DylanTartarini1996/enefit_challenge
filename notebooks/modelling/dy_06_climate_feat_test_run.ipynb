{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from enefit_challenge.utils.dataset import load_enefit_training_data\n",
    "from enefit_challenge.models.lightgbm.lightgbm_forecaster import LightGBMForecaster\n",
    "from enefit_challenge.models.catboost.catboost_forecaster import CatBoostForecaster\n",
    "from enefit_challenge.models.xgboost.xgboost_forecaster import XGBoostForecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['county', 'is_business', 'product_type', 'target', 'is_consumption',\n",
       "       'datetime', 'data_block_id', 'row_id', 'prediction_unit_id', 'date',\n",
       "       'time', 'year', 'datediff_in_days', 'hour', 'hour_sine', 'hour_cosine',\n",
       "       'dayofweek', 'dayofweek_sine', 'dayofweek_cosine', 'week', 'week_sine',\n",
       "       'week_cosine', 'month', 'month_sine', 'month_cosine',\n",
       "       'target_1_days_ago', 'target_7_days_ago', 'eic_count',\n",
       "       'installed_capacity', 'euros_per_mwh', 'lowest_price_per_mwh',\n",
       "       'highest_price_per_mwh', 'temperature_min_f', 'temperature_mean_f',\n",
       "       'temperature_max_f', 'temperature_std_f', 'dewpoint_min_f',\n",
       "       'dewpoint_mean_f', 'dewpoint_max_f', 'dewpoint_std_f',\n",
       "       'cloudcover_high_min_f', 'cloudcover_high_mean_f',\n",
       "       'cloudcover_high_max_f', 'cloudcover_high_std_f',\n",
       "       'cloudcover_low_min_f', 'cloudcover_low_mean_f', 'cloudcover_low_max_f',\n",
       "       'cloudcover_low_std_f', 'cloudcover_mid_min_f', 'cloudcover_mid_mean_f',\n",
       "       'cloudcover_mid_max_f', 'cloudcover_mid_std_f',\n",
       "       'cloudcover_total_min_f', 'cloudcover_total_mean_f',\n",
       "       'cloudcover_total_max_f', 'cloudcover_total_std_f',\n",
       "       '10_metre_u_wind_component_min_f', '10_metre_u_wind_component_mean_f',\n",
       "       '10_metre_u_wind_component_max_f', '10_metre_u_wind_component_std_f',\n",
       "       '10_metre_v_wind_component_min_f', '10_metre_v_wind_component_mean_f',\n",
       "       '10_metre_v_wind_component_max_f', '10_metre_v_wind_component_std_f',\n",
       "       'direct_solar_radiation_min_f', 'direct_solar_radiation_mean_f',\n",
       "       'direct_solar_radiation_max_f', 'direct_solar_radiation_std_f',\n",
       "       'surface_solar_radiation_downwards_min_f',\n",
       "       'surface_solar_radiation_downwards_mean_f',\n",
       "       'surface_solar_radiation_downwards_max_f',\n",
       "       'surface_solar_radiation_downwards_std_f', 'snowfall_min_f',\n",
       "       'snowfall_mean_f', 'snowfall_max_f', 'snowfall_std_f',\n",
       "       'total_precipitation_min_f', 'total_precipitation_mean_f',\n",
       "       'total_precipitation_max_f', 'total_precipitation_std_f'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = load_enefit_training_data()\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_feature_columns = ['datetime', 'row_id','prediction_unit_id','date','time', 'data_block_id']\n",
    "cat_columns = ['county', 'product_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:53:24,031] A new study created in memory with name: catboost\n",
      "[I 2023-12-02 16:56:33,632] Trial 0 finished with value: 62.02602994620245 and parameters: {'n_estimators': 132, 'learning_rate': 0.25968501651282105, 'depth': 6, 'l2_leaf_reg': 0.0028108122006183516, 'model_size_reg': 0.00017240426024865184, 'colsample_bylevel': 0.6813047017599905, 'subsample': 0.7187936056313462}. Best is trial 0 with value: 62.02602994620245.\n",
      "[I 2023-12-02 17:00:23,660] Trial 1 finished with value: 73.97970100066219 and parameters: {'n_estimators': 184, 'learning_rate': 0.8051162438854074, 'depth': 4, 'l2_leaf_reg': 0.8265143168905288, 'model_size_reg': 0.001945132532521717, 'colsample_bylevel': 0.6112401049845391, 'subsample': 0.9627983191463305}. Best is trial 0 with value: 62.02602994620245.\n",
      "[I 2023-12-02 17:01:52,993] Trial 2 finished with value: 212.42796793438808 and parameters: {'n_estimators': 60, 'learning_rate': 0.014870229550172432, 'depth': 3, 'l2_leaf_reg': 2.1193293640885194, 'model_size_reg': 0.6047383484167921, 'colsample_bylevel': 0.8830109334221373, 'subsample': 0.989309171116382}. Best is trial 0 with value: 62.02602994620245.\n",
      "[I 2023-12-02 17:05:23,992] Trial 3 finished with value: 65.20274323912543 and parameters: {'n_estimators': 170, 'learning_rate': 0.08178593595271055, 'depth': 8, 'l2_leaf_reg': 1.5231555549417795e-07, 'model_size_reg': 0.025073225886269215, 'colsample_bylevel': 0.22901795866814179, 'subsample': 0.972334458524792}. Best is trial 0 with value: 62.02602994620245.\n",
      "[I 2023-12-02 17:07:34,124] Trial 4 finished with value: 69.66616155875647 and parameters: {'n_estimators': 128, 'learning_rate': 0.06608253686851315, 'depth': 4, 'l2_leaf_reg': 0.5525058672982482, 'model_size_reg': 0.0003643370370790407, 'colsample_bylevel': 0.6115905539817836, 'subsample': 0.5093949002181776}. Best is trial 0 with value: 62.02602994620245.\n",
      "[I 2023-12-02 17:10:22,441] Trial 5 finished with value: 60.792793907895124 and parameters: {'n_estimators': 143, 'learning_rate': 0.16238883252893319, 'depth': 6, 'l2_leaf_reg': 27.38298478755852, 'model_size_reg': 0.06579653011946042, 'colsample_bylevel': 0.42355711051640743, 'subsample': 0.7185159768996707}. Best is trial 5 with value: 60.792793907895124.\n",
      "[I 2023-12-02 17:12:54,093] Trial 6 finished with value: 148.44540586879623 and parameters: {'n_estimators': 155, 'learning_rate': 0.013155559915552555, 'depth': 7, 'l2_leaf_reg': 0.05086027407695684, 'model_size_reg': 1.270064013533414e-06, 'colsample_bylevel': 0.21603366788936798, 'subsample': 0.6577141754620919}. Best is trial 5 with value: 60.792793907895124.\n",
      "[I 2023-12-02 17:14:31,067] Trial 7 finished with value: 75.84952891627324 and parameters: {'n_estimators': 104, 'learning_rate': 0.13418118177281668, 'depth': 5, 'l2_leaf_reg': 76.51355496738435, 'model_size_reg': 1.0482095387452709e-07, 'colsample_bylevel': 0.2879890804853512, 'subsample': 0.5806547589424982}. Best is trial 5 with value: 60.792793907895124.\n",
      "[I 2023-12-02 17:16:40,995] Trial 8 finished with value: 112.03045338785684 and parameters: {'n_estimators': 148, 'learning_rate': 0.03169135303132934, 'depth': 5, 'l2_leaf_reg': 2.7813517732809443e-06, 'model_size_reg': 3.8877276777818826e-07, 'colsample_bylevel': 0.19933762704787464, 'subsample': 0.8281647947326367}. Best is trial 5 with value: 60.792793907895124.\n",
      "[I 2023-12-02 17:18:09,321] Trial 9 finished with value: 165.0727423839079 and parameters: {'n_estimators': 70, 'learning_rate': 0.024478543715901857, 'depth': 4, 'l2_leaf_reg': 1.6215572955007176, 'model_size_reg': 9.354331532348289e-08, 'colsample_bylevel': 0.8541504167489236, 'subsample': 0.5480492039469815}. Best is trial 5 with value: 60.792793907895124.\n",
      "[I 2023-12-02 17:21:38,311] Trial 10 finished with value: 63.5496666252036 and parameters: {'n_estimators': 102, 'learning_rate': 0.279980887875902, 'depth': 10, 'l2_leaf_reg': 0.00025830718382586126, 'model_size_reg': 31.102939772802433, 'colsample_bylevel': 0.45714362049547674, 'subsample': 0.8030336544410054}. Best is trial 5 with value: 60.792793907895124.\n",
      "[I 2023-12-02 17:24:27,675] Trial 11 finished with value: 60.24221165830968 and parameters: {'n_estimators': 127, 'learning_rate': 0.218468591988735, 'depth': 7, 'l2_leaf_reg': 0.003391078146248877, 'model_size_reg': 6.483250195523507e-05, 'colsample_bylevel': 0.4621411366469723, 'subsample': 0.7018429688729976}. Best is trial 11 with value: 60.24221165830968.\n",
      "[I 2023-12-02 17:26:41,378] Trial 12 finished with value: 63.348122872254294 and parameters: {'n_estimators': 99, 'learning_rate': 0.20119700570162974, 'depth': 7, 'l2_leaf_reg': 0.00014822883531201092, 'model_size_reg': 2.537765035316112e-05, 'colsample_bylevel': 0.4192537774915308, 'subsample': 0.6645573965668006}. Best is trial 11 with value: 60.24221165830968.\n",
      "[I 2023-12-02 17:32:28,137] Trial 13 finished with value: 64.35921471384022 and parameters: {'n_estimators': 195, 'learning_rate': 0.48647340949418083, 'depth': 10, 'l2_leaf_reg': 40.793890430147236, 'model_size_reg': 0.037791739180170175, 'colsample_bylevel': 0.40359589043030475, 'subsample': 0.7433284141610936}. Best is trial 11 with value: 60.24221165830968.\n",
      "[I 2023-12-02 17:35:42,664] Trial 14 finished with value: 61.52860682725929 and parameters: {'n_estimators': 148, 'learning_rate': 0.11366006740531968, 'depth': 6, 'l2_leaf_reg': 0.012208538697378727, 'model_size_reg': 1.0595566049260742e-05, 'colsample_bylevel': 0.5127224833972391, 'subsample': 0.6411284935445092}. Best is trial 11 with value: 60.24221165830968.\n",
      "[I 2023-12-02 17:37:57,870] Trial 15 finished with value: 85.93206372818362 and parameters: {'n_estimators': 114, 'learning_rate': 0.4465174737693204, 'depth': 8, 'l2_leaf_reg': 3.392891648040596e-05, 'model_size_reg': 0.005691878680884234, 'colsample_bylevel': 0.10879803078513534, 'subsample': 0.8004985124751873}. Best is trial 11 with value: 60.24221165830968.\n",
      "[I 2023-12-02 17:39:55,294] Trial 16 finished with value: 71.06790103284105 and parameters: {'n_estimators': 82, 'learning_rate': 0.1581252722444222, 'depth': 6, 'l2_leaf_reg': 0.02618754235066024, 'model_size_reg': 1.1472968184733827e-08, 'colsample_bylevel': 0.35220784427257673, 'subsample': 0.6101860517027012}. Best is trial 11 with value: 60.24221165830968.\n",
      "[I 2023-12-02 17:45:09,904] Trial 17 finished with value: 62.972138092454344 and parameters: {'n_estimators': 166, 'learning_rate': 0.0491677042380106, 'depth': 8, 'l2_leaf_reg': 0.002746876728998875, 'model_size_reg': 0.22769575707451428, 'colsample_bylevel': 0.5110311889014713, 'subsample': 0.6976949729105918}. Best is trial 11 with value: 60.24221165830968.\n",
      "[I 2023-12-02 17:48:12,964] Trial 18 finished with value: 66.20378096658906 and parameters: {'n_estimators': 135, 'learning_rate': 0.1017079094388861, 'depth': 5, 'l2_leaf_reg': 8.896358945498164e-06, 'model_size_reg': 5.4677039659784115e-05, 'colsample_bylevel': 0.34724307639250857, 'subsample': 0.7566864135818969}. Best is trial 11 with value: 60.24221165830968.\n",
      "[I 2023-12-02 17:52:11,934] Trial 19 finished with value: 58.91158519335306 and parameters: {'n_estimators': 117, 'learning_rate': 0.2024471182769706, 'depth': 9, 'l2_leaf_reg': 0.0008095752172608845, 'model_size_reg': 0.0005962299316285019, 'colsample_bylevel': 0.7178138417488098, 'subsample': 0.615924052369172}. Best is trial 19 with value: 58.91158519335306.\n",
      "[I 2023-12-02 17:56:13,404] Trial 20 finished with value: 58.12052090727813 and parameters: {'n_estimators': 116, 'learning_rate': 0.3080816709940246, 'depth': 9, 'l2_leaf_reg': 0.0003236227419553738, 'model_size_reg': 0.000469032391437577, 'colsample_bylevel': 0.7676112080941573, 'subsample': 0.6156442679096018}. Best is trial 20 with value: 58.12052090727813.\n",
      "[I 2023-12-02 18:00:14,483] Trial 21 finished with value: 61.09119028506461 and parameters: {'n_estimators': 119, 'learning_rate': 0.29296238286268705, 'depth': 9, 'l2_leaf_reg': 0.0004429948345803654, 'model_size_reg': 0.0007381353131102324, 'colsample_bylevel': 0.7617049887445675, 'subsample': 0.5963797013327863}. Best is trial 20 with value: 58.12052090727813.\n",
      "[I 2023-12-02 18:03:28,231] Trial 22 finished with value: 62.09097046679935 and parameters: {'n_estimators': 86, 'learning_rate': 0.20366239220175278, 'depth': 9, 'l2_leaf_reg': 0.004009900888288345, 'model_size_reg': 7.207486469075499e-06, 'colsample_bylevel': 0.9779241044323387, 'subsample': 0.6200273338444373}. Best is trial 20 with value: 58.12052090727813.\n",
      "[I 2023-12-02 18:08:36,394] Trial 23 finished with value: 64.60136056054944 and parameters: {'n_estimators': 113, 'learning_rate': 0.3912624472838741, 'depth': 10, 'l2_leaf_reg': 0.000718755775234635, 'model_size_reg': 0.00012353991817564992, 'colsample_bylevel': 0.7249172785919619, 'subsample': 0.5626509742900468}. Best is trial 20 with value: 58.12052090727813.\n",
      "[I 2023-12-02 18:11:35,445] Trial 24 finished with value: 76.79785667069335 and parameters: {'n_estimators': 87, 'learning_rate': 0.8370771300519916, 'depth': 9, 'l2_leaf_reg': 7.177975988248611e-05, 'model_size_reg': 0.0013160220887220317, 'colsample_bylevel': 0.602293655392769, 'subsample': 0.6696521483010588}. Best is trial 20 with value: 58.12052090727813.\n",
      "[I 2023-12-02 18:15:01,461] Trial 25 finished with value: 62.32940944618957 and parameters: {'n_estimators': 121, 'learning_rate': 0.12652972186464365, 'depth': 7, 'l2_leaf_reg': 0.0011755722412198045, 'model_size_reg': 7.221996730918004e-05, 'colsample_bylevel': 0.7845825078786004, 'subsample': 0.629545206272495}. Best is trial 20 with value: 58.12052090727813.\n",
      "[I 2023-12-02 18:18:00,960] Trial 26 finished with value: 60.519854703434106 and parameters: {'n_estimators': 108, 'learning_rate': 0.21960266590072958, 'depth': 8, 'l2_leaf_reg': 2.8045873914822622e-05, 'model_size_reg': 4.421727392858826e-06, 'colsample_bylevel': 0.6818767332569988, 'subsample': 0.5246129433929541}. Best is trial 20 with value: 58.12052090727813.\n",
      "[I 2023-12-02 18:20:56,798] Trial 27 finished with value: 63.782990582922174 and parameters: {'n_estimators': 96, 'learning_rate': 0.3666401851674744, 'depth': 9, 'l2_leaf_reg': 0.06171598015396052, 'model_size_reg': 0.00030370078823761885, 'colsample_bylevel': 0.5577035738622322, 'subsample': 0.5827842815167708}. Best is trial 20 with value: 58.12052090727813.\n",
      "[I 2023-12-02 18:23:08,617] Trial 28 finished with value: 67.61009807053783 and parameters: {'n_estimators': 50, 'learning_rate': 0.5700492766339699, 'depth': 10, 'l2_leaf_reg': 0.0002724494118915219, 'model_size_reg': 4.7002899038343875e-05, 'colsample_bylevel': 0.6808320878870007, 'subsample': 0.537728298543343}. Best is trial 20 with value: 58.12052090727813.\n",
      "[I 2023-12-02 18:26:51,353] Trial 29 finished with value: 60.39905249138051 and parameters: {'n_estimators': 132, 'learning_rate': 0.28128715648221025, 'depth': 7, 'l2_leaf_reg': 0.004921348775816275, 'model_size_reg': 0.00605065634669265, 'colsample_bylevel': 0.6689518762274875, 'subsample': 0.6801562494677463}. Best is trial 20 with value: 58.12052090727813.\n",
      "[I 2023-12-02 18:31:14,243] Trial 30 finished with value: 57.82667132414705 and parameters: {'n_estimators': 126, 'learning_rate': 0.17462983892274445, 'depth': 9, 'l2_leaf_reg': 0.0017625141495547563, 'model_size_reg': 0.00019759255327983773, 'colsample_bylevel': 0.8085214400493435, 'subsample': 0.6283699422004394}. Best is trial 30 with value: 57.82667132414705.\n",
      "[I 2023-12-02 18:36:17,513] Trial 31 finished with value: 57.75790087755026 and parameters: {'n_estimators': 125, 'learning_rate': 0.17243751956856906, 'depth': 9, 'l2_leaf_reg': 0.0013168282982537198, 'model_size_reg': 0.00021289371710794025, 'colsample_bylevel': 0.8142072476575577, 'subsample': 0.6224285840499202}. Best is trial 31 with value: 57.75790087755026.\n",
      "[I 2023-12-02 18:41:42,267] Trial 32 finished with value: 57.567468151109885 and parameters: {'n_estimators': 137, 'learning_rate': 0.15966128356246978, 'depth': 9, 'l2_leaf_reg': 0.0010998489458360916, 'model_size_reg': 0.00027912908081288956, 'colsample_bylevel': 0.8062750377856239, 'subsample': 0.6165179030876903}. Best is trial 32 with value: 57.567468151109885.\n",
      "[I 2023-12-02 18:49:51,460] Trial 33 finished with value: 57.24152350544451 and parameters: {'n_estimators': 140, 'learning_rate': 0.150288107221941, 'depth': 10, 'l2_leaf_reg': 8.139330491943157e-05, 'model_size_reg': 0.00026501704680696785, 'colsample_bylevel': 0.8240816615828058, 'subsample': 0.5666872599567936}. Best is trial 33 with value: 57.24152350544451.\n",
      "[I 2023-12-02 18:59:43,595] Trial 34 finished with value: 59.891455905345566 and parameters: {'n_estimators': 164, 'learning_rate': 0.08324868712492851, 'depth': 10, 'l2_leaf_reg': 7.758515940830932e-05, 'model_size_reg': 0.003453006410828331, 'colsample_bylevel': 0.8656023936952234, 'subsample': 0.5795250628107644}. Best is trial 33 with value: 57.24152350544451.\n"
     ]
    }
   ],
   "source": [
    "cbf = CatBoostForecaster()\n",
    "cbf.train_model(\n",
    "    train_df=df_train,\n",
    "    target_col=\"target\",\n",
    "    model_name=\"catboost_enefit\",\n",
    "    exclude_cols=not_feature_columns,\n",
    "    categorical_features=cat_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 15:59:06,288] A new study created in memory with name: lightgbm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 15:59:25,648] Trial 0 finished with value: 73.71418821555197 and parameters: {'n_estimators': 121, 'boosting_type': 'gbdt', 'eta': 0.11957168053633911, 'max_depth': 2, 'min_child_weight': 0.38333321561566636, 'colsample_bytree': 0.2738969595234697, 'subsample': 0.927727492754704, 'lambda': 7.155682161754866, 'alpha': 0.03417952912061012}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 15:59:45,664] Trial 1 finished with value: 98.57322394762025 and parameters: {'n_estimators': 179, 'boosting_type': 'dart', 'eta': 0.6769776337422189, 'max_depth': 1, 'min_child_weight': 0.002231090560744304, 'colsample_bytree': 0.10476552591086516, 'subsample': 0.8904582312865974, 'lambda': 1.2960656597279734, 'alpha': 3.020289640158666}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:00:03,635] Trial 2 finished with value: 87.6237244780208 and parameters: {'n_estimators': 242, 'boosting_type': 'gbdt', 'eta': 0.3496801474075936, 'max_depth': 1, 'min_child_weight': 0.3628140404024382, 'colsample_bytree': 0.1391083782005788, 'subsample': 0.9623735634056996, 'lambda': 0.12229065947034369, 'alpha': 0.045566719139214756}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:00:19,559] Trial 3 finished with value: 88.16746303987969 and parameters: {'n_estimators': 76, 'boosting_type': 'gbdt', 'eta': 0.13310833122227744, 'max_depth': 1, 'min_child_weight': 0.29548945587266856, 'colsample_bytree': 0.40935087469661635, 'subsample': 0.7668062477401033, 'lambda': 5.9565160187105715, 'alpha': 0.5336803302756714}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:00:41,416] Trial 4 finished with value: 242.43504428625567 and parameters: {'n_estimators': 89, 'boosting_type': 'dart', 'eta': 0.013155559915552555, 'max_depth': 4, 'min_child_weight': 0.4814503186400563, 'colsample_bytree': 0.1623239345911629, 'subsample': 0.5467397969664458, 'lambda': 0.01826894228153231, 'alpha': 0.028499883436971588}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:01:02,125] Trial 5 finished with value: 254.6439872225435 and parameters: {'n_estimators': 125, 'boosting_type': 'dart', 'eta': 0.015915358693657216, 'max_depth': 1, 'min_child_weight': 0.004418125737902547, 'colsample_bytree': 0.44989205684622635, 'subsample': 0.5959617329725612, 'lambda': 0.07332348382056167, 'alpha': 0.009499535455183799}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:01:17,464] Trial 6 finished with value: 257.37013051511025 and parameters: {'n_estimators': 64, 'boosting_type': 'dart', 'eta': 0.018762369507155215, 'max_depth': 1, 'min_child_weight': 0.02984699979785086, 'colsample_bytree': 0.662206180587672, 'subsample': 0.5348110858445883, 'lambda': 2.247913678489981, 'alpha': 0.002423224390060893}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:02:57,080] Trial 7 finished with value: 84.5991107564766 and parameters: {'n_estimators': 241, 'boosting_type': 'dart', 'eta': 0.1571148597057821, 'max_depth': 5, 'min_child_weight': 0.0014346671987806323, 'colsample_bytree': 0.19178161105195696, 'subsample': 0.5434414678644279, 'lambda': 0.015295398277813739, 'alpha': 0.002984770033451216}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:03:18,500] Trial 8 finished with value: 72.93784069853903 and parameters: {'n_estimators': 83, 'boosting_type': 'gbdt', 'eta': 0.2341630986802013, 'max_depth': 3, 'min_child_weight': 0.01152279840302872, 'colsample_bytree': 0.3336169083954025, 'subsample': 0.533640664000213, 'lambda': 0.20127321428268347, 'alpha': 5.214165242447227}. Best is trial 8 with value: 72.93784069853903.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:03:37,583] Trial 9 finished with value: 81.06194289451936 and parameters: {'n_estimators': 83, 'boosting_type': 'gbdt', 'eta': 0.2610340855190969, 'max_depth': 1, 'min_child_weight': 0.0054046235343238, 'colsample_bytree': 0.3859339058863831, 'subsample': 0.5070175342790754, 'lambda': 2.0689982192171295, 'alpha': 0.0010441957103804478}. Best is trial 8 with value: 72.93784069853903.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038102 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117967 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:03:58,698] Trial 10 finished with value: 67.10791507725772 and parameters: {'n_estimators': 51, 'boosting_type': 'gbdt', 'eta': 0.0545470197466688, 'max_depth': 8, 'min_child_weight': 6.267878593618888, 'colsample_bytree': 0.945515609149866, 'subsample': 0.6346624474453691, 'lambda': 0.3341080705306471, 'alpha': 9.48084849128898}. Best is trial 10 with value: 67.10791507725772.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.131384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:04:20,597] Trial 11 finished with value: 77.63159139966186 and parameters: {'n_estimators': 51, 'boosting_type': 'gbdt', 'eta': 0.04441475551169314, 'max_depth': 10, 'min_child_weight': 8.190089323058663, 'colsample_bytree': 0.8267355890798979, 'subsample': 0.6352820489447292, 'lambda': 0.23484188448733628, 'alpha': 7.891776929031949}. Best is trial 10 with value: 67.10791507725772.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:04:40,759] Trial 12 finished with value: 63.866171813828466 and parameters: {'n_estimators': 53, 'boosting_type': 'gbdt', 'eta': 0.05840872722237289, 'max_depth': 9, 'min_child_weight': 8.356746083342582, 'colsample_bytree': 0.9436570799603321, 'subsample': 0.6481649533517366, 'lambda': 0.0013616188957842135, 'alpha': 1.2971461679559289}. Best is trial 12 with value: 63.866171813828466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:04:59,984] Trial 13 finished with value: 66.36963611460509 and parameters: {'n_estimators': 50, 'boosting_type': 'gbdt', 'eta': 0.056827751246046346, 'max_depth': 10, 'min_child_weight': 9.47878547995462, 'colsample_bytree': 0.949070608285757, 'subsample': 0.6810372491253096, 'lambda': 0.001549938455049924, 'alpha': 0.9314720204285705}. Best is trial 12 with value: 63.866171813828466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:05:21,541] Trial 14 finished with value: 66.25380578817602 and parameters: {'n_estimators': 60, 'boosting_type': 'gbdt', 'eta': 0.05585884795696196, 'max_depth': 7, 'min_child_weight': 2.502157372304177, 'colsample_bytree': 0.6209612611540423, 'subsample': 0.7224866208392954, 'lambda': 0.0015440363518525759, 'alpha': 0.4188697223880712}. Best is trial 12 with value: 63.866171813828466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:05:45,167] Trial 15 finished with value: 85.44508520966731 and parameters: {'n_estimators': 62, 'boosting_type': 'gbdt', 'eta': 0.03477254734313827, 'max_depth': 6, 'min_child_weight': 2.54274229401728, 'colsample_bytree': 0.6063059818872701, 'subsample': 0.7515718987335854, 'lambda': 0.0011910979048357791, 'alpha': 0.3011586428951019}. Best is trial 12 with value: 63.866171813828466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028469 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:06:13,906] Trial 16 finished with value: 58.22504240533576 and parameters: {'n_estimators': 104, 'boosting_type': 'gbdt', 'eta': 0.07809853687079434, 'max_depth': 7, 'min_child_weight': 1.4931356529138706, 'colsample_bytree': 0.6210095923909388, 'subsample': 0.738204036523041, 'lambda': 0.0034798424169360503, 'alpha': 0.20944441627266694}. Best is trial 16 with value: 58.22504240533576.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.146172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:06:48,025] Trial 17 finished with value: 60.544727458003685 and parameters: {'n_estimators': 106, 'boosting_type': 'gbdt', 'eta': 0.08393905717296767, 'max_depth': 5, 'min_child_weight': 1.4281764499281888, 'colsample_bytree': 0.7373959768498988, 'subsample': 0.8106765540527653, 'lambda': 0.004575862849554225, 'alpha': 0.12256203967901345}. Best is trial 16 with value: 58.22504240533576.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:07:16,024] Trial 18 finished with value: 62.91727561699176 and parameters: {'n_estimators': 105, 'boosting_type': 'gbdt', 'eta': 0.09665959649135596, 'max_depth': 5, 'min_child_weight': 0.07062985958192339, 'colsample_bytree': 0.4964142243275759, 'subsample': 0.8147110605200386, 'lambda': 0.005802947889603856, 'alpha': 0.14347812786996073}. Best is trial 16 with value: 58.22504240533576.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:07:41,624] Trial 19 finished with value: 64.91670455839802 and parameters: {'n_estimators': 145, 'boosting_type': 'gbdt', 'eta': 0.08764215980697401, 'max_depth': 3, 'min_child_weight': 1.1809009933574914, 'colsample_bytree': 0.686038316280143, 'subsample': 0.8343042856666202, 'lambda': 0.0059918731204437825, 'alpha': 0.11973116630505895}. Best is trial 16 with value: 58.22504240533576.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:08:08,148] Trial 20 finished with value: 74.60841027116122 and parameters: {'n_estimators': 98, 'boosting_type': 'gbdt', 'eta': 0.029657869992124507, 'max_depth': 6, 'min_child_weight': 0.13459471948456192, 'colsample_bytree': 0.5162313551997192, 'subsample': 0.8090790278485139, 'lambda': 0.03135027464048997, 'alpha': 0.14210510656599024}. Best is trial 16 with value: 58.22504240533576.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:08:32,938] Trial 21 finished with value: 63.33943641161803 and parameters: {'n_estimators': 110, 'boosting_type': 'gbdt', 'eta': 0.0808128213903702, 'max_depth': 4, 'min_child_weight': 0.08265630843931593, 'colsample_bytree': 0.5152219095396298, 'subsample': 0.8486861509888382, 'lambda': 0.005254994605183772, 'alpha': 0.12934257284587672}. Best is trial 16 with value: 58.22504240533576.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:08:58,153] Trial 22 finished with value: 55.75473748224589 and parameters: {'n_estimators': 101, 'boosting_type': 'gbdt', 'eta': 0.10405131501324516, 'max_depth': 7, 'min_child_weight': 1.060186145157725, 'colsample_bytree': 0.7240169498057725, 'subsample': 0.7870159536858642, 'lambda': 0.005004758450904133, 'alpha': 0.24442966556231824}. Best is trial 22 with value: 55.75473748224589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:09:24,062] Trial 23 finished with value: 57.23197491283162 and parameters: {'n_estimators': 99, 'boosting_type': 'gbdt', 'eta': 0.1674109876259231, 'max_depth': 7, 'min_child_weight': 1.4402922703599568, 'colsample_bytree': 0.766508520789315, 'subsample': 0.7665283568307872, 'lambda': 0.0035470798320363624, 'alpha': 0.2697879508210151}. Best is trial 22 with value: 55.75473748224589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:09:50,384] Trial 24 finished with value: 56.423281892073454 and parameters: {'n_estimators': 94, 'boosting_type': 'gbdt', 'eta': 0.1734043170318076, 'max_depth': 8, 'min_child_weight': 0.8380724317104743, 'colsample_bytree': 0.7928324530394005, 'subsample': 0.7585058456176184, 'lambda': 0.002966968289918267, 'alpha': 0.39087911578177675}. Best is trial 22 with value: 55.75473748224589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066815 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:10:15,707] Trial 25 finished with value: 56.432159958429004 and parameters: {'n_estimators': 93, 'boosting_type': 'gbdt', 'eta': 0.1875332142742202, 'max_depth': 8, 'min_child_weight': 0.7137966116680442, 'colsample_bytree': 0.7763608450700255, 'subsample': 0.6986501678087413, 'lambda': 0.012412779329209814, 'alpha': 0.7351039806834055}. Best is trial 22 with value: 55.75473748224589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:10:45,435] Trial 26 finished with value: 57.52594665054057 and parameters: {'n_estimators': 74, 'boosting_type': 'dart', 'eta': 0.19979840566498636, 'max_depth': 10, 'min_child_weight': 0.597375695395375, 'colsample_bytree': 0.8251088783616067, 'subsample': 0.6960844710637246, 'lambda': 0.013249260777934811, 'alpha': 0.8511608640344666}. Best is trial 22 with value: 55.75473748224589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:11:10,378] Trial 27 finished with value: 59.03690580827442 and parameters: {'n_estimators': 92, 'boosting_type': 'gbdt', 'eta': 0.33457921786612027, 'max_depth': 10, 'min_child_weight': 0.19441491065013872, 'colsample_bytree': 0.9752054474131928, 'subsample': 0.7004880667322546, 'lambda': 0.008913715374823673, 'alpha': 1.9388885953173647}. Best is trial 22 with value: 55.75473748224589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:11:38,644] Trial 28 finished with value: 59.257801058000155 and parameters: {'n_estimators': 121, 'boosting_type': 'gbdt', 'eta': 0.12146380549994852, 'max_depth': 8, 'min_child_weight': 0.7151752469258897, 'colsample_bytree': 0.5582433912930167, 'subsample': 0.7749587334794703, 'lambda': 0.030840622715388913, 'alpha': 0.5590428018548037}. Best is trial 22 with value: 55.75473748224589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:12:01,694] Trial 29 finished with value: 64.01734765517794 and parameters: {'n_estimators': 122, 'boosting_type': 'gbdt', 'eta': 0.1197027468307015, 'max_depth': 2, 'min_child_weight': 0.22969103122067405, 'colsample_bytree': 0.7209140448578413, 'subsample': 0.8675570682631203, 'lambda': 0.00304883440011573, 'alpha': 1.5816840492488475}. Best is trial 22 with value: 55.75473748224589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:12:27,787] Trial 30 finished with value: 58.44780959564688 and parameters: {'n_estimators': 91, 'boosting_type': 'gbdt', 'eta': 0.17529001228941288, 'max_depth': 8, 'min_child_weight': 0.6476463040637449, 'colsample_bytree': 0.8066537804561819, 'subsample': 0.9132580451433504, 'lambda': 0.0023383582394301975, 'alpha': 0.6279320038162143}. Best is trial 22 with value: 55.75473748224589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127573 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:12:57,690] Trial 31 finished with value: 58.57044750241426 and parameters: {'n_estimators': 99, 'boosting_type': 'gbdt', 'eta': 0.1539086927949475, 'max_depth': 7, 'min_child_weight': 3.6243817391330486, 'colsample_bytree': 0.7721725955561604, 'subsample': 0.7832265673628934, 'lambda': 0.0027943866007857443, 'alpha': 0.2915212900229139}. Best is trial 22 with value: 55.75473748224589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:13:24,824] Trial 32 finished with value: 61.05802101373479 and parameters: {'n_estimators': 79, 'boosting_type': 'gbdt', 'eta': 0.1957227372096206, 'max_depth': 6, 'min_child_weight': 1.1326908602572487, 'colsample_bytree': 0.6003843800273215, 'subsample': 0.7355080097407616, 'lambda': 0.007846336410537717, 'alpha': 0.3736751517038198}. Best is trial 22 with value: 55.75473748224589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085755 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107488 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:13:50,945] Trial 33 finished with value: 61.16263826766946 and parameters: {'n_estimators': 92, 'boosting_type': 'gbdt', 'eta': 0.4447855585374992, 'max_depth': 8, 'min_child_weight': 0.3884755808196064, 'colsample_bytree': 0.7174186651199487, 'subsample': 0.875789380786156, 'lambda': 0.002412896612178041, 'alpha': 0.06250832463825362}. Best is trial 22 with value: 55.75473748224589.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139765 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:14:15,696] Trial 34 finished with value: 55.27262226861408 and parameters: {'n_estimators': 72, 'boosting_type': 'gbdt', 'eta': 0.12717586219313182, 'max_depth': 6, 'min_child_weight': 3.536955067056772, 'colsample_bytree': 0.8555744327733105, 'subsample': 0.7795540273991449, 'lambda': 0.011515707342111974, 'alpha': 2.759747495918391}. Best is trial 34 with value: 55.27262226861408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:14:43,424] Trial 35 finished with value: 65.32968401951462 and parameters: {'n_estimators': 74, 'boosting_type': 'dart', 'eta': 0.11488479646630106, 'max_depth': 4, 'min_child_weight': 5.140675874193728, 'colsample_bytree': 0.8639179544812875, 'subsample': 0.9955814249028115, 'lambda': 0.010448902870675854, 'alpha': 3.0591816178195685}. Best is trial 34 with value: 55.27262226861408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:15:06,735] Trial 36 finished with value: 65.94066081962849 and parameters: {'n_estimators': 84, 'boosting_type': 'gbdt', 'eta': 0.2889992033108478, 'max_depth': 6, 'min_child_weight': 3.5549365635973245, 'colsample_bytree': 0.6743626883120442, 'subsample': 0.7187249523590604, 'lambda': 0.022517659396384875, 'alpha': 2.5524948151565447}. Best is trial 34 with value: 55.27262226861408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:15:34,015] Trial 37 finished with value: 57.603868235978815 and parameters: {'n_estimators': 70, 'boosting_type': 'dart', 'eta': 0.4457226627170305, 'max_depth': 9, 'min_child_weight': 0.34827987281121614, 'colsample_bytree': 0.8593630149655906, 'subsample': 0.7886820429531295, 'lambda': 0.009280747326153837, 'alpha': 0.8178290583624209}. Best is trial 34 with value: 55.27262226861408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050080 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:15:57,542] Trial 38 finished with value: 63.56598436225742 and parameters: {'n_estimators': 72, 'boosting_type': 'gbdt', 'eta': 0.22326474771932978, 'max_depth': 5, 'min_child_weight': 0.8550938769768698, 'colsample_bytree': 0.4493906986721295, 'subsample': 0.7516082688636614, 'lambda': 0.0451470940751218, 'alpha': 1.2045780542360063}. Best is trial 34 with value: 55.27262226861408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:16:22,971] Trial 39 finished with value: 71.56047237363838 and parameters: {'n_estimators': 68, 'boosting_type': 'dart', 'eta': 0.12093216892373272, 'max_depth': 4, 'min_child_weight': 2.211793956664167, 'colsample_bytree': 0.5674088791524308, 'subsample': 0.9094281030360833, 'lambda': 0.015455588786472872, 'alpha': 0.5848935995799145}. Best is trial 34 with value: 55.27262226861408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:16:46,850] Trial 40 finished with value: 54.358635636252764 and parameters: {'n_estimators': 80, 'boosting_type': 'gbdt', 'eta': 0.15874067824761268, 'max_depth': 9, 'min_child_weight': 0.5055231028827599, 'colsample_bytree': 0.9894648911367524, 'subsample': 0.6751406102948082, 'lambda': 0.02064343714848684, 'alpha': 0.05355494123328612}. Best is trial 40 with value: 54.358635636252764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:17:09,536] Trial 41 finished with value: 55.061994288007355 and parameters: {'n_estimators': 80, 'boosting_type': 'gbdt', 'eta': 0.15188079604236762, 'max_depth': 9, 'min_child_weight': 0.7785448867085508, 'colsample_bytree': 0.904677839762714, 'subsample': 0.6740725868798644, 'lambda': 0.01792107928497161, 'alpha': 0.041118161353069047}. Best is trial 40 with value: 54.358635636252764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:17:34,265] Trial 42 finished with value: 54.6087533127123 and parameters: {'n_estimators': 81, 'boosting_type': 'gbdt', 'eta': 0.14491914972496087, 'max_depth': 9, 'min_child_weight': 0.4494669712362375, 'colsample_bytree': 0.9889223307811628, 'subsample': 0.6722163146455281, 'lambda': 0.018574874734526663, 'alpha': 0.030794678288352372}. Best is trial 40 with value: 54.358635636252764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:17:58,080] Trial 43 finished with value: 54.78938197789541 and parameters: {'n_estimators': 80, 'boosting_type': 'gbdt', 'eta': 0.14220466245950106, 'max_depth': 9, 'min_child_weight': 0.4206679436084697, 'colsample_bytree': 0.9914167958632719, 'subsample': 0.6754630994840576, 'lambda': 0.05512938500524502, 'alpha': 0.027719142183302722}. Best is trial 40 with value: 54.358635636252764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:18:20,334] Trial 44 finished with value: 54.42126456442556 and parameters: {'n_estimators': 79, 'boosting_type': 'gbdt', 'eta': 0.14917089781337398, 'max_depth': 9, 'min_child_weight': 0.45034771394398676, 'colsample_bytree': 0.99292096738745, 'subsample': 0.6713765426929659, 'lambda': 0.0598875251224776, 'alpha': 0.0256145616206205}. Best is trial 40 with value: 54.358635636252764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:18:42,981] Trial 45 finished with value: 54.42510479336101 and parameters: {'n_estimators': 82, 'boosting_type': 'gbdt', 'eta': 0.14572592577316348, 'max_depth': 9, 'min_child_weight': 0.29024891395962604, 'colsample_bytree': 0.9941103277786558, 'subsample': 0.6642996336902819, 'lambda': 0.06645042096225717, 'alpha': 0.024238729105002244}. Best is trial 40 with value: 54.358635636252764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.259144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:19:08,181] Trial 46 finished with value: 56.05034981314455 and parameters: {'n_estimators': 86, 'boosting_type': 'gbdt', 'eta': 0.23292680063680743, 'max_depth': 10, 'min_child_weight': 0.4550233164345572, 'colsample_bytree': 0.978008805732886, 'subsample': 0.6097545213226598, 'lambda': 0.072619563250558, 'alpha': 0.022776668646849487}. Best is trial 40 with value: 54.358635636252764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:19:34,426] Trial 47 finished with value: 54.04847253015826 and parameters: {'n_estimators': 78, 'boosting_type': 'gbdt', 'eta': 0.13778816750014036, 'max_depth': 9, 'min_child_weight': 0.267206298053086, 'colsample_bytree': 0.9956888314127073, 'subsample': 0.6660659349298703, 'lambda': 0.10136322223027791, 'alpha': 0.019902158077648162}. Best is trial 47 with value: 54.04847253015826.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:20:01,756] Trial 48 finished with value: 58.22801842832246 and parameters: {'n_estimators': 66, 'boosting_type': 'dart', 'eta': 0.28510415343016265, 'max_depth': 10, 'min_child_weight': 0.27063757581440034, 'colsample_bytree': 0.8879566112451709, 'subsample': 0.6630833078591065, 'lambda': 0.14336139675313295, 'alpha': 0.0166951168245499}. Best is trial 47 with value: 54.04847253015826.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:20:28,126] Trial 49 finished with value: 54.63870436542488 and parameters: {'n_estimators': 85, 'boosting_type': 'gbdt', 'eta': 0.1408819313433906, 'max_depth': 9, 'min_child_weight': 0.17379154362578655, 'colsample_bytree': 0.8940155560515672, 'subsample': 0.6218808340107743, 'lambda': 0.0911347595186104, 'alpha': 0.06859705989009261}. Best is trial 47 with value: 54.04847253015826.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:20:50,628] Trial 50 finished with value: 55.01508171025284 and parameters: {'n_estimators': 60, 'boosting_type': 'gbdt', 'eta': 0.10095573810999799, 'max_depth': 7, 'min_child_weight': 0.2748873123889194, 'colsample_bytree': 0.9843822839579315, 'subsample': 0.6536021307676141, 'lambda': 0.11630731367057266, 'alpha': 0.009013074811047891}. Best is trial 47 with value: 54.04847253015826.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:21:14,824] Trial 51 finished with value: 54.318955758286044 and parameters: {'n_estimators': 77, 'boosting_type': 'gbdt', 'eta': 0.13994015263434925, 'max_depth': 10, 'min_child_weight': 0.12820531319237513, 'colsample_bytree': 0.8847599534416374, 'subsample': 0.6257403873227031, 'lambda': 0.08929462294056116, 'alpha': 0.06643262418189545}. Best is trial 47 with value: 54.04847253015826.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:21:36,985] Trial 52 finished with value: 57.316186699011894 and parameters: {'n_estimators': 75, 'boosting_type': 'gbdt', 'eta': 0.21746789742522013, 'max_depth': 9, 'min_child_weight': 0.13835471597635216, 'colsample_bytree': 0.9848741048784451, 'subsample': 0.6402820595721774, 'lambda': 0.054994555077500035, 'alpha': 0.057754230373068025}. Best is trial 47 with value: 54.04847253015826.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:21:59,559] Trial 53 finished with value: 55.476561870084076 and parameters: {'n_estimators': 79, 'boosting_type': 'gbdt', 'eta': 0.15298427205779896, 'max_depth': 10, 'min_child_weight': 0.2973760706388116, 'colsample_bytree': 0.8781120188368644, 'subsample': 0.5951390125927474, 'lambda': 0.15443007125300395, 'alpha': 0.036093621066531405}. Best is trial 47 with value: 54.04847253015826.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:22:25,881] Trial 54 finished with value: 54.34789091354327 and parameters: {'n_estimators': 87, 'boosting_type': 'gbdt', 'eta': 0.07377395667872641, 'max_depth': 8, 'min_child_weight': 0.06660172881120907, 'colsample_bytree': 0.9995187721477126, 'subsample': 0.6513914967704161, 'lambda': 0.08122432513862858, 'alpha': 0.016784717169083467}. Best is trial 47 with value: 54.04847253015826.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:22:48,020] Trial 55 finished with value: 60.8278384988055 and parameters: {'n_estimators': 67, 'boosting_type': 'gbdt', 'eta': 0.07055011462020358, 'max_depth': 8, 'min_child_weight': 0.054153997211262274, 'colsample_bytree': 0.6737062494054005, 'subsample': 0.6523449084481113, 'lambda': 0.2553147630524773, 'alpha': 0.0131866777708058}. Best is trial 47 with value: 54.04847253015826.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043640 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.146725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:23:12,428] Trial 56 finished with value: 56.90715497660119 and parameters: {'n_estimators': 76, 'boosting_type': 'gbdt', 'eta': 0.06446694777181844, 'max_depth': 7, 'min_child_weight': 0.10516855637138073, 'colsample_bytree': 0.8188775084902044, 'subsample': 0.6256624810969166, 'lambda': 0.09774569540914795, 'alpha': 0.04822647272914893}. Best is trial 47 with value: 54.04847253015826.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:23:37,799] Trial 57 finished with value: 53.874138645535886 and parameters: {'n_estimators': 89, 'boosting_type': 'gbdt', 'eta': 0.09821894201184904, 'max_depth': 10, 'min_child_weight': 0.04558987744396178, 'colsample_bytree': 0.9180763975851031, 'subsample': 0.5930983584456269, 'lambda': 0.38674313602165433, 'alpha': 0.07898932963767887}. Best is trial 57 with value: 53.874138645535886.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037360 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:24:05,374] Trial 58 finished with value: 53.469472329280734 and parameters: {'n_estimators': 88, 'boosting_type': 'gbdt', 'eta': 0.0962975933744993, 'max_depth': 10, 'min_child_weight': 0.03726199243861717, 'colsample_bytree': 0.9048368344244411, 'subsample': 0.5766825965443921, 'lambda': 0.46094590980922295, 'alpha': 0.07907370984688356}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033865 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:24:30,604] Trial 59 finished with value: 54.2698373206158 and parameters: {'n_estimators': 88, 'boosting_type': 'gbdt', 'eta': 0.09106847566149226, 'max_depth': 10, 'min_child_weight': 0.03185719823325862, 'colsample_bytree': 0.7549893558259958, 'subsample': 0.5898866477234926, 'lambda': 0.3348070543581155, 'alpha': 0.07148914351464149}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:24:55,217] Trial 60 finished with value: 54.03592581771655 and parameters: {'n_estimators': 88, 'boosting_type': 'gbdt', 'eta': 0.09131919717693457, 'max_depth': 10, 'min_child_weight': 0.03125771955663457, 'colsample_bytree': 0.7581667252128003, 'subsample': 0.5803425635532641, 'lambda': 0.5217596409550109, 'alpha': 0.08671826046541443}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:25:19,986] Trial 61 finished with value: 54.494445744781366 and parameters: {'n_estimators': 95, 'boosting_type': 'gbdt', 'eta': 0.07500235418359684, 'max_depth': 10, 'min_child_weight': 0.03217510392788004, 'colsample_bytree': 0.7567455951023628, 'subsample': 0.5870380548210856, 'lambda': 0.45245580610886016, 'alpha': 0.08821689168323199}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:25:44,741] Trial 62 finished with value: 53.99156690362969 and parameters: {'n_estimators': 87, 'boosting_type': 'gbdt', 'eta': 0.08545057458280349, 'max_depth': 10, 'min_child_weight': 0.04504612489142011, 'colsample_bytree': 0.9030516208724557, 'subsample': 0.5743831690674428, 'lambda': 0.5178693395714082, 'alpha': 0.09862681312546388}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:26:11,227] Trial 63 finished with value: 57.618835486005175 and parameters: {'n_estimators': 88, 'boosting_type': 'gbdt', 'eta': 0.09091566516102381, 'max_depth': 10, 'min_child_weight': 0.01950315661718275, 'colsample_bytree': 0.6500898851977986, 'subsample': 0.5761354708269745, 'lambda': 0.5045010519470309, 'alpha': 0.17178898898488365}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104360 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:26:38,426] Trial 64 finished with value: 54.328534445406895 and parameters: {'n_estimators': 111, 'boosting_type': 'gbdt', 'eta': 0.10339910894308256, 'max_depth': 10, 'min_child_weight': 0.04778995983298188, 'colsample_bytree': 0.8109264674575278, 'subsample': 0.5638655181716659, 'lambda': 0.47825151255353276, 'alpha': 0.11076911554399743}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085969 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:27:06,318] Trial 65 finished with value: 54.00163847178044 and parameters: {'n_estimators': 90, 'boosting_type': 'gbdt', 'eta': 0.08458580275795934, 'max_depth': 8, 'min_child_weight': 0.03370961453572234, 'colsample_bytree': 0.9114067851214925, 'subsample': 0.6060761089264862, 'lambda': 0.778714975196722, 'alpha': 0.08113071796614732}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:27:32,414] Trial 66 finished with value: 54.991124013810385 and parameters: {'n_estimators': 88, 'boosting_type': 'gbdt', 'eta': 0.0859876435476952, 'max_depth': 8, 'min_child_weight': 0.022737532268051978, 'colsample_bytree': 0.7201809404394092, 'subsample': 0.5579652703787682, 'lambda': 0.8655761903571847, 'alpha': 0.08734459872688362}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:28:13,505] Trial 67 finished with value: 92.47245575104712 and parameters: {'n_estimators': 96, 'boosting_type': 'dart', 'eta': 0.04890080003705504, 'max_depth': 7, 'min_child_weight': 0.038007339599590746, 'colsample_bytree': 0.7690979371051041, 'subsample': 0.6070212680072069, 'lambda': 0.8271811068963087, 'alpha': 0.08648391302786242}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:28:41,936] Trial 68 finished with value: 54.17829711891118 and parameters: {'n_estimators': 102, 'boosting_type': 'gbdt', 'eta': 0.06529732928317904, 'max_depth': 8, 'min_child_weight': 0.013743336276307848, 'colsample_bytree': 0.914200263051443, 'subsample': 0.5825530506362812, 'lambda': 0.31342910346837194, 'alpha': 0.16298262585880186}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:29:09,729] Trial 69 finished with value: 54.0293208046665 and parameters: {'n_estimators': 102, 'boosting_type': 'gbdt', 'eta': 0.06635026087282937, 'max_depth': 8, 'min_child_weight': 0.013472051403452968, 'colsample_bytree': 0.9026723591831345, 'subsample': 0.53906379383354, 'lambda': 0.18515916661039977, 'alpha': 0.16423462243851772}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:29:36,643] Trial 70 finished with value: 55.85358024997132 and parameters: {'n_estimators': 106, 'boosting_type': 'gbdt', 'eta': 0.062101381788120004, 'max_depth': 6, 'min_child_weight': 0.04595039450565471, 'colsample_bytree': 0.8288121005567458, 'subsample': 0.518198468002445, 'lambda': 0.19693178471281333, 'alpha': 0.19885809338526256}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:30:04,931] Trial 71 finished with value: 53.83954722356489 and parameters: {'n_estimators': 99, 'boosting_type': 'gbdt', 'eta': 0.07008355282145182, 'max_depth': 8, 'min_child_weight': 0.013296802590640729, 'colsample_bytree': 0.9039601989227174, 'subsample': 0.5779090805438188, 'lambda': 0.2915463030691883, 'alpha': 0.17891394004773653}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:30:32,707] Trial 72 finished with value: 54.21966909739049 and parameters: {'n_estimators': 98, 'boosting_type': 'gbdt', 'eta': 0.08117563457281364, 'max_depth': 7, 'min_child_weight': 0.009429072306737446, 'colsample_bytree': 0.9184431500287925, 'subsample': 0.5452614282656645, 'lambda': 0.1921629028399201, 'alpha': 0.12180040205620961}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:30:57,815] Trial 73 finished with value: 54.69510751786415 and parameters: {'n_estimators': 91, 'boosting_type': 'gbdt', 'eta': 0.10542093768160558, 'max_depth': 8, 'min_child_weight': 0.022413661299404128, 'colsample_bytree': 0.915080180234581, 'subsample': 0.5732241076706832, 'lambda': 0.7125585615157398, 'alpha': 0.20876448080154184}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046183 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:31:24,973] Trial 74 finished with value: 56.583498823805115 and parameters: {'n_estimators': 96, 'boosting_type': 'gbdt', 'eta': 0.052274553386914724, 'max_depth': 8, 'min_child_weight': 0.06969860221708056, 'colsample_bytree': 0.8255792789627013, 'subsample': 0.5626304317458178, 'lambda': 1.2821595290106373, 'alpha': 0.10151924563202247}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.160564 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:31:56,967] Trial 75 finished with value: 54.06580495686236 and parameters: {'n_estimators': 116, 'boosting_type': 'gbdt', 'eta': 0.06990330906247308, 'max_depth': 9, 'min_child_weight': 0.01656225866083459, 'colsample_bytree': 0.8456652836397562, 'subsample': 0.5347010597209099, 'lambda': 0.252747605347565, 'alpha': 0.14462603326123225}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083493 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:32:24,523] Trial 76 finished with value: 58.009711534267716 and parameters: {'n_estimators': 84, 'boosting_type': 'gbdt', 'eta': 0.058724389633188344, 'max_depth': 7, 'min_child_weight': 0.026839755952688837, 'colsample_bytree': 0.7107201705634184, 'subsample': 0.5559796319805873, 'lambda': 0.3266278328739541, 'alpha': 0.04347068077467399}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:32:56,430] Trial 77 finished with value: 54.55461572935326 and parameters: {'n_estimators': 103, 'boosting_type': 'gbdt', 'eta': 0.11011787891812808, 'max_depth': 9, 'min_child_weight': 0.010859540683939947, 'colsample_bytree': 0.797848052323202, 'subsample': 0.6029446567867235, 'lambda': 0.6017806822127505, 'alpha': 0.24923742674142965}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124578 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:33:17,556] Trial 78 finished with value: 68.12963483516391 and parameters: {'n_estimators': 91, 'boosting_type': 'gbdt', 'eta': 0.09345852990773794, 'max_depth': 1, 'min_child_weight': 0.04066478775055654, 'colsample_bytree': 0.9128557388114048, 'subsample': 0.5726063673215313, 'lambda': 0.4220954031838861, 'alpha': 0.10260080783224117}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:33:44,930] Trial 79 finished with value: 95.91781768196563 and parameters: {'n_estimators': 106, 'boosting_type': 'dart', 'eta': 0.08361750031700978, 'max_depth': 2, 'min_child_weight': 0.05617559366246018, 'colsample_bytree': 0.6626459787781006, 'subsample': 0.5489056434549181, 'lambda': 0.5916019962990368, 'alpha': 0.18204732735514428}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:34:09,432] Trial 80 finished with value: 60.120115711711286 and parameters: {'n_estimators': 99, 'boosting_type': 'gbdt', 'eta': 0.12481151747697523, 'max_depth': 3, 'min_child_weight': 0.02721500050064576, 'colsample_bytree': 0.931501116614589, 'subsample': 0.5816684984165189, 'lambda': 0.39085012074728726, 'alpha': 0.14149255921241866}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:34:38,756] Trial 81 finished with value: 54.064400614539615 and parameters: {'n_estimators': 114, 'boosting_type': 'gbdt', 'eta': 0.06972190005909507, 'max_depth': 9, 'min_child_weight': 0.01613168760004705, 'colsample_bytree': 0.8454445761424888, 'subsample': 0.5427174636656523, 'lambda': 0.24698309661154663, 'alpha': 0.07637469144574925}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106765 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:35:06,618] Trial 82 finished with value: 54.217982792315986 and parameters: {'n_estimators': 93, 'boosting_type': 'gbdt', 'eta': 0.07983661580500903, 'max_depth': 10, 'min_child_weight': 0.01758191729998464, 'colsample_bytree': 0.8590750554058255, 'subsample': 0.5956636085832863, 'lambda': 0.25827746845010857, 'alpha': 0.08043513253478562}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036680 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:35:31,248] Trial 83 finished with value: 55.532320245807284 and parameters: {'n_estimators': 83, 'boosting_type': 'gbdt', 'eta': 0.09681835085479387, 'max_depth': 6, 'min_child_weight': 0.03676832207121311, 'colsample_bytree': 0.7823353957236455, 'subsample': 0.5689966293587683, 'lambda': 0.203570600507868, 'alpha': 0.05135926517511787}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:35:59,359] Trial 84 finished with value: 53.51575085771815 and parameters: {'n_estimators': 115, 'boosting_type': 'gbdt', 'eta': 0.06785369522707409, 'max_depth': 9, 'min_child_weight': 0.007842142913632772, 'colsample_bytree': 0.9304435691934724, 'subsample': 0.525022481533504, 'lambda': 0.37228715350399777, 'alpha': 0.11270597452284274}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:36:26,934] Trial 85 finished with value: 54.64468520501705 and parameters: {'n_estimators': 109, 'boosting_type': 'gbdt', 'eta': 0.1128961157071399, 'max_depth': 8, 'min_child_weight': 0.006895509152865959, 'colsample_bytree': 0.9232362683879043, 'subsample': 0.5180170225373993, 'lambda': 0.3600882854477812, 'alpha': 0.32371143535111124}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:36:56,577] Trial 86 finished with value: 56.879235672445525 and parameters: {'n_estimators': 102, 'boosting_type': 'gbdt', 'eta': 0.047371288426558994, 'max_depth': 9, 'min_child_weight': 0.024632362168825986, 'colsample_bytree': 0.7518034640554682, 'subsample': 0.5807772353126063, 'lambda': 1.182013522442613, 'alpha': 0.12811033077729833}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:37:26,664] Trial 87 finished with value: 59.4340794417472 and parameters: {'n_estimators': 89, 'boosting_type': 'gbdt', 'eta': 0.056670618949121024, 'max_depth': 10, 'min_child_weight': 0.012877631355539529, 'colsample_bytree': 0.7040845432746429, 'subsample': 0.5543565673250207, 'lambda': 0.5627000892069352, 'alpha': 0.23325145949641218}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093266 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:37:55,479] Trial 88 finished with value: 54.514661058293484 and parameters: {'n_estimators': 85, 'boosting_type': 'gbdt', 'eta': 0.07712012528778739, 'max_depth': 7, 'min_child_weight': 0.07878919041566672, 'colsample_bytree': 0.9353087152552004, 'subsample': 0.5655772818926783, 'lambda': 0.9753812346460464, 'alpha': 0.1021619855694074}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:38:24,754] Trial 89 finished with value: 60.37137669334658 and parameters: {'n_estimators': 95, 'boosting_type': 'gbdt', 'eta': 0.0397501626329159, 'max_depth': 5, 'min_child_weight': 0.004313813422386219, 'colsample_bytree': 0.8734637369324249, 'subsample': 0.5915553150030599, 'lambda': 0.6274723092201137, 'alpha': 0.06312760102087318}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:38:53,141] Trial 90 finished with value: 54.94037524907961 and parameters: {'n_estimators': 100, 'boosting_type': 'gbdt', 'eta': 0.062380067021547386, 'max_depth': 9, 'min_child_weight': 0.03389736195764564, 'colsample_bytree': 0.7985515268653951, 'subsample': 0.6123493697171324, 'lambda': 1.5872693449546362, 'alpha': 0.037480925895058914}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:39:24,341] Trial 91 finished with value: 54.513495586428085 and parameters: {'n_estimators': 115, 'boosting_type': 'gbdt', 'eta': 0.06729969935996971, 'max_depth': 9, 'min_child_weight': 0.015718647750997322, 'colsample_bytree': 0.8443845661623435, 'subsample': 0.5429823396000294, 'lambda': 0.3007826327586602, 'alpha': 0.06907034599463732}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.133952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:39:56,594] Trial 92 finished with value: 53.74922079291116 and parameters: {'n_estimators': 126, 'boosting_type': 'gbdt', 'eta': 0.07224606894330662, 'max_depth': 8, 'min_child_weight': 0.020191298942876804, 'colsample_bytree': 0.9470791920085407, 'subsample': 0.5306039818319696, 'lambda': 0.4244448204702531, 'alpha': 0.1497537146454505}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:40:30,691] Trial 93 finished with value: 53.83453280809936 and parameters: {'n_estimators': 119, 'boosting_type': 'gbdt', 'eta': 0.09349180185192466, 'max_depth': 8, 'min_child_weight': 0.02050680539577261, 'colsample_bytree': 0.937619835199743, 'subsample': 0.5274127984089422, 'lambda': 0.4052774529308688, 'alpha': 0.16371416262987104}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038578 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:41:02,914] Trial 94 finished with value: 53.768174225689194 and parameters: {'n_estimators': 124, 'boosting_type': 'gbdt', 'eta': 0.08931122369259534, 'max_depth': 8, 'min_child_weight': 0.022202046174316585, 'colsample_bytree': 0.9438304898088573, 'subsample': 0.5291403881868453, 'lambda': 0.43204649806307116, 'alpha': 0.16198713405642873}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:41:34,061] Trial 95 finished with value: 53.9342456075953 and parameters: {'n_estimators': 128, 'boosting_type': 'gbdt', 'eta': 0.07570905675385114, 'max_depth': 8, 'min_child_weight': 0.01910912349907029, 'colsample_bytree': 0.9441725000042672, 'subsample': 0.5271234055749654, 'lambda': 0.42026061206834464, 'alpha': 0.16920888302370213}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:42:26,820] Trial 96 finished with value: 63.47556945527742 and parameters: {'n_estimators': 130, 'boosting_type': 'dart', 'eta': 0.09996950481484768, 'max_depth': 7, 'min_child_weight': 0.021580783600205076, 'colsample_bytree': 0.9479258395015766, 'subsample': 0.5300897339984239, 'lambda': 0.38232332299473204, 'alpha': 0.28055430660148023}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076865 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:42:58,408] Trial 97 finished with value: 54.10236162038581 and parameters: {'n_estimators': 130, 'boosting_type': 'gbdt', 'eta': 0.07734322141150572, 'max_depth': 8, 'min_child_weight': 0.019558428434062, 'colsample_bytree': 0.9472325025892455, 'subsample': 0.5241737032543692, 'lambda': 0.6744011924104709, 'alpha': 0.20779913110331444}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:43:29,455] Trial 98 finished with value: 56.44683541352405 and parameters: {'n_estimators': 140, 'boosting_type': 'gbdt', 'eta': 0.11617061362304566, 'max_depth': 6, 'min_child_weight': 0.026421550445438972, 'colsample_bytree': 0.8725886616790199, 'subsample': 0.506104558838858, 'lambda': 0.4294864485279907, 'alpha': 0.12196785315973316}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099266 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.153277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-02 16:44:01,345] Trial 99 finished with value: 54.93058386138663 and parameters: {'n_estimators': 120, 'boosting_type': 'gbdt', 'eta': 0.08309260886375562, 'max_depth': 7, 'min_child_weight': 0.04232051900341139, 'colsample_bytree': 0.9455823829883485, 'subsample': 0.5044259459305019, 'lambda': 0.7224562064850906, 'alpha': 0.1526001789306401}. Best is trial 58 with value: 53.469472329280734.\n"
     ]
    }
   ],
   "source": [
    "# lgbf = LightGBMForecaster()\n",
    "\n",
    "# lgbf.train_model(\n",
    "#     train_df=df_train,\n",
    "#     target_col=\"target\",\n",
    "#     model_name=\"lightgbm_enefit\",\n",
    "#     exclude_cols=not_feature_columns,\n",
    "#     categorical_features=cat_columns\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgbf = XGBoostForecaster()\n",
    "\n",
    "# xgbf.train_model(\n",
    "#     train_df=df_train,\n",
    "#     target_col=\"target\",\n",
    "#     model_name=\"xgboost_enefit\",\n",
    "#     exclude_cols=not_feature_columns,\n",
    "#     categorical_features=cat_columns\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enefit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
