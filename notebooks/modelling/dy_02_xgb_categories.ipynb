{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "import joblib\n",
    "\n",
    "from typing import Optional, Dict, Tuple, Literal\n",
    "from enefit_challenge.models.forecaster import Forecaster\n",
    "from enefit_challenge.utils.dataset import load_enefit_training_data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "TRACKING_URI = \"http://127.0.0.1:5000/\" # local tracking URI -> launch mlflow before training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost and Categorical Variables\n",
    "\n",
    "From previous experiments, we noticed that `XGBoostForecaster` produces results comparable to `CatBoostForecaster`, and does so without the usage of categorical variables, which are not allowed automatically in the `XGBRegressor` base model.  \n",
    "\n",
    "Goal of this notebook is to expand the `XGBoostForecaster` class to enable it to use categorical features and therefore put it on equal ground with the other baseline class `CatBoostForecaster`.  \n",
    "\n",
    "Our guess is, with access to categorical features, the XGBoost version will perform even better than CatBoost.\n",
    "\n",
    "### How to treat Categorical Variables?\n",
    "Categorical variables are `['county', 'product_type']`; since they do not have a lot of cardinality, there are at least three obviuos way to treat them:\n",
    "1. using the `enable_categorical=True` parameter in the XGBRegressor, after having transformed columns into categorical columns in training dataframe using `df[col].astype('category')`;\n",
    "2. using OneHotEconding (`pd.get_dummies()`)\n",
    "3. keeping them as they are and casting values to `int`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Categorical Enabling\n",
    "\n",
    "This method does not seem to work with the XGBRegressor -> need to test the other one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_enefit_training_data()\n",
    "\n",
    "not_feature_columns = ['datetime', 'row_id','prediction_unit_id','date','time', 'data_block_id']\n",
    "cat_columns = ['county', 'product_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns to category type\n",
    "for col in cat_columns:\n",
    "    df_train[col] = df_train[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], ordered=False, categories_dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"county\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "county\n",
       "0     212872\n",
       "11    197948\n",
       "7     173042\n",
       "5     151592\n",
       "15    148714\n",
       "4     147226\n",
       "10    134604\n",
       "14    125776\n",
       "3     122464\n",
       "9     122464\n",
       "13    121024\n",
       "2     115170\n",
       "1      91848\n",
       "8      91848\n",
       "6      30616\n",
       "12     30616\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"county\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=[0, 1, 2, 3], ordered=False, categories_dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"product_type\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_type\n",
       "3    918480\n",
       "1    781428\n",
       "0    170500\n",
       "2    147416\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"product_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostForecaster(Forecaster):\n",
    "    \"\"\"\n",
    "        Implementaiton of a Forecaster using `XGBRegressor` as base model, \n",
    "        `optuna` for hyperparameters optimization and `mlflow` as backend to track experiments\n",
    "        and register best-in-class model for time series prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self)-> None:\n",
    "        self.tracking_uri = mlflow.set_tracking_uri(TRACKING_URI)\n",
    "        pass\n",
    "\n",
    "    def fit_model(\n",
    "        self,  \n",
    "        X:pd.DataFrame,\n",
    "        y:pd.Series,\n",
    "        params:Optional[Dict]=None,\n",
    "    ) -> XGBRegressor:\n",
    "        \"\"\"\n",
    "        Trains a `XGBRegressor`\n",
    "\n",
    "        -------     \n",
    "        params:\n",
    "        -------\n",
    "        `X`:`pd.DataFrame`\n",
    "            Features to use for fitting\n",
    "        `y`:`pd.Series`\n",
    "            Target variable\n",
    "        `params`: `Optional[Dict]`\n",
    "            optional dictionary of parameters to use\n",
    "        -------     \n",
    "        returns:\n",
    "        -------\n",
    "        fitted `XGBRegressor`\n",
    "        \"\"\"\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=100, \n",
    "            objective='reg:squarederror',\n",
    "            enable_categorical=True,\n",
    "            tree_method=\"approx\"\n",
    "        )\n",
    "        if params:\n",
    "            model.set_params(**params)\n",
    "\n",
    "        model.fit(X, y)\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def fit_and_test_fold(\n",
    "        self, \n",
    "        params:Dict,\n",
    "        X: pd.DataFrame, \n",
    "        y: pd.Series, \n",
    "        year_month_train, \n",
    "        year_month_test,\n",
    "        categorical_features: list=[],\n",
    "        experiment_name: str=\"xgboost\",\n",
    "        artifact_path: str=\"xgboost_model\",\n",
    "        metrics: list=[\"mae\"]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Used for cross validation on different time splits; \n",
    "        also in charge of logging every experiment run / study trial into the backend.\n",
    "        \"\"\"\n",
    "        \n",
    "        first_dates_month = pd.to_datetime(X[['year', 'month']].assign(day=1))\n",
    "        train_index = first_dates_month.isin(year_month_train)\n",
    "        test_index = first_dates_month.isin(year_month_test)\n",
    "\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        # fit model on training data\n",
    "        model = self.fit_model(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            params\n",
    "        )\n",
    "        # generate predictions\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        # self.signature = infer_signature(X_train, y_test_pred)\n",
    "        mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "        mlflow.xgboost.log_model(\n",
    "            model, \n",
    "            artifact_path=artifact_path,\n",
    "            # signature=self.signature\n",
    "        )\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        return mae\n",
    "\n",
    "    def train_model(\n",
    "        self, \n",
    "        train_df: pd.DataFrame, \n",
    "        target_col: str,\n",
    "        model_name: str,\n",
    "        exclude_cols: list=[],\n",
    "        categorical_features: list=[],\n",
    "        experiment_name: str=\"xgboost\",\n",
    "        artifact_path: str=\"xgboost_model\",\n",
    "        params: Optional[Dict]=None,\n",
    "        metrics: list=[\"MAE\"]\n",
    "    ) -> None:\n",
    "        \"\"\" \n",
    "        Takes an instance of `XGBRegressor` model and tracks the hyperparameter tuning\n",
    "        experiment on training set using `mlflow` and `optuna`.  \n",
    "        Registers the best version of the model according to a specified metric (to be implemented).\n",
    "        \n",
    "        -------     \n",
    "        params:\n",
    "        -------\n",
    "        `experiment_name`: `str`\n",
    "            the name of the experiment used to store runs in mlflow, \n",
    "            as well as the name of the optuna study\n",
    "        `model_name`: `str`\n",
    "            the name the final model will have in the registry\n",
    "        `train_df`: `pd.DataFrame`\n",
    "            the training data for the model.\n",
    "        `target_col`: `str`\n",
    "            the time-series target column\n",
    "        `exclude_cols`: `list`  \n",
    "            columns in dataset that should not be used\n",
    "        `categorical_features`: `list` \n",
    "            list fo categorical features\n",
    "        `artifact_path`: `str`\n",
    "            the path pointing to the mlflow artifact\n",
    "        `metrics`: `list`\n",
    "            list of the metrics to track in the mlflow experiment run.\n",
    "        `params`: `Optional[Dict]`\n",
    "            optional dictionary of parameters to use\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "\n",
    "        if len(categorical_features) > 0: \n",
    "            for col in cat_columns:\n",
    "                train_df[col] = df_train[col].astype('category')\n",
    "        \n",
    "        X = train_df.drop([target_col] + exclude_cols, axis=1)\n",
    "        y = train_df[target_col]\n",
    "        # unique year-month combinations -> to be used in cross-validation\n",
    "        timesteps = np.sort(np.array(\n",
    "            pd.to_datetime(X[['year', 'month']].assign(day=1)).unique().tolist()\n",
    "        ))\n",
    "\n",
    "        # define mlflow callback Handler for optuna \n",
    "        mlflc = MLflowCallback(\n",
    "            metric_name=\"MAE\",\n",
    "        )\n",
    "    \n",
    "        @mlflc.track_in_mlflow() # decorator to allow mlflow logging\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 200, log=True),\n",
    "                'eta': trial.suggest_float('eta', 0.01, 0.95,log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 1, 10, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 25, log=True),\n",
    "                'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1, log=True),\n",
    "                'colsample_bylevel': trial.suggest_float(\"colsample_bylevel\", 0.1, 1, log=True),\n",
    "                'colsample_bynode': trial.suggest_float(\"colsample_bynode\", 0.1, 1, log=True),\n",
    "                'subsample': trial.suggest_float(\"subsample\", 0.5, 1, log=True),\n",
    "                'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "                'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True)\n",
    "            }\n",
    "            cv = TimeSeriesSplit(n_splits=3) # cross validation\n",
    "            cv_mae = [None]*3\n",
    "            for i, (train_index, test_index) in enumerate(cv.split(timesteps)):\n",
    "                cv_mae[i] = self.fit_and_test_fold(\n",
    "                    params,\n",
    "                    X, \n",
    "                    y, \n",
    "                    timesteps[train_index], \n",
    "                    timesteps[test_index],\n",
    "                    categorical_features\n",
    "                )\n",
    "            trial.set_user_attr('split_mae', cv_mae)\n",
    "            return np.mean(cv_mae)\n",
    "\n",
    "        \n",
    "        sampler = optuna.samplers.TPESampler(\n",
    "            n_startup_trials=10, \n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        self.study = optuna.create_study(\n",
    "            directions=['minimize'],\n",
    "            sampler=sampler,\n",
    "            study_name=experiment_name\n",
    "        )\n",
    "\n",
    "        self.study.optimize(objective, n_trials=10, timeout= 7200, callbacks=[mlflc]) \n",
    "        \n",
    "        # # search for the best run at the end of the experiment # not implemented now bc of callback bug\n",
    "        # best_run = mlflow.search_runs(max_results=1,order_by=[\"metrics.MAE\"]).run_id\n",
    "        # # register new model version in mlflow\n",
    "        # self.result = mlflow.register_model(\n",
    "        #     model_uri=f\"runs:/{best_run}/{artifact_path}\",\n",
    "        #     name=self.model_name\n",
    "        # )\n",
    "\n",
    "    def forecast(\n",
    "        self, \n",
    "        input_data: pd.DataFrame,\n",
    "        use_best_from_run: bool=True,\n",
    "        use_env_model: Literal[\"Staging\", \"Production\", None]=None,\n",
    "        use_version: int=None\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Fetches a version of the model from the mlflow backend and uses it\n",
    "        to perform prediction on new input data.  \n",
    "        What version is used depends on params settings, \n",
    "        defaults to using the best version from the last experiment run (currently not implemented). \n",
    "        -------     \n",
    "        params:\n",
    "        -------\n",
    "        `input_data`: `pd.DataFrame`\n",
    "            the input data for prediction,\n",
    "              must have the same schema as what's in the model's signature.\n",
    "        `use_best_from_run`: `bool=True`      \n",
    "            use the best model from the current series of iterations, defaults to True\n",
    "        `use_env_model`: `Literal[\"Staging\", \"Production\", None]=None`\n",
    "            use model from a given mlflow environment, defaults to None.  \n",
    "            Said model might come from past iterations, depending on what you decide in the UI\n",
    "        `use_version`: `int=None`\n",
    "            use a previously trained version of the model. \n",
    "            Said version must have been registered from a previous iteration,  \n",
    "            either by the UI or with mlflow's API\n",
    "        \"\"\"\n",
    "        if use_best_from_run:\n",
    "            # not implemented now bc of callback bug\n",
    "            use_prod_model=None\n",
    "            use_version=None\n",
    "        \n",
    "            # model = mlflow.pyfunc.load_model(\n",
    "            #     model_uri=f\"models:/{self.model_name}/{self.result.version}\"\n",
    "            # )\n",
    "            # y_pred = model.predict(input_data)\n",
    "            # return y_pred\n",
    "        \n",
    "        if use_env_model is not None:\n",
    "            use_version = None\n",
    "\n",
    "            model = mlflow.pyfunc.load_model(\n",
    "                # get registered model in given environment\n",
    "                model_uri=f\"models:/{self.model_name}/{use_env_model}\"\n",
    "            )\n",
    "            y_pred = model.predict(input_data)\n",
    "            return y_pred\n",
    "\n",
    "        if use_version is not None:\n",
    "            # get specific registered version of model\n",
    "            model = mlflow.pyfunc.load_model(\n",
    "                model_uri=f\"models:/{self.model_name}/{use_version}\"\n",
    "            )\n",
    "            y_pred = model.predict(input_data)\n",
    "            return y_pred\n",
    "\n",
    "        \n",
    "        if (not use_best_from_run) & (use_env_model is None) & (use_version is None):\n",
    "            return ValueError(\n",
    "                    \"You must specify which kind of XGBoostForecaster you intend to use for prediction\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_enefit_training_data()\n",
    "\n",
    "not_feature_columns = ['datetime', 'row_id','prediction_unit_id','date','time', 'data_block_id']\n",
    "cat_columns = ['county', 'product_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-19 11:17:26,367] A new study created in memory with name: xgboost\n",
      "[W 2023-11-19 11:17:28,522] Trial 0 failed with parameters: {'n_estimators': 84, 'eta': 0.7590145927293601, 'max_depth': 5, 'min_child_weight': 5, 'colsample_bytree': 0.1432249371823025, 'colsample_bylevel': 0.14321698289111517, 'colsample_bynode': 0.1143098387631322, 'subsample': 0.9114125527116832, 'lambda': 0.25378155082656645, 'alpha': 0.6796578090758157} because of the following error: XGBoostError('[11:17:28] /Users/runner/miniforge3/conda-bld/xgboost-split_1697107917112/work/src/tree/tree_model.cc:869: Check failed: !HasCategoricalSplit(): Please use JSON/UBJSON for saving models with categorical splits.\\nStack trace:\\n  [bt] (0) 1   libxgboost.dylib                    0x000000013be706c0 dmlc::LogMessageFatal::~LogMessageFatal() + 140\\n  [bt] (1) 2   libxgboost.dylib                    0x000000013c0372a8 xgboost::RegTree::Save(dmlc::Stream*) const + 1120\\n  [bt] (2) 3   libxgboost.dylib                    0x000000013bf79200 xgboost::gbm::GBTreeModel::Save(dmlc::Stream*) const + 304\\n  [bt] (3) 4   libxgboost.dylib                    0x000000013bf843f4 xgboost::LearnerIO::SaveModel(dmlc::Stream*) const + 1224\\n  [bt] (4) 5   libxgboost.dylib                    0x000000013be8fabc XGBoosterSaveModel + 940\\n  [bt] (5) 6   libffi.8.dylib                      0x0000000102df804c ffi_call_SYSV + 76\\n  [bt] (6) 7   libffi.8.dylib                      0x0000000102df5834 ffi_call_int + 1404\\n  [bt] (7) 8   _ctypes.cpython-311-darwin.so       0x0000000102d488bc _ctypes_callproc + 1232\\n  [bt] (8) 9   _ctypes.cpython-311-darwin.so       0x0000000102d42a70 PyCFuncPtr_call + 1216\\n\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/integration/mlflow.py\", line 218, in wrapper\n",
      "    return func(trial)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/var/folders/2c/394jj2r140x4jgq0vzwn32680000gn/T/ipykernel_6745/1460953200.py\", line 168, in objective\n",
      "    cv_mae[i] = self.fit_and_test_fold(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/2c/394jj2r140x4jgq0vzwn32680000gn/T/ipykernel_6745/1460953200.py\", line 84, in fit_and_test_fold\n",
      "    mlflow.xgboost.log_model(\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/xgboost/__init__.py\", line 270, in log_model\n",
      "    return Model.log(\n",
      "           ^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/models/model.py\", line 619, in log\n",
      "    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/xgboost/__init__.py\", line 169, in save_model\n",
      "    xgb_model.save_model(model_data_path)\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/sklearn.py\", line 767, in save_model\n",
      "    self.get_booster().save_model(fname)\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/core.py\", line 2389, in save_model\n",
      "    _check_call(_LIB.XGBoosterSaveModel(\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/core.py\", line 279, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [11:17:28] /Users/runner/miniforge3/conda-bld/xgboost-split_1697107917112/work/src/tree/tree_model.cc:869: Check failed: !HasCategoricalSplit(): Please use JSON/UBJSON for saving models with categorical splits.\n",
      "Stack trace:\n",
      "  [bt] (0) 1   libxgboost.dylib                    0x000000013be706c0 dmlc::LogMessageFatal::~LogMessageFatal() + 140\n",
      "  [bt] (1) 2   libxgboost.dylib                    0x000000013c0372a8 xgboost::RegTree::Save(dmlc::Stream*) const + 1120\n",
      "  [bt] (2) 3   libxgboost.dylib                    0x000000013bf79200 xgboost::gbm::GBTreeModel::Save(dmlc::Stream*) const + 304\n",
      "  [bt] (3) 4   libxgboost.dylib                    0x000000013bf843f4 xgboost::LearnerIO::SaveModel(dmlc::Stream*) const + 1224\n",
      "  [bt] (4) 5   libxgboost.dylib                    0x000000013be8fabc XGBoosterSaveModel + 940\n",
      "  [bt] (5) 6   libffi.8.dylib                      0x0000000102df804c ffi_call_SYSV + 76\n",
      "  [bt] (6) 7   libffi.8.dylib                      0x0000000102df5834 ffi_call_int + 1404\n",
      "  [bt] (7) 8   _ctypes.cpython-311-darwin.so       0x0000000102d488bc _ctypes_callproc + 1232\n",
      "  [bt] (8) 9   _ctypes.cpython-311-darwin.so       0x0000000102d42a70 PyCFuncPtr_call + 1216\n",
      "\n",
      "\n",
      "[W 2023-11-19 11:17:28,524] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[11:17:28] /Users/runner/miniforge3/conda-bld/xgboost-split_1697107917112/work/src/tree/tree_model.cc:869: Check failed: !HasCategoricalSplit(): Please use JSON/UBJSON for saving models with categorical splits.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000013be706c0 dmlc::LogMessageFatal::~LogMessageFatal() + 140\n  [bt] (1) 2   libxgboost.dylib                    0x000000013c0372a8 xgboost::RegTree::Save(dmlc::Stream*) const + 1120\n  [bt] (2) 3   libxgboost.dylib                    0x000000013bf79200 xgboost::gbm::GBTreeModel::Save(dmlc::Stream*) const + 304\n  [bt] (3) 4   libxgboost.dylib                    0x000000013bf843f4 xgboost::LearnerIO::SaveModel(dmlc::Stream*) const + 1224\n  [bt] (4) 5   libxgboost.dylib                    0x000000013be8fabc XGBoosterSaveModel + 940\n  [bt] (5) 6   libffi.8.dylib                      0x0000000102df804c ffi_call_SYSV + 76\n  [bt] (6) 7   libffi.8.dylib                      0x0000000102df5834 ffi_call_int + 1404\n  [bt] (7) 8   _ctypes.cpython-311-darwin.so       0x0000000102d488bc _ctypes_callproc + 1232\n  [bt] (8) 9   _ctypes.cpython-311-darwin.so       0x0000000102d42a70 PyCFuncPtr_call + 1216\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m xgbf \u001b[39m=\u001b[39m XGBoostForecaster()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m xgbf\u001b[39m.\u001b[39;49mtrain_model(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_df\u001b[39m=\u001b[39;49mdf_train,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     target_col\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtarget\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mxgboost_enefit\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     exclude_cols\u001b[39m=\u001b[39;49mnot_feature_columns,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     categorical_features\u001b[39m=\u001b[39;49mcat_columns\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "\u001b[1;32m/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=179'>180</a>\u001b[0m sampler \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m     n_startup_trials\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=181'>182</a>\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstudy \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m     directions\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m     sampler\u001b[39m=\u001b[39msampler,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=187'>188</a>\u001b[0m     study_name\u001b[39m=\u001b[39mexperiment_name\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m )\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstudy\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, timeout\u001b[39m=\u001b[39;49m \u001b[39m7200\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[mlflc])\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/integration/mlflow.py:218\u001b[0m, in \u001b[0;36mMLflowCallback.track_in_mlflow.<locals>.decorator.<locals>.wrapper\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(trial\u001b[39m.\u001b[39mnumber), nested\u001b[39m=\u001b[39mnested) \u001b[39mas\u001b[39;00m run:\n\u001b[1;32m    214\u001b[0m     trial\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mset_trial_system_attr(\n\u001b[1;32m    215\u001b[0m         trial\u001b[39m.\u001b[39m_trial_id, RUN_ID_ATTRIBUTE_KEY, run\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id\n\u001b[1;32m    216\u001b[0m     )\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m func(trial)\n",
      "\u001b[1;32m/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m cv_mae \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m]\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (train_index, test_index) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(cv\u001b[39m.\u001b[39msplit(timesteps)):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m     cv_mae[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_and_test_fold(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=168'>169</a>\u001b[0m         params,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m         X, \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=170'>171</a>\u001b[0m         y, \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m         timesteps[train_index], \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m         timesteps[test_index],\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m         categorical_features\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m trial\u001b[39m.\u001b[39mset_user_attr(\u001b[39m'\u001b[39m\u001b[39msplit_mae\u001b[39m\u001b[39m'\u001b[39m, cv_mae)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=176'>177</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(cv_mae)\n",
      "\u001b[1;32m/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb Cell 12\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m# self.signature = infer_signature(X_train, y_test_pred)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m mae \u001b[39m=\u001b[39m mean_absolute_error(y_test, y_test_pred)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m mlflow\u001b[39m.\u001b[39;49mxgboost\u001b[39m.\u001b[39;49mlog_model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     model, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     artifact_path\u001b[39m=\u001b[39;49martifact_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     \u001b[39m# signature=self.signature\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m mlflow\u001b[39m.\u001b[39mlog_params(params)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_02_xgb_categories.ipynb#X14sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39mreturn\u001b[39;00m mae\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/xgboost/__init__.py:270\u001b[0m, in \u001b[0;36mlog_model\u001b[0;34m(xgb_model, artifact_path, conda_env, code_paths, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, model_format, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39m@format_docstring\u001b[39m(LOG_MODEL_PARAM_DOCS\u001b[39m.\u001b[39mformat(package_name\u001b[39m=\u001b[39mFLAVOR_NAME))\n\u001b[1;32m    226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_model\u001b[39m(\n\u001b[1;32m    227\u001b[0m     xgb_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    240\u001b[0m ):\n\u001b[1;32m    241\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[39m    Log an XGBoost model as an MLflow artifact for the current run.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m             metadata of the logged model.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mreturn\u001b[39;00m Model\u001b[39m.\u001b[39;49mlog(\n\u001b[1;32m    271\u001b[0m         artifact_path\u001b[39m=\u001b[39;49martifact_path,\n\u001b[1;32m    272\u001b[0m         flavor\u001b[39m=\u001b[39;49mmlflow\u001b[39m.\u001b[39;49mxgboost,\n\u001b[1;32m    273\u001b[0m         registered_model_name\u001b[39m=\u001b[39;49mregistered_model_name,\n\u001b[1;32m    274\u001b[0m         xgb_model\u001b[39m=\u001b[39;49mxgb_model,\n\u001b[1;32m    275\u001b[0m         model_format\u001b[39m=\u001b[39;49mmodel_format,\n\u001b[1;32m    276\u001b[0m         conda_env\u001b[39m=\u001b[39;49mconda_env,\n\u001b[1;32m    277\u001b[0m         code_paths\u001b[39m=\u001b[39;49mcode_paths,\n\u001b[1;32m    278\u001b[0m         signature\u001b[39m=\u001b[39;49msignature,\n\u001b[1;32m    279\u001b[0m         input_example\u001b[39m=\u001b[39;49minput_example,\n\u001b[1;32m    280\u001b[0m         await_registration_for\u001b[39m=\u001b[39;49mawait_registration_for,\n\u001b[1;32m    281\u001b[0m         pip_requirements\u001b[39m=\u001b[39;49mpip_requirements,\n\u001b[1;32m    282\u001b[0m         extra_pip_requirements\u001b[39m=\u001b[39;49mextra_pip_requirements,\n\u001b[1;32m    283\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    284\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    285\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/models/model.py:619\u001b[0m, in \u001b[0;36mModel.log\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    614\u001b[0m     (tracking_uri \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdatabricks\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m get_uri_scheme(tracking_uri) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdatabricks\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    615\u001b[0m     \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msignature\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    616\u001b[0m     \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39minput_example\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    617\u001b[0m ):\n\u001b[1;32m    618\u001b[0m     _logger\u001b[39m.\u001b[39mwarning(_LOG_MODEL_MISSING_SIGNATURE_WARNING)\n\u001b[0;32m--> 619\u001b[0m flavor\u001b[39m.\u001b[39;49msave_model(path\u001b[39m=\u001b[39;49mlocal_path, mlflow_model\u001b[39m=\u001b[39;49mmlflow_model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    620\u001b[0m mlflow\u001b[39m.\u001b[39mtracking\u001b[39m.\u001b[39mfluent\u001b[39m.\u001b[39mlog_artifacts(local_path, mlflow_model\u001b[39m.\u001b[39martifact_path)\n\u001b[1;32m    621\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/xgboost/__init__.py:169\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(xgb_model, path, conda_env, code_paths, mlflow_model, signature, input_example, pip_requirements, extra_pip_requirements, model_format, metadata)\u001b[0m\n\u001b[1;32m    166\u001b[0m model_data_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, model_data_subpath)\n\u001b[1;32m    168\u001b[0m \u001b[39m# Save an XGBoost model\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m xgb_model\u001b[39m.\u001b[39;49msave_model(model_data_path)\n\u001b[1;32m    170\u001b[0m xgb_model_class \u001b[39m=\u001b[39m _get_fully_qualified_class_name(xgb_model)\n\u001b[1;32m    171\u001b[0m pyfunc\u001b[39m.\u001b[39madd_to_model(\n\u001b[1;32m    172\u001b[0m     mlflow_model,\n\u001b[1;32m    173\u001b[0m     loader_module\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmlflow.xgboost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     code\u001b[39m=\u001b[39mcode_dir_subpath,\n\u001b[1;32m    178\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/sklearn.py:767\u001b[0m, in \u001b[0;36mXGBModel.save_model\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m    765\u001b[0m meta_str \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(meta)\n\u001b[1;32m    766\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_booster()\u001b[39m.\u001b[39mset_attr(scikit_learn\u001b[39m=\u001b[39mmeta_str)\n\u001b[0;32m--> 767\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_booster()\u001b[39m.\u001b[39;49msave_model(fname)\n\u001b[1;32m    768\u001b[0m \u001b[39m# Delete the attribute after save\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_booster()\u001b[39m.\u001b[39mset_attr(scikit_learn\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/core.py:2389\u001b[0m, in \u001b[0;36mBooster.save_model\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m   2387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, (\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike)):  \u001b[39m# assume file name\u001b[39;00m\n\u001b[1;32m   2388\u001b[0m     fname \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexpanduser(fname))\n\u001b[0;32m-> 2389\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterSaveModel(\n\u001b[1;32m   2390\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, c_str(fname)))\n\u001b[1;32m   2391\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2392\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mfname must be a string or os PathLike\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [11:17:28] /Users/runner/miniforge3/conda-bld/xgboost-split_1697107917112/work/src/tree/tree_model.cc:869: Check failed: !HasCategoricalSplit(): Please use JSON/UBJSON for saving models with categorical splits.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000013be706c0 dmlc::LogMessageFatal::~LogMessageFatal() + 140\n  [bt] (1) 2   libxgboost.dylib                    0x000000013c0372a8 xgboost::RegTree::Save(dmlc::Stream*) const + 1120\n  [bt] (2) 3   libxgboost.dylib                    0x000000013bf79200 xgboost::gbm::GBTreeModel::Save(dmlc::Stream*) const + 304\n  [bt] (3) 4   libxgboost.dylib                    0x000000013bf843f4 xgboost::LearnerIO::SaveModel(dmlc::Stream*) const + 1224\n  [bt] (4) 5   libxgboost.dylib                    0x000000013be8fabc XGBoosterSaveModel + 940\n  [bt] (5) 6   libffi.8.dylib                      0x0000000102df804c ffi_call_SYSV + 76\n  [bt] (6) 7   libffi.8.dylib                      0x0000000102df5834 ffi_call_int + 1404\n  [bt] (7) 8   _ctypes.cpython-311-darwin.so       0x0000000102d488bc _ctypes_callproc + 1232\n  [bt] (8) 9   _ctypes.cpython-311-darwin.so       0x0000000102d42a70 PyCFuncPtr_call + 1216\n\n"
     ]
    }
   ],
   "source": [
    "xgbf = XGBoostForecaster()\n",
    "\n",
    "xgbf.train_model(\n",
    "    train_df=df_train,\n",
    "    target_col=\"target\",\n",
    "    model_name=\"xgboost_enefit\",\n",
    "    exclude_cols=not_feature_columns,\n",
    "    categorical_features=cat_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_enefit_training_data()\n",
    "\n",
    "not_feature_columns = ['datetime', 'row_id','prediction_unit_id','date','time', 'data_block_id']\n",
    "cat_columns = ['county', 'product_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df_train, columns=cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['is_business', 'target', 'is_consumption', 'datetime', 'data_block_id',\n",
       "       'row_id', 'prediction_unit_id', 'date', 'time', 'year',\n",
       "       'datediff_in_days', 'hour', 'hour_sine', 'hour_cosine', 'dayofweek',\n",
       "       'dayofweek_sine', 'dayofweek_cosine', 'week', 'week_sine',\n",
       "       'week_cosine', 'month', 'month_sine', 'month_cosine',\n",
       "       'target_2_days_ago', 'eic_count', 'installed_capacity', 'euros_per_mwh',\n",
       "       'lowest_price_per_mwh', 'highest_price_per_mwh', 'county_0', 'county_1',\n",
       "       'county_2', 'county_3', 'county_4', 'county_5', 'county_6', 'county_7',\n",
       "       'county_8', 'county_9', 'county_10', 'county_11', 'county_12',\n",
       "       'county_13', 'county_14', 'county_15', 'product_type_0',\n",
       "       'product_type_1', 'product_type_2', 'product_type_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostForecaster(Forecaster):\n",
    "    \"\"\"\n",
    "        Implementation of a Forecaster using `XGBRegressor` as base model, \n",
    "        `optuna` for hyperparameters optimization and `mlflow` as backend to track experiments\n",
    "        and register best-in-class model for time series prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self)-> None:\n",
    "        self.tracking_uri = mlflow.set_tracking_uri(TRACKING_URI)\n",
    "        pass\n",
    "\n",
    "    def fit_model(\n",
    "        self,  \n",
    "        X:pd.DataFrame,\n",
    "        y:pd.Series,\n",
    "        params:Optional[Dict]=None,\n",
    "    ) -> XGBRegressor:\n",
    "        \"\"\"\n",
    "        Trains a `XGBRegressor`\n",
    "\n",
    "        -------     \n",
    "        params:\n",
    "        -------\n",
    "        `X`:`pd.DataFrame`\n",
    "            Features to use for fitting\n",
    "        `y`:`pd.Series`\n",
    "            Target variable\n",
    "        `params`: `Optional[Dict]`\n",
    "            optional dictionary of parameters to use\n",
    "        -------     \n",
    "        returns:\n",
    "        -------\n",
    "        fitted `XGBRegressor`\n",
    "        \"\"\"\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=100, \n",
    "            objective='reg:squarederror'\n",
    "        )\n",
    "        if params:\n",
    "            model.set_params(**params)\n",
    "\n",
    "        model.fit(X, y)\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def fit_and_test_fold(\n",
    "        self, \n",
    "        params:Dict,\n",
    "        X: pd.DataFrame, \n",
    "        y: pd.Series, \n",
    "        year_month_train, \n",
    "        year_month_test,\n",
    "        experiment_name: str=\"xgboost\",\n",
    "        artifact_path: str=\"xgboost_model\",\n",
    "        metrics: list=[\"mae\"]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Used for cross validation on different time splits; \n",
    "        also in charge of logging every experiment run / study trial into the backend.\n",
    "        \"\"\"\n",
    "        \n",
    "        first_dates_month = pd.to_datetime(X[['year', 'month']].assign(day=1))\n",
    "        train_index = first_dates_month.isin(year_month_train)\n",
    "        test_index = first_dates_month.isin(year_month_test)\n",
    "\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        # fit model on training data\n",
    "        model = self.fit_model(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            params\n",
    "        )\n",
    "        # generate predictions\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        self.signature = infer_signature(X_train, y_test_pred)\n",
    "        mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "        mlflow.xgboost.log_model(\n",
    "            model, \n",
    "            artifact_path=artifact_path,\n",
    "            signature=self.signature\n",
    "        )\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        return mae\n",
    "\n",
    "    def train_model(\n",
    "        self, \n",
    "        train_df: pd.DataFrame, \n",
    "        target_col: str,\n",
    "        model_name: str,\n",
    "        exclude_cols: list=[],\n",
    "        categorical_features: list=[],\n",
    "        experiment_name: str=\"xgboost\",\n",
    "        artifact_path: str=\"xgboost_model\",\n",
    "        params: Optional[Dict]=None,\n",
    "        metrics: list=[\"MAE\"]\n",
    "    ) -> None:\n",
    "        \"\"\" \n",
    "        Takes an instance of `XGBRegressor` model and tracks the hyperparameter tuning\n",
    "        experiment on training set using `mlflow` and `optuna`.  \n",
    "        Registers the best version of the model according to a specified metric (to be implemented).\n",
    "        \n",
    "        -------     \n",
    "        params:\n",
    "        -------\n",
    "        `experiment_name`: `str`\n",
    "            the name of the experiment used to store runs in mlflow, \n",
    "            as well as the name of the optuna study\n",
    "        `model_name`: `str`\n",
    "            the name the final model will have in the registry\n",
    "        `train_df`: `pd.DataFrame`\n",
    "            the training data for the model.\n",
    "        `target_col`: `str`\n",
    "            the time-series target column\n",
    "        `exclude_cols`: `list`  \n",
    "            columns in dataset that should not be used\n",
    "        `artifact_path`: `str`\n",
    "            the path pointing to the mlflow artifact\n",
    "        `metrics`: `list`\n",
    "            list of the metrics to track in the mlflow experiment run.\n",
    "        `params`: `Optional[Dict]`\n",
    "            optional dictionary of parameters to use\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "\n",
    "        if len(categorical_features) > 0: \n",
    "           train_df = pd.get_dummies(train_df, columns=cat_columns)\n",
    "\n",
    "        X = train_df.drop([target_col] + exclude_cols, axis=1)\n",
    "        y = train_df[target_col]\n",
    "        # unique year-month combinations -> to be used in cross-validation\n",
    "        timesteps = np.sort(np.array(\n",
    "            pd.to_datetime(X[['year', 'month']].assign(day=1)).unique().tolist()\n",
    "        ))\n",
    "\n",
    "        # define mlflow callback Handler for optuna \n",
    "        mlflc = MLflowCallback(\n",
    "            metric_name=\"MAE\",\n",
    "        )\n",
    "    \n",
    "        @mlflc.track_in_mlflow() # decorator to allow mlflow logging\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 200, log=True),\n",
    "                'eta': trial.suggest_float('eta', 0.01, 0.95,log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 1, 10, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 25, log=True),\n",
    "                'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1, log=True),\n",
    "                'colsample_bylevel': trial.suggest_float(\"colsample_bylevel\", 0.1, 1, log=True),\n",
    "                'colsample_bynode': trial.suggest_float(\"colsample_bynode\", 0.1, 1, log=True),\n",
    "                'subsample': trial.suggest_float(\"subsample\", 0.5, 1, log=True),\n",
    "                'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "                'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True)\n",
    "            }\n",
    "            cv = TimeSeriesSplit(n_splits=3) # cross validation\n",
    "            cv_mae = [None]*3\n",
    "            for i, (train_index, test_index) in enumerate(cv.split(timesteps)):\n",
    "                cv_mae[i] = self.fit_and_test_fold(\n",
    "                    params,\n",
    "                    X, \n",
    "                    y, \n",
    "                    timesteps[train_index], \n",
    "                    timesteps[test_index]\n",
    "                )\n",
    "            trial.set_user_attr('split_mae', cv_mae)\n",
    "            return np.mean(cv_mae)\n",
    "\n",
    "        \n",
    "        sampler = optuna.samplers.TPESampler(\n",
    "            n_startup_trials=10, \n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        self.study = optuna.create_study(\n",
    "            directions=['minimize'],\n",
    "            sampler=sampler,\n",
    "            study_name=experiment_name\n",
    "        )\n",
    "\n",
    "        self.study.optimize(objective, n_trials=50, timeout= 7200, callbacks=[mlflc]) \n",
    "        \n",
    "        # # search for the best run at the end of the experiment # not implemented now bc of callback bug\n",
    "        # best_run = mlflow.search_runs(max_results=1,order_by=[\"metrics.MAE\"]).run_id\n",
    "        # # register new model version in mlflow\n",
    "        # self.result = mlflow.register_model(\n",
    "        #     model_uri=f\"runs:/{best_run}/{artifact_path}\",\n",
    "        #     name=self.model_name\n",
    "        # )\n",
    "\n",
    "    def forecast(\n",
    "        self, \n",
    "        input_data: pd.DataFrame,\n",
    "        use_best_from_run: bool=True,\n",
    "        use_env_model: Literal[\"Staging\", \"Production\", None]=None,\n",
    "        use_version: int=None\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Fetches a version of the model from the mlflow backend and uses it\n",
    "        to perform prediction on new input data.  \n",
    "        What version is used depends on params settings, \n",
    "        defaults to using the best version from the last experiment run (currently not implemented). \n",
    "        -------     \n",
    "        params:\n",
    "        -------\n",
    "        `input_data`: `pd.DataFrame`\n",
    "            the input data for prediction,\n",
    "              must have the same schema as what's in the model's signature.\n",
    "        `use_best_from_run`: `bool=True`      \n",
    "            use the best model from the current series of iterations, defaults to True\n",
    "        `use_env_model`: `Literal[\"Staging\", \"Production\", None]=None`\n",
    "            use model from a given mlflow environment, defaults to None.  \n",
    "            Said model might come from past iterations, depending on what you decide in the UI\n",
    "        `use_version`: `int=None`\n",
    "            use a previously trained version of the model. \n",
    "            Said version must have been registered from a previous iteration,  \n",
    "            either by the UI or with mlflow's API\n",
    "        \"\"\"\n",
    "        if use_best_from_run:\n",
    "            # not implemented now bc of callback bug\n",
    "            use_prod_model=None\n",
    "            use_version=None\n",
    "        \n",
    "            # model = mlflow.pyfunc.load_model(\n",
    "            #     model_uri=f\"models:/{self.model_name}/{self.result.version}\"\n",
    "            # )\n",
    "            # y_pred = model.predict(input_data)\n",
    "            # return y_pred\n",
    "        \n",
    "        if use_env_model is not None:\n",
    "            use_version = None\n",
    "\n",
    "            model = mlflow.pyfunc.load_model(\n",
    "                # get registered model in given environment\n",
    "                model_uri=f\"models:/{self.model_name}/{use_env_model}\"\n",
    "            )\n",
    "            y_pred = model.predict(input_data)\n",
    "            return y_pred\n",
    "\n",
    "        if use_version is not None:\n",
    "            # get specific registered version of model\n",
    "            model = mlflow.pyfunc.load_model(\n",
    "                model_uri=f\"models:/{self.model_name}/{use_version}\"\n",
    "            )\n",
    "            y_pred = model.predict(input_data)\n",
    "            return y_pred\n",
    "\n",
    "        \n",
    "        if (not use_best_from_run) & (use_env_model is None) & (use_version is None):\n",
    "            return ValueError(\n",
    "                    \"You must specify which kind of XGBoostForecaster you intend to use for prediction\"\n",
    "                    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_enefit_training_data()\n",
    "\n",
    "not_feature_columns = ['datetime', 'row_id','prediction_unit_id','date','time', 'data_block_id']\n",
    "cat_columns = ['county', 'product_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-19 11:42:00,222] A new study created in memory with name: xgboost\n",
      "[I 2023-11-19 11:42:30,915] Trial 0 finished with value: 176.29408523071473 and parameters: {'n_estimators': 84, 'eta': 0.7590145927293601, 'max_depth': 5, 'min_child_weight': 5, 'colsample_bytree': 0.1432249371823025, 'colsample_bylevel': 0.14321698289111517, 'colsample_bynode': 0.1143098387631322, 'subsample': 0.9114125527116832, 'lambda': 0.25378155082656645, 'alpha': 0.6796578090758157}. Best is trial 0 with value: 176.29408523071473.\n",
      "[I 2023-11-19 11:42:55,161] Trial 1 finished with value: 206.06682436048575 and parameters: {'n_estimators': 51, 'eta': 0.8283494908181351, 'max_depth': 6, 'min_child_weight': 1, 'colsample_bytree': 0.1519934830130981, 'colsample_bylevel': 0.15254729458052607, 'colsample_bynode': 0.20148477884158655, 'subsample': 0.7193453335958095, 'lambda': 0.05342937261279776, 'alpha': 0.014618962793704969}. Best is trial 0 with value: 176.29408523071473.\n",
      "[I 2023-11-19 11:43:19,824] Trial 2 finished with value: 319.6287466738586 and parameters: {'n_estimators': 116, 'eta': 0.01887471058056493, 'max_depth': 1, 'min_child_weight': 2, 'colsample_bytree': 0.2858051065806936, 'colsample_bylevel': 0.6097839109531512, 'colsample_bynode': 0.15837031559118753, 'subsample': 0.7141180248258306, 'lambda': 0.23423849847112918, 'alpha': 0.0015339162591163618}. Best is trial 0 with value: 176.29408523071473.\n",
      "[I 2023-11-19 11:43:47,680] Trial 3 finished with value: 287.1557503287175 and parameters: {'n_estimators': 116, 'eta': 0.02173950167319137, 'max_depth': 1, 'min_child_weight': 21, 'colsample_bytree': 0.9239150319627247, 'colsample_bylevel': 0.6432759992849892, 'colsample_bynode': 0.2016572169180859, 'subsample': 0.5350227390366598, 'lambda': 0.5456725485601478, 'alpha': 0.057624872164786005}. Best is trial 0 with value: 176.29408523071473.\n",
      "[I 2023-11-19 11:44:06,831] Trial 4 finished with value: 312.09967921534786 and parameters: {'n_estimators': 59, 'eta': 0.09535051957858227, 'max_depth': 1, 'min_child_weight': 18, 'colsample_bytree': 0.18145961353490248, 'colsample_bylevel': 0.4597505784732165, 'colsample_bynode': 0.2049798052095018, 'subsample': 0.7170114293588699, 'lambda': 0.1537592023548176, 'alpha': 0.0054880470007660455}. Best is trial 0 with value: 176.29408523071473.\n",
      "[I 2023-11-19 11:45:21,410] Trial 5 finished with value: 141.97707691533307 and parameters: {'n_estimators': 192, 'eta': 0.3411917190406207, 'max_depth': 9, 'min_child_weight': 17, 'colsample_bytree': 0.39618677904065835, 'colsample_bylevel': 0.835361075531176, 'colsample_bynode': 0.12260057359187529, 'subsample': 0.5727521453035685, 'lambda': 0.0015167330688076208, 'alpha': 0.02001342062287998}. Best is trial 5 with value: 141.97707691533307.\n",
      "[I 2023-11-19 11:45:51,446] Trial 6 finished with value: 273.1512181698851 and parameters: {'n_estimators': 85, 'eta': 0.03440752114258332, 'max_depth': 6, 'min_child_weight': 2, 'colsample_bytree': 0.19095652801045376, 'colsample_bylevel': 0.3488960745139221, 'colsample_bynode': 0.1383324997521996, 'subsample': 0.8718772746165767, 'lambda': 0.0019870215385428634, 'alpha': 8.862326508576256}. Best is trial 5 with value: 141.97707691533307.\n",
      "[I 2023-11-19 11:46:24,690] Trial 7 finished with value: 250.74047456655163 and parameters: {'n_estimators': 146, 'eta': 0.024717508610145263, 'max_depth': 1, 'min_child_weight': 12, 'colsample_bytree': 0.5091635945818552, 'colsample_bylevel': 0.5358055009231864, 'colsample_bynode': 0.5905685925060635, 'subsample': 0.5263318671916041, 'lambda': 0.02715581955282941, 'alpha': 0.0029072088906598446}. Best is trial 5 with value: 141.97707691533307.\n",
      "[I 2023-11-19 11:46:53,230] Trial 8 finished with value: 236.2662957899498 and parameters: {'n_estimators': 166, 'eta': 0.17088794158649134, 'max_depth': 1, 'min_child_weight': 1, 'colsample_bytree': 0.204636133634816, 'colsample_bylevel': 0.2114381362663436, 'colsample_bynode': 0.5365450324352024, 'subsample': 0.7778465452676808, 'lambda': 3.538758864779242, 'alpha': 0.07742116473996247}. Best is trial 5 with value: 141.97707691533307.\n",
      "[I 2023-11-19 11:47:20,466] Trial 9 finished with value: 152.01612877349723 and parameters: {'n_estimators': 59, 'eta': 0.2573955605115437, 'max_depth': 5, 'min_child_weight': 5, 'colsample_bytree': 0.5901564798023388, 'colsample_bylevel': 0.31174220030046296, 'colsample_bynode': 0.33322135755462345, 'subsample': 0.6724696295065364, 'lambda': 0.0012637946338082875, 'alpha': 0.0027012557725439087}. Best is trial 5 with value: 141.97707691533307.\n",
      "[I 2023-11-19 11:48:33,444] Trial 10 finished with value: 256.59517262568505 and parameters: {'n_estimators': 195, 'eta': 0.01027686781014547, 'max_depth': 10, 'min_child_weight': 10, 'colsample_bytree': 0.36321456121327655, 'colsample_bylevel': 0.8084716496554308, 'colsample_bynode': 0.10357222330387059, 'subsample': 0.590962180200284, 'lambda': 0.007077493671122793, 'alpha': 0.02854006015311615}. Best is trial 5 with value: 141.97707691533307.\n",
      "[I 2023-11-19 11:49:04,295] Trial 11 finished with value: 124.64368608515956 and parameters: {'n_estimators': 73, 'eta': 0.3007039456565225, 'max_depth': 3, 'min_child_weight': 6, 'colsample_bytree': 0.5710136534589846, 'colsample_bylevel': 0.8713537531728947, 'colsample_bynode': 0.33954891134400905, 'subsample': 0.5947479630715633, 'lambda': 0.0010741358214098335, 'alpha': 0.0010473363433053052}. Best is trial 11 with value: 124.64368608515956.\n",
      "[I 2023-11-19 11:49:54,200] Trial 12 finished with value: 133.65964396591787 and parameters: {'n_estimators': 192, 'eta': 0.36648366847799646, 'max_depth': 2, 'min_child_weight': 9, 'colsample_bytree': 0.45206457604868155, 'colsample_bylevel': 0.9859968871861825, 'colsample_bynode': 0.3357562089754236, 'subsample': 0.5852557106469622, 'lambda': 0.0056251036036845305, 'alpha': 0.001051482122690196}. Best is trial 11 with value: 124.64368608515956.\n",
      "[I 2023-11-19 11:50:37,168] Trial 13 finished with value: 124.67704329407796 and parameters: {'n_estimators': 138, 'eta': 0.4507753982031469, 'max_depth': 2, 'min_child_weight': 8, 'colsample_bytree': 0.6982004135446469, 'colsample_bylevel': 0.9975728241075645, 'colsample_bynode': 0.3516010854661032, 'subsample': 0.6170444394390245, 'lambda': 0.005749036926066968, 'alpha': 0.001029213642200514}. Best is trial 11 with value: 124.64368608515956.\n",
      "[I 2023-11-19 11:51:20,571] Trial 14 finished with value: 101.72793929020794 and parameters: {'n_estimators': 79, 'eta': 0.11230950992631686, 'max_depth': 2, 'min_child_weight': 6, 'colsample_bytree': 0.7935659734302277, 'colsample_bylevel': 0.9685804279326053, 'colsample_bynode': 0.9661020594950355, 'subsample': 0.6350973408050398, 'lambda': 0.011948591447667889, 'alpha': 0.0010769901674986659}. Best is trial 14 with value: 101.72793929020794.\n",
      "[I 2023-11-19 11:52:15,211] Trial 15 finished with value: 96.0689941241817 and parameters: {'n_estimators': 77, 'eta': 0.1026673913377282, 'max_depth': 3, 'min_child_weight': 4, 'colsample_bytree': 0.9397752932066444, 'colsample_bylevel': 0.6981817606006993, 'colsample_bynode': 0.9668636476961853, 'subsample': 0.5039181099276334, 'lambda': 0.018677451442757308, 'alpha': 0.0068297417731095515}. Best is trial 15 with value: 96.0689941241817.\n",
      "[I 2023-11-19 11:53:17,050] Trial 16 finished with value: 98.46910194301076 and parameters: {'n_estimators': 98, 'eta': 0.09183743678442872, 'max_depth': 3, 'min_child_weight': 4, 'colsample_bytree': 0.9685122875092715, 'colsample_bylevel': 0.6717121343637893, 'colsample_bynode': 0.9222126495738339, 'subsample': 0.5026984039359553, 'lambda': 0.02041560952010118, 'alpha': 0.006574846895579296}. Best is trial 15 with value: 96.0689941241817.\n",
      "[I 2023-11-19 11:54:12,725] Trial 17 finished with value: 103.60292058673372 and parameters: {'n_estimators': 109, 'eta': 0.06520229176643928, 'max_depth': 3, 'min_child_weight': 3, 'colsample_bytree': 0.9640643052227789, 'colsample_bylevel': 0.41534485265831084, 'colsample_bynode': 0.9478822722409825, 'subsample': 0.5116497112558397, 'lambda': 0.03404006338562045, 'alpha': 0.007426994919607327}. Best is trial 15 with value: 96.0689941241817.\n",
      "[I 2023-11-19 11:55:03,154] Trial 18 finished with value: 111.15599529025302 and parameters: {'n_estimators': 98, 'eta': 0.05905431324325636, 'max_depth': 3, 'min_child_weight': 3, 'colsample_bytree': 0.7217939082548545, 'colsample_bylevel': 0.6753762962978832, 'colsample_bynode': 0.7103113830203178, 'subsample': 0.5046487399852135, 'lambda': 0.019666052271835904, 'alpha': 0.007730542505184485}. Best is trial 15 with value: 96.0689941241817.\n",
      "[I 2023-11-19 11:56:04,564] Trial 19 finished with value: 102.5082899416072 and parameters: {'n_estimators': 92, 'eta': 0.16428645186446986, 'max_depth': 4, 'min_child_weight': 4, 'colsample_bytree': 0.9977010788632573, 'colsample_bylevel': 0.5271218933912359, 'colsample_bynode': 0.7704704195852619, 'subsample': 0.5475411401712341, 'lambda': 0.06103726304159701, 'alpha': 0.0335843103080211}. Best is trial 15 with value: 96.0689941241817.\n",
      "[I 2023-11-19 11:56:34,637] Trial 20 finished with value: 157.03752675631378 and parameters: {'n_estimators': 73, 'eta': 0.05519345349489299, 'max_depth': 2, 'min_child_weight': 2, 'colsample_bytree': 0.6849683192273527, 'colsample_bylevel': 0.7251441204416716, 'colsample_bynode': 0.5065593730331497, 'subsample': 0.5023436860642825, 'lambda': 0.014803838681662296, 'alpha': 0.2259189946975513}. Best is trial 15 with value: 96.0689941241817.\n",
      "[I 2023-11-19 11:57:13,942] Trial 21 finished with value: 112.57557808831145 and parameters: {'n_estimators': 78, 'eta': 0.11803426807433694, 'max_depth': 2, 'min_child_weight': 6, 'colsample_bytree': 0.7933352618005858, 'colsample_bylevel': 0.6802981200090746, 'colsample_bynode': 0.9340708195628603, 'subsample': 0.5483048963866319, 'lambda': 0.014053588318403086, 'alpha': 0.004449885866183069}. Best is trial 15 with value: 96.0689941241817.\n",
      "[I 2023-11-19 11:58:15,826] Trial 22 finished with value: 99.68755592204178 and parameters: {'n_estimators': 98, 'eta': 0.09251546386180128, 'max_depth': 3, 'min_child_weight': 4, 'colsample_bytree': 0.8053118876446752, 'colsample_bylevel': 0.7740519586172269, 'colsample_bynode': 0.9791293700344893, 'subsample': 0.6447570467458271, 'lambda': 0.05892121348216465, 'alpha': 0.0024615304190229816}. Best is trial 15 with value: 96.0689941241817.\n",
      "[I 2023-11-19 11:59:15,958] Trial 23 finished with value: 100.82208612287451 and parameters: {'n_estimators': 102, 'eta': 0.08541014314443222, 'max_depth': 3, 'min_child_weight': 3, 'colsample_bytree': 0.8310043753471513, 'colsample_bylevel': 0.7635073160420539, 'colsample_bynode': 0.7602125822501079, 'subsample': 0.5626867547933577, 'lambda': 0.07456351148720756, 'alpha': 0.011255876153359897}. Best is trial 15 with value: 96.0689941241817.\n",
      "[I 2023-11-19 12:00:06,955] Trial 24 finished with value: 115.44462146223186 and parameters: {'n_estimators': 97, 'eta': 0.1650486021112697, 'max_depth': 4, 'min_child_weight': 4, 'colsample_bytree': 0.6118004170988388, 'colsample_bylevel': 0.5638532442866985, 'colsample_bynode': 0.6657782367733275, 'subsample': 0.5339827479146617, 'lambda': 0.03741840332823868, 'alpha': 0.0029469991119492047}. Best is trial 15 with value: 96.0689941241817.\n",
      "[I 2023-11-19 12:00:49,678] Trial 25 finished with value: 124.99142159299195 and parameters: {'n_estimators': 88, 'eta': 0.04443004787632749, 'max_depth': 3, 'min_child_weight': 4, 'colsample_bytree': 0.8316755784362219, 'colsample_bylevel': 0.4393122861393953, 'colsample_bynode': 0.8481846015246542, 'subsample': 0.5005471661852414, 'lambda': 0.09713786448624913, 'alpha': 0.00803897881939762}. Best is trial 15 with value: 96.0689941241817.\n",
      "[I 2023-11-19 12:02:23,840] Trial 26 finished with value: 90.9510268839361 and parameters: {'n_estimators': 119, 'eta': 0.07829437559985686, 'max_depth': 4, 'min_child_weight': 7, 'colsample_bytree': 0.995184692991504, 'colsample_bylevel': 0.7574664749496949, 'colsample_bynode': 0.8208498728089958, 'subsample': 0.6343811730726666, 'lambda': 0.030920390691992282, 'alpha': 0.0027382683167750835}. Best is trial 26 with value: 90.9510268839361.\n",
      "[I 2023-11-19 12:03:50,308] Trial 27 finished with value: 94.36865801027129 and parameters: {'n_estimators': 124, 'eta': 0.06476520305804906, 'max_depth': 4, 'min_child_weight': 13, 'colsample_bytree': 0.9765490191789877, 'colsample_bylevel': 0.5669812955027314, 'colsample_bynode': 0.7919205877948522, 'subsample': 0.5547929461511026, 'lambda': 0.024292489390638158, 'alpha': 0.013717299524522901}. Best is trial 26 with value: 90.9510268839361.\n",
      "[I 2023-11-19 12:04:33,532] Trial 28 finished with value: 235.58091024998976 and parameters: {'n_estimators': 125, 'eta': 0.039967292140180556, 'max_depth': 7, 'min_child_weight': 25, 'colsample_bytree': 0.10486202879482837, 'colsample_bylevel': 0.572695830428853, 'colsample_bynode': 0.6328359447272738, 'subsample': 0.5528671945066161, 'lambda': 0.003469273106494566, 'alpha': 0.014513843430046636}. Best is trial 26 with value: 90.9510268839361.\n",
      "[I 2023-11-19 12:05:27,704] Trial 29 finished with value: 128.15317389159793 and parameters: {'n_estimators': 129, 'eta': 0.06887074837898466, 'max_depth': 4, 'min_child_weight': 12, 'colsample_bytree': 0.6457170492877885, 'colsample_bylevel': 0.48012910248405827, 'colsample_bynode': 0.4519070787639733, 'subsample': 0.6148239154974203, 'lambda': 0.0030850464607638664, 'alpha': 0.04012942880119156}. Best is trial 26 with value: 90.9510268839361.\n",
      "[I 2023-11-19 12:06:16,942] Trial 30 finished with value: 217.51843979397538 and parameters: {'n_estimators': 108, 'eta': 0.04769758130609423, 'max_depth': 8, 'min_child_weight': 8, 'colsample_bytree': 0.5275875928875318, 'colsample_bylevel': 0.10126481540611705, 'colsample_bynode': 0.7844918497213599, 'subsample': 0.5684057911500557, 'lambda': 0.009809872359848294, 'alpha': 0.019559771249277717}. Best is trial 26 with value: 90.9510268839361.\n",
      "[I 2023-11-19 12:08:06,659] Trial 31 finished with value: 91.66625393634041 and parameters: {'n_estimators': 119, 'eta': 0.07477612594295811, 'max_depth': 5, 'min_child_weight': 7, 'colsample_bytree': 0.9989876125262316, 'colsample_bylevel': 0.6525092440204308, 'colsample_bynode': 0.8582550120623295, 'subsample': 0.5210051735412876, 'lambda': 0.022470284967578268, 'alpha': 0.0045317780962322776}. Best is trial 26 with value: 90.9510268839361.\n",
      "[I 2023-11-19 12:09:45,543] Trial 32 finished with value: 94.02429433026892 and parameters: {'n_estimators': 122, 'eta': 0.07433003021466983, 'max_depth': 5, 'min_child_weight': 14, 'colsample_bytree': 0.8842239427192794, 'colsample_bylevel': 0.6153713104524632, 'colsample_bynode': 0.814525536309713, 'subsample': 0.5301163586472044, 'lambda': 0.03326521834254997, 'alpha': 0.004120741460867337}. Best is trial 26 with value: 90.9510268839361.\n",
      "[I 2023-11-19 12:11:10,249] Trial 33 finished with value: 98.61886229974361 and parameters: {'n_estimators': 121, 'eta': 0.06882968153533416, 'max_depth': 5, 'min_child_weight': 14, 'colsample_bytree': 0.7354241898166157, 'colsample_bylevel': 0.6005470299889342, 'colsample_bynode': 0.681210160363336, 'subsample': 0.5280878986296365, 'lambda': 0.04874341152709301, 'alpha': 0.004149026029052219}. Best is trial 26 with value: 90.9510268839361.\n",
      "[I 2023-11-19 12:13:49,536] Trial 34 finished with value: 90.1290692156036 and parameters: {'n_estimators': 135, 'eta': 0.03607835691388184, 'max_depth': 6, 'min_child_weight': 15, 'colsample_bytree': 0.8638239048336498, 'colsample_bylevel': 0.8498184460879808, 'colsample_bynode': 0.8430536760039524, 'subsample': 0.562301562268576, 'lambda': 0.03034661936819694, 'alpha': 0.0017996333947654806}. Best is trial 34 with value: 90.1290692156036.\n",
      "[I 2023-11-19 12:16:00,179] Trial 35 finished with value: 92.70402793863978 and parameters: {'n_estimators': 138, 'eta': 0.031541992213212425, 'max_depth': 6, 'min_child_weight': 16, 'colsample_bytree': 0.8239857819223284, 'colsample_bylevel': 0.8559575099394088, 'colsample_bynode': 0.6092401625807653, 'subsample': 0.5785193548783206, 'lambda': 0.14749670856025138, 'alpha': 0.0018690704667620836}. Best is trial 34 with value: 90.1290692156036.\n",
      "[I 2023-11-19 12:18:21,750] Trial 36 finished with value: 96.06333776907252 and parameters: {'n_estimators': 148, 'eta': 0.033662347807918394, 'max_depth': 7, 'min_child_weight': 17, 'colsample_bytree': 0.6740599529548034, 'colsample_bylevel': 0.8565338061608243, 'colsample_bynode': 0.5944035110444831, 'subsample': 0.5810217180367511, 'lambda': 0.18148763628248693, 'alpha': 0.002068340387953209}. Best is trial 34 with value: 90.1290692156036.\n",
      "[I 2023-11-19 12:20:32,808] Trial 37 finished with value: 93.12640449039986 and parameters: {'n_estimators': 137, 'eta': 0.02983521903234318, 'max_depth': 6, 'min_child_weight': 21, 'colsample_bytree': 0.756682162415391, 'colsample_bylevel': 0.8632714296170076, 'colsample_bynode': 0.6830220218593294, 'subsample': 0.6033425593744643, 'lambda': 0.1200189228435107, 'alpha': 0.0017573385172311112}. Best is trial 34 with value: 90.1290692156036.\n",
      "[I 2023-11-19 12:24:25,419] Trial 38 finished with value: 93.18235621553295 and parameters: {'n_estimators': 164, 'eta': 0.04725068699611203, 'max_depth': 8, 'min_child_weight': 25, 'colsample_bytree': 0.8670648256823609, 'colsample_bylevel': 0.7506033819120906, 'colsample_bynode': 0.843680278536958, 'subsample': 0.6602667464493144, 'lambda': 0.4831192939171036, 'alpha': 0.0019167133582214746}. Best is trial 34 with value: 90.1290692156036.\n",
      "[I 2023-11-19 12:27:32,857] Trial 39 finished with value: 98.53979952050014 and parameters: {'n_estimators': 114, 'eta': 0.01854800758430078, 'max_depth': 10, 'min_child_weight': 10, 'colsample_bytree': 0.8638425101153369, 'colsample_bylevel': 0.9274146658171163, 'colsample_bynode': 0.5251221818778032, 'subsample': 0.5769475744303534, 'lambda': 0.08923883652759651, 'alpha': 0.001672742539786876}. Best is trial 34 with value: 90.1290692156036.\n",
      "[I 2023-11-19 12:29:14,750] Trial 40 finished with value: 102.61309231578808 and parameters: {'n_estimators': 115, 'eta': 0.026947908694227002, 'max_depth': 6, 'min_child_weight': 18, 'colsample_bytree': 0.6231806678108155, 'colsample_bylevel': 0.7971648758408559, 'colsample_bynode': 0.7268408107948394, 'subsample': 0.6879499322749836, 'lambda': 0.12157348092276615, 'alpha': 0.0029658165148476034}. Best is trial 34 with value: 90.1290692156036.\n",
      "[I 2023-11-19 12:31:36,735] Trial 41 finished with value: 92.48880470179809 and parameters: {'n_estimators': 134, 'eta': 0.031142664412000608, 'max_depth': 6, 'min_child_weight': 21, 'colsample_bytree': 0.7650542028410975, 'colsample_bylevel': 0.8697222323562621, 'colsample_bynode': 0.6696520055396493, 'subsample': 0.6022269433164629, 'lambda': 0.2502806351422048, 'alpha': 0.0018507692610355403}. Best is trial 34 with value: 90.1290692156036.\n",
      "[I 2023-11-19 12:34:02,087] Trial 42 finished with value: 94.10518191776093 and parameters: {'n_estimators': 132, 'eta': 0.03834321487214111, 'max_depth': 7, 'min_child_weight': 16, 'colsample_bytree': 0.7563088696956665, 'colsample_bylevel': 0.8971597873867472, 'colsample_bynode': 0.5964223820602695, 'subsample': 0.625302210231928, 'lambda': 0.22384222902562334, 'alpha': 0.004487964939982235}. Best is trial 34 with value: 90.1290692156036.\n",
      "[I 2023-11-19 12:37:24,222] Trial 43 finished with value: 87.13940550417293 and parameters: {'n_estimators': 149, 'eta': 0.034250196880289095, 'max_depth': 6, 'min_child_weight': 20, 'colsample_bytree': 0.9980024824408155, 'colsample_bylevel': 0.8280895773237331, 'colsample_bynode': 0.8676301202799425, 'subsample': 0.6041567777532298, 'lambda': 0.3466785357846091, 'alpha': 0.0014174569495829885}. Best is trial 43 with value: 87.13940550417293.\n",
      "[I 2023-11-19 12:39:43,662] Trial 44 finished with value: 92.61403825777012 and parameters: {'n_estimators': 158, 'eta': 0.020880231250263396, 'max_depth': 5, 'min_child_weight': 12, 'colsample_bytree': 0.9034631828603819, 'colsample_bylevel': 0.6435651739270594, 'colsample_bynode': 0.896649928873177, 'subsample': 0.6035680533105792, 'lambda': 0.9278713270300203, 'alpha': 0.0031721026293326836}. Best is trial 43 with value: 87.13940550417293.\n",
      "[I 2023-11-19 12:43:53,712] Trial 45 finished with value: 88.47596316466267 and parameters: {'n_estimators': 147, 'eta': 0.0532544863929099, 'max_depth': 8, 'min_child_weight': 21, 'colsample_bytree': 0.9906698558280964, 'colsample_bylevel': 0.7819793855927635, 'colsample_bynode': 0.8774885761683312, 'subsample': 0.6496750325662713, 'lambda': 0.32799266933335797, 'alpha': 0.0015196838942502832}. Best is trial 43 with value: 87.13940550417293.\n",
      "[I 2023-11-19 12:48:10,920] Trial 46 finished with value: 92.49447818162035 and parameters: {'n_estimators': 147, 'eta': 0.05124138130080766, 'max_depth': 9, 'min_child_weight': 11, 'colsample_bytree': 0.975770386825904, 'colsample_bylevel': 0.7563684323516353, 'colsample_bynode': 0.8480848263892703, 'subsample': 0.6481487387201825, 'lambda': 0.45430540948832276, 'alpha': 0.0013195229554154577}. Best is trial 43 with value: 87.13940550417293.\n",
      "[I 2023-11-19 12:51:39,508] Trial 47 finished with value: 93.87399686326229 and parameters: {'n_estimators': 182, 'eta': 0.05106015474399351, 'max_depth': 8, 'min_child_weight': 8, 'colsample_bytree': 0.8908496788896966, 'colsample_bylevel': 0.63881032115345, 'colsample_bynode': 0.7271985663486702, 'subsample': 0.6898793184615521, 'lambda': 0.04491610075112862, 'alpha': 0.0013989927834165625}. Best is trial 43 with value: 87.13940550417293.\n",
      "[I 2023-11-19 12:56:56,588] Trial 48 finished with value: 94.66476195206381 and parameters: {'n_estimators': 156, 'eta': 0.023382021628001366, 'max_depth': 10, 'min_child_weight': 20, 'colsample_bytree': 0.6792181474518046, 'colsample_bylevel': 0.9608172528803602, 'colsample_bynode': 0.9963824684247669, 'subsample': 0.6302618868735598, 'lambda': 0.3382035140561886, 'alpha': 0.0050898104723613394}. Best is trial 43 with value: 87.13940550417293.\n",
      "[I 2023-11-19 13:00:03,695] Trial 49 finished with value: 93.56148460686582 and parameters: {'n_estimators': 144, 'eta': 0.08056441044198298, 'max_depth': 7, 'min_child_weight': 15, 'colsample_bytree': 0.993930756261745, 'colsample_bylevel': 0.7127944611697148, 'colsample_bynode': 0.8381354709688202, 'subsample': 0.6510330742910289, 'lambda': 0.06455674945975082, 'alpha': 0.0026951449790054392}. Best is trial 43 with value: 87.13940550417293.\n"
     ]
    }
   ],
   "source": [
    "xgbf = XGBoostForecaster()\n",
    "\n",
    "xgbf.train_model(\n",
    "    train_df=df_train,\n",
    "    target_col=\"target\",\n",
    "    model_name=\"xgboost_enefit\",\n",
    "    exclude_cols=not_feature_columns,\n",
    "    categorical_features=cat_columns\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enefit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
