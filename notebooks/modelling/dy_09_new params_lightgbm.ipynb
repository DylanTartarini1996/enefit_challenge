{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sktime.performance_metrics.forecasting import (MeanAbsoluteScaledError, \n",
    "    MeanAbsolutePercentageError, MeanAbsoluteError, MeanSquaredError)\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "import joblib\n",
    "\n",
    "from typing import Optional, Dict, Tuple, Literal\n",
    "from enefit_challenge.models.forecaster import Forecaster\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "TRACKING_URI = \"http://127.0.0.1:5000/\" # local tracking URI -> launch mlflow before training \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we're going to do two things:\n",
    "1. inform the LightGBMForecaster of the uselessness of some features, according to previous model's iteration feature importance \n",
    "2. define a new subset of parameters to run experiments on, according to mlflow's [parallel coordinates chart](../../assets/lgbm_par_coordinates.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enefit_challenge.utils.dataset import load_enefit_training_data\n",
    "not_feature_columns = ['datetime', 'row_id','prediction_unit_id','date','time', 'data_block_id']\n",
    "cat_columns = ['county', 'product_type']\n",
    "to_drop_cols = [\n",
    "    '10_metre_u_wind_component_mean_f',\n",
    "    '10_metre_v_wind_component_min_f',\n",
    "    'cloudcover_low_mean_f',\n",
    "    'dayofweek_sine',\n",
    "    'direct_solar_radiation_max_f',\n",
    "    'eic_count',\n",
    "    'euros_per_mwh', # not sure about this one\n",
    "    'hour_sine',\n",
    "    'temperature_min_f',\n",
    "    'total_precipitation_max_f',\n",
    "    'week_sine',\n",
    "    '10_metre_u_wind_component_min_f',\n",
    "    '10_metre_v_wind_component_max_f',\n",
    "    '10_metre_v_wind_component_std_f',\n",
    "    'cloudcover_high_mean_f',\n",
    "    'cloudcover_high_std_f',\n",
    "    'cloudcover_low_min_f',\n",
    "    'cloudcover_low_std_f',\n",
    "    'cloudcover_mid_std_f',\n",
    "    'cloudcover_total_std_f',\n",
    "    'county_12', # find a way to drop\n",
    "    'county_3', # find a way to drop\n",
    "    'county_9', # find a way to drop\n",
    "    'direct_solar_radiation_min_f',\n",
    "    'direct_solar_radiation_std_f',\n",
    "    'highest_price_per_mwh', # not sure about this one\n",
    "    # 'installed_capacity', #this one should alspo be dropped?\n",
    "    'month_cosine',\n",
    "    'product_type_3', # find a way to drop\n",
    "    'snowfall_max_f',\n",
    "    'temperature_std_f',\n",
    "    'total_precipitation_min_f',\n",
    "    'total_precipitation_std_f',\n",
    "    # 'year'\n",
    "]\n",
    "df_train = load_enefit_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMForecaster(Forecaster):\n",
    "    \"\"\"\n",
    "        Implementation of a Forecaster using `LGBMRegressor` as base model, \n",
    "        `optuna` for hyperparameters optimization and `mlflow` as backend to track experiments\n",
    "        and register best-in-class model for time series prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self)-> None:\n",
    "        self.tracking_uri = mlflow.set_tracking_uri(TRACKING_URI)\n",
    "        pass\n",
    "\n",
    "    def fit_model(\n",
    "        self,  \n",
    "        X:pd.DataFrame,\n",
    "        y:pd.Series,\n",
    "        params:Optional[Dict]=None,\n",
    "    ) -> LGBMRegressor:\n",
    "        \"\"\"\n",
    "        Trains a `LGBMRegressor`\n",
    "\n",
    "        -------     \n",
    "        params:\n",
    "        -------\n",
    "        `X`:`pd.DataFrame`\n",
    "            Features to use for fitting\n",
    "        `y`:`pd.Series`\n",
    "            Target variable\n",
    "        `params`: `Optional[Dict]`\n",
    "            optional dictionary of parameters to use\n",
    "        -------     \n",
    "        returns:\n",
    "        -------\n",
    "        fitted `LGBMRegressor`\n",
    "        \"\"\"\n",
    "        model = LGBMRegressor(\n",
    "            n_estimators=100, \n",
    "            objective='regression',\n",
    "        )\n",
    "        if params:\n",
    "            model.set_params(**params)\n",
    "\n",
    "        model.fit(X, y)\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def fit_and_test_fold(\n",
    "        self, \n",
    "        params:Dict,\n",
    "        X: pd.DataFrame, \n",
    "        y: pd.Series, \n",
    "        year_month_train, \n",
    "        year_month_test,\n",
    "        experiment_name: str=\"lightgbm\",\n",
    "        artifact_path: str=\"lightgbm_model\",\n",
    "        metrics: list=[\"mae\"]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Used for cross validation on different time splits; \n",
    "        also in charge of logging every experiment run / study trial into the backend.\n",
    "        \"\"\"\n",
    "        \n",
    "        first_dates_month = pd.to_datetime(X[['year', 'month']].assign(day=1))\n",
    "        train_index = first_dates_month.isin(year_month_train)\n",
    "        test_index = first_dates_month.isin(year_month_test)\n",
    "\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        # fit model on training data\n",
    "        mlflow.lightgbm.autolog(log_models=False, log_datasets=False)\n",
    "        model = self.fit_model(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            params\n",
    "        )\n",
    "        \n",
    "        # generate predictions\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        self.signature = infer_signature(X_train, y_test_pred)\n",
    "        MAE = MeanAbsoluteError()\n",
    "        mae = MAE(y_test, y_test_pred)\n",
    "        MASE = MeanAbsoluteScaledError()\n",
    "        mase = MASE(y_test, y_test_pred, y_train=y_train)\n",
    "        MAPE = MeanAbsolutePercentageError()\n",
    "        mape = MAPE(y_test, y_test_pred)\n",
    "        MSE = MeanSquaredError()\n",
    "        mse = MSE(y_test, y_test_pred)\n",
    "        RMSE = MeanSquaredError(square_root=True)\n",
    "        rmse = RMSE(y_test, y_test_pred)\n",
    "\n",
    "        mlflow.lightgbm.log_model(\n",
    "            model, \n",
    "            artifact_path=artifact_path,\n",
    "            signature=self.signature\n",
    "        )\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        return mae, mase, mse, rmse, mape\n",
    "\n",
    "    def train_model(\n",
    "        self, \n",
    "        train_df: pd.DataFrame, \n",
    "        target_col: str,\n",
    "        model_name: str,\n",
    "        exclude_cols: list=[],\n",
    "        categorical_features: list=[],\n",
    "        experiment_name: str=\"lightgbm\",\n",
    "        artifact_path: str=\"lightgbm_model\",\n",
    "        params: Optional[Dict]=None\n",
    "    ) -> None:\n",
    "        \"\"\" \n",
    "        Takes an instance of `LGBMRegressor` model and tracks the hyperparameter tuning\n",
    "        experiment on training set using `mlflow` and `optuna`.  \n",
    "        Registers the best version of the model according to a specified metric (to be implemented).\n",
    "        \n",
    "        -------     \n",
    "        params:\n",
    "        -------\n",
    "        `experiment_name`: `str`\n",
    "            the name of the experiment used to store runs in mlflow, \n",
    "            as well as the name of the optuna study\n",
    "        `model_name`: `str`\n",
    "            the name the final model will have in the registry\n",
    "        `train_df`: `pd.DataFrame`\n",
    "            the training data for the model.\n",
    "        `target_col`: `str`\n",
    "            the time-series target column\n",
    "        `exclude_cols`: `list`  \n",
    "            columns in dataset that should not be used\n",
    "        `categorical_features`: `list`\n",
    "            list of categorical features in the dataset\n",
    "        `artifact_path`: `str`\n",
    "            the path pointing to the mlflow artifact\n",
    "        `params`: `Optional[Dict]`\n",
    "            optional dictionary of parameters to use\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "\n",
    "        if len(categorical_features) > 0: \n",
    "           train_df = pd.get_dummies(train_df, columns=categorical_features)\n",
    "\n",
    "        X = train_df.drop([target_col] + exclude_cols, axis=1)\n",
    "        y = train_df[target_col]\n",
    "        # unique year-month combinations -> to be used in cross-validation\n",
    "        timesteps = np.sort(np.array(\n",
    "            pd.to_datetime(X[['year', 'month']].assign(day=1)).unique().tolist()\n",
    "        ))\n",
    "        \n",
    "        # define mlflow callback Handler for optuna \n",
    "        mlflc = MLflowCallback(\n",
    "            metric_name=[\"MAE\"]\n",
    "        )\n",
    "    \n",
    "        @mlflc.track_in_mlflow() # decorator to allow mlflow logging\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 200, log=True),\n",
    "                'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"gbdt\"]),\n",
    "                'learning_rate': trial.suggest_float('eta', 0.01, 0.2,log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 50, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 1, 25, log=True),\n",
    "                'min_child_weight': trial.suggest_float('min_child_weight', 0.01, 0.1, log=True),\n",
    "                'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.5, 1, log=True),\n",
    "                'subsample': trial.suggest_float(\"subsample\", 0.5, 0.85, log=True),\n",
    "                'reg_alpha': trial.suggest_float('alpha', 1e-3, 0.2, log=True), #L1\n",
    "                'reg_lambda': trial.suggest_float('lambda', 2, 10.0, log=True) #L2\n",
    "            }\n",
    "            cv = TimeSeriesSplit(n_splits=3) # cross validation\n",
    "            cv_mae = [None]*3\n",
    "            cv_mase = [None]*3\n",
    "            cv_mse = [None]*3\n",
    "            cv_rmse = [None]*3\n",
    "            cv_mape = [None]*3\n",
    "            for i, (train_index, test_index) in enumerate(cv.split(timesteps)):\n",
    "                cv_mae[i], cv_mase[i], cv_mse[i], cv_rmse[i], cv_mape[i] = self.fit_and_test_fold(\n",
    "                    params,\n",
    "                    X, \n",
    "                    y, \n",
    "                    timesteps[train_index], \n",
    "                    timesteps[test_index]\n",
    "                )\n",
    "            trial.set_user_attr('split_mae', cv_mae)\n",
    "            trial.set_user_attr('split_mase', cv_mase)\n",
    "            trial.set_user_attr('split_mse', cv_mse)\n",
    "            trial.set_user_attr('split_rmse', cv_rmse)\n",
    "            trial.set_user_attr('split_mape', cv_mape)\n",
    "\n",
    "            mlflow.log_metrics(\n",
    "                {\n",
    "                    \"MAE\":np.mean(cv_mae),\n",
    "                    \"MASE\": np.mean(cv_mase),\n",
    "                    \"MSE\": np.mean(cv_mse),\n",
    "                    \"RMSE\":np.mean(cv_rmse),\n",
    "                    \"MAPE\":np.mean(cv_mape)\n",
    "                }\n",
    "            )\n",
    "            return np.mean(cv_mae) \n",
    "\n",
    "        \n",
    "        sampler = optuna.samplers.TPESampler(\n",
    "            n_startup_trials=10, \n",
    "            seed=0\n",
    "        )\n",
    "\n",
    "        self.study = optuna.create_study(\n",
    "            directions=['minimize'],\n",
    "            sampler=sampler,\n",
    "            study_name=experiment_name\n",
    "        )\n",
    "\n",
    "        self.study.optimize(objective, n_trials=100, timeout= 7200, callbacks=[mlflc]) \n",
    "        \n",
    "        # # search for the best run at the end of the experiment # not implemented now bc of callback bug\n",
    "        # best_run = mlflow.search_runs(max_results=1,order_by=[\"metrics.MAE\"]).run_id\n",
    "        # # register new model version in mlflow\n",
    "        # self.result = mlflow.register_model(\n",
    "        #     model_uri=f\"runs:/{best_run}/{artifact_path}\",\n",
    "        #     name=self.model_name\n",
    "        # )\n",
    "\n",
    "    def forecast(\n",
    "        self, \n",
    "        input_data: pd.DataFrame,\n",
    "        use_best_from_run: bool=True,\n",
    "        use_env_model: Literal[\"Staging\", \"Production\", None]=None,\n",
    "        use_version: int=None\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Fetches a version of the model from the mlflow backend and uses it\n",
    "        to perform prediction on new input data.  \n",
    "        What version is used depends on params settings, \n",
    "        defaults to using the best version from the last experiment run (currently not implemented). \n",
    "        -------     \n",
    "        params:\n",
    "        -------\n",
    "        `input_data`: `pd.DataFrame`\n",
    "            the input data for prediction,\n",
    "              must have the same schema as what's in the model's signature.\n",
    "        `use_best_from_run`: `bool=True`      \n",
    "            use the best model from the current series of iterations, defaults to True\n",
    "        `use_env_model`: `Literal[\"Staging\", \"Production\", None]=None`\n",
    "            use model from a given mlflow environment, defaults to None.  \n",
    "            Said model might come from past iterations, depending on what you decide in the UI\n",
    "        `use_version`: `int=None`\n",
    "            use a previously trained version of the model. \n",
    "            Said version must have been registered from a previous iteration,  \n",
    "            either by the UI or with mlflow's API\n",
    "        \"\"\"\n",
    "        if use_best_from_run:\n",
    "            # not implemented now bc of callback bug\n",
    "            use_prod_model=None\n",
    "            use_version=None\n",
    "        \n",
    "            # model = mlflow.pyfunc.load_model(\n",
    "            #     model_uri=f\"models:/{self.model_name}/{self.result.version}\"\n",
    "            # )\n",
    "            # y_pred = model.predict(input_data)\n",
    "            # return y_pred\n",
    "        \n",
    "        if use_env_model is not None:\n",
    "            use_version = None\n",
    "\n",
    "            model = mlflow.pyfunc.load_model(\n",
    "                # get registered model in given environment\n",
    "                model_uri=f\"models:/{self.model_name}/{use_env_model}\"\n",
    "            )\n",
    "            y_pred = model.predict(input_data)\n",
    "            return y_pred\n",
    "\n",
    "        if use_version is not None:\n",
    "            # get specific registered version of model\n",
    "            model = mlflow.pyfunc.load_model(\n",
    "                model_uri=f\"models:/{self.model_name}/{use_version}\"\n",
    "            )\n",
    "            y_pred = model.predict(input_data)\n",
    "            return y_pred\n",
    "\n",
    "        \n",
    "        if (not use_best_from_run) & (use_env_model is None) & (use_version is None):\n",
    "            return ValueError(\n",
    "                    \"You must specify which kind of LightGBMForecaster you intend to use for prediction\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:13:20,855] A new study created in memory with name: lightgbm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061658 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:13:37,059] Trial 0 finished with value: 61.177352286648954 and parameters: {'n_estimators': 107, 'boosting_type': 'gbdt', 'eta': 0.0852084800119005, 'num_leaves': 35, 'max_depth': 4, 'min_child_weight': 0.02652496376759476, 'colsample_bytree': 0.7823543585516992, 'subsample': 0.630683552411224, 'alpha': 0.11271881751387774, 'lambda': 9.431947170347655}. Best is trial 0 with value: 61.177352286648954.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:13:52,556] Trial 1 finished with value: 67.26341474725164 and parameters: {'n_estimators': 85, 'boosting_type': 'gbdt', 'eta': 0.10716624725460688, 'num_leaves': 32, 'max_depth': 5, 'min_child_weight': 0.08425518534432111, 'colsample_bytree': 0.5252353993848682, 'subsample': 0.523659344156774, 'alpha': 0.0011130716950781927, 'lambda': 7.638468512687414}. Best is trial 0 with value: 61.177352286648954.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:14:11,660] Trial 2 finished with value: 57.91875937787831 and parameters: {'n_estimators': 147, 'boosting_type': 'gbdt', 'eta': 0.13549145341210658, 'num_leaves': 49, 'max_depth': 12, 'min_child_weight': 0.028938722964976897, 'colsample_bytree': 0.8588804134707009, 'subsample': 0.5323854995623077, 'alpha': 0.02968092728356274, 'lambda': 2.519008561636219}. Best is trial 2 with value: 57.91875937787831.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:14:26,296] Trial 3 finished with value: 76.28210654870028 and parameters: {'n_estimators': 186, 'boosting_type': 'gbdt', 'eta': 0.047746365667787276, 'num_leaves': 29, 'max_depth': 1, 'min_child_weight': 0.0594612027220171, 'colsample_bytree': 0.6859381169365696, 'subsample': 0.6760284835300165, 'alpha': 0.0011046784818354066, 'lambda': 5.404290448600519}. Best is trial 2 with value: 57.91875937787831.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:14:44,489] Trial 4 finished with value: 62.34474654229973 and parameters: {'n_estimators': 117, 'boosting_type': 'gbdt', 'eta': 0.06348185887563147, 'num_leaves': 48, 'max_depth': 7, 'min_child_weight': 0.022882733388778425, 'colsample_bytree': 0.6769081319858106, 'subsample': 0.7239996578505465, 'alpha': 0.001375880186894093, 'lambda': 5.84897721635556}. Best is trial 2 with value: 57.91875937787831.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013955 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051083 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:14:58,441] Trial 5 finished with value: 93.63055127154895 and parameters: {'n_estimators': 126, 'boosting_type': 'gbdt', 'eta': 0.01878079717563113, 'num_leaves': 22, 'max_depth': 2, 'min_child_weight': 0.023105255265581114, 'colsample_bytree': 0.7423630299423081, 'subsample': 0.6310230886383932, 'alpha': 0.18805195308138078, 'lambda': 2.356981954384445}. Best is trial 2 with value: 57.91875937787831.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014259 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:15:09,672] Trial 6 finished with value: 255.38083042254917 and parameters: {'n_estimators': 66, 'boosting_type': 'gbdt', 'eta': 0.016213191836360573, 'num_leaves': 36, 'max_depth': 1, 'min_child_weight': 0.029262455944124445, 'colsample_bytree': 0.5923105105111744, 'subsample': 0.5440068564604433, 'alpha': 0.0017946365588584771, 'lambda': 5.751547246142357}. Best is trial 2 with value: 57.91875937787831.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:15:22,959] Trial 7 finished with value: 143.4524771675743 and parameters: {'n_estimators': 60, 'boosting_type': 'gbdt', 'eta': 0.01802019761039029, 'num_leaves': 28, 'max_depth': 13, 'min_child_weight': 0.012505506197842537, 'colsample_bytree': 0.8937510318451007, 'subsample': 0.5261575162884858, 'alpha': 0.17654788473821764, 'lambda': 4.252096656371395}. Best is trial 2 with value: 57.91875937787831.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:15:38,859] Trial 8 finished with value: 78.80385617079062 and parameters: {'n_estimators': 194, 'boosting_type': 'gbdt', 'eta': 0.061224065356728224, 'num_leaves': 39, 'max_depth': 1, 'min_child_weight': 0.019178161105195695, 'colsample_bytree': 0.5434414678644279, 'subsample': 0.5850799201270402, 'alpha': 0.0018758410807197005, 'lambda': 3.3364967023098626}. Best is trial 2 with value: 57.91875937787831.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:15:54,772] Trial 9 finished with value: 148.43251985752102 and parameters: {'n_estimators': 88, 'boosting_type': 'gbdt', 'eta': 0.01211874973417475, 'num_leaves': 38, 'max_depth': 5, 'min_child_weight': 0.01842423612626269, 'colsample_bytree': 0.7185936308872152, 'subsample': 0.5255553890596194, 'alpha': 0.02114804189919046, 'lambda': 8.924423331132585}. Best is trial 2 with value: 57.91875937787831.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063595 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:16:13,575] Trial 10 finished with value: 54.98517604201817 and parameters: {'n_estimators': 149, 'boosting_type': 'gbdt', 'eta': 0.19161164510536693, 'num_leaves': 48, 'max_depth': 19, 'min_child_weight': 0.04469523068011732, 'colsample_bytree': 0.9905610397995873, 'subsample': 0.7863384072097491, 'alpha': 0.028306417231634785, 'lambda': 2.042447099346659}. Best is trial 10 with value: 54.98517604201817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:16:32,885] Trial 11 finished with value: 56.80056780716919 and parameters: {'n_estimators': 153, 'boosting_type': 'gbdt', 'eta': 0.1742103278992428, 'num_leaves': 50, 'max_depth': 25, 'min_child_weight': 0.04626489205423121, 'colsample_bytree': 0.9678794335595072, 'subsample': 0.8406058452973463, 'alpha': 0.025121340380418467, 'lambda': 2.0180968783445676}. Best is trial 10 with value: 54.98517604201817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:16:52,034] Trial 12 finished with value: 56.17155901693871 and parameters: {'n_estimators': 147, 'boosting_type': 'gbdt', 'eta': 0.19492234881064777, 'num_leaves': 50, 'max_depth': 22, 'min_child_weight': 0.045333068856824366, 'colsample_bytree': 0.9820274837389471, 'subsample': 0.8367443278736354, 'alpha': 0.009647167901136115, 'lambda': 2.0255896049682742}. Best is trial 10 with value: 54.98517604201817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:17:10,486] Trial 13 finished with value: 58.29770107773917 and parameters: {'n_estimators': 145, 'boosting_type': 'gbdt', 'eta': 0.18949007173508128, 'num_leaves': 43, 'max_depth': 23, 'min_child_weight': 0.04179480916051598, 'colsample_bytree': 0.9843876914900025, 'subsample': 0.8469148562292328, 'alpha': 0.007837935795689173, 'lambda': 2.013017122064861}. Best is trial 10 with value: 54.98517604201817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:17:30,279] Trial 14 finished with value: 58.202662541690664 and parameters: {'n_estimators': 168, 'boosting_type': 'gbdt', 'eta': 0.19303414795125526, 'num_leaves': 42, 'max_depth': 13, 'min_child_weight': 0.045847904133583174, 'colsample_bytree': 0.8815591530292018, 'subsample': 0.7852858754268979, 'alpha': 0.008252758871173204, 'lambda': 2.745633906959694}. Best is trial 10 with value: 54.98517604201817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:17:48,447] Trial 15 finished with value: 53.38855566335648 and parameters: {'n_estimators': 129, 'boosting_type': 'gbdt', 'eta': 0.11835306637536634, 'num_leaves': 44, 'max_depth': 25, 'min_child_weight': 0.06595710482440453, 'colsample_bytree': 0.9888359546569474, 'subsample': 0.7676981609938867, 'alpha': 0.05932591256744542, 'lambda': 3.1052312469131715}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:18:10,554] Trial 16 finished with value: 57.84436547464053 and parameters: {'n_estimators': 124, 'boosting_type': 'gbdt', 'eta': 0.10670123925280477, 'num_leaves': 44, 'max_depth': 8, 'min_child_weight': 0.09680586758159869, 'colsample_bytree': 0.807271736441217, 'subsample': 0.7487230705775093, 'alpha': 0.05921896872659023, 'lambda': 3.144982705057178}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:18:26,293] Trial 17 finished with value: 54.64485501338311 and parameters: {'n_estimators': 100, 'boosting_type': 'gbdt', 'eta': 0.13481598096974373, 'num_leaves': 44, 'max_depth': 16, 'min_child_weight': 0.06740801577855261, 'colsample_bytree': 0.9206014926075726, 'subsample': 0.7071634855650185, 'alpha': 0.05908576848026663, 'lambda': 3.3573591786794172}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018586 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:18:42,454] Trial 18 finished with value: 62.83341639378895 and parameters: {'n_estimators': 95, 'boosting_type': 'gbdt', 'eta': 0.03397477239992321, 'num_leaves': 41, 'max_depth': 9, 'min_child_weight': 0.06639849202791576, 'colsample_bytree': 0.9184196748144178, 'subsample': 0.6896772401471618, 'alpha': 0.06885007880350086, 'lambda': 3.724818189115957}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:18:56,939] Trial 19 finished with value: 56.20768798437569 and parameters: {'n_estimators': 82, 'boosting_type': 'gbdt', 'eta': 0.12373958328091654, 'num_leaves': 45, 'max_depth': 15, 'min_child_weight': 0.0729981016409297, 'colsample_bytree': 0.8355630952760592, 'subsample': 0.7277711101065882, 'alpha': 0.060556882509067676, 'lambda': 2.9976185915864013}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:19:13,499] Trial 20 finished with value: 53.86583827584525 and parameters: {'n_estimators': 103, 'boosting_type': 'gbdt', 'eta': 0.08288870555697492, 'num_leaves': 40, 'max_depth': 18, 'min_child_weight': 0.06054753754088572, 'colsample_bytree': 0.9148270140939067, 'subsample': 0.6922318959406989, 'alpha': 0.09883298377055871, 'lambda': 3.9223851584489053}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:19:31,081] Trial 21 finished with value: 53.88464209808342 and parameters: {'n_estimators': 105, 'boosting_type': 'gbdt', 'eta': 0.08646177477887693, 'num_leaves': 40, 'max_depth': 18, 'min_child_weight': 0.05934981886233259, 'colsample_bytree': 0.9220577140986127, 'subsample': 0.6779746183251312, 'alpha': 0.09655395875327007, 'lambda': 3.8889796175704947}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034427 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:19:47,681] Trial 22 finished with value: 53.62694090496518 and parameters: {'n_estimators': 114, 'boosting_type': 'gbdt', 'eta': 0.08381187182100432, 'num_leaves': 39, 'max_depth': 24, 'min_child_weight': 0.056473113143026575, 'colsample_bytree': 0.9301960820680982, 'subsample': 0.6537017949764191, 'alpha': 0.11525252073505185, 'lambda': 4.1239788265197985}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:20:04,124] Trial 23 finished with value: 54.44803627897383 and parameters: {'n_estimators': 116, 'boosting_type': 'gbdt', 'eta': 0.07479407744045255, 'num_leaves': 35, 'max_depth': 24, 'min_child_weight': 0.0788324116461236, 'colsample_bytree': 0.8411956082282958, 'subsample': 0.6465906471400199, 'alpha': 0.11851545950263816, 'lambda': 4.653276473909019}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:20:23,077] Trial 24 finished with value: 53.573432580555014 and parameters: {'n_estimators': 130, 'boosting_type': 'gbdt', 'eta': 0.10086955514897522, 'num_leaves': 38, 'max_depth': 10, 'min_child_weight': 0.09664353532651561, 'colsample_bytree': 0.9361343261999412, 'subsample': 0.6626227518451218, 'alpha': 0.19812607233304294, 'lambda': 3.6374002424796856}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:20:41,267] Trial 25 finished with value: 54.60998881109591 and parameters: {'n_estimators': 131, 'boosting_type': 'gbdt', 'eta': 0.1067364158751428, 'num_leaves': 37, 'max_depth': 12, 'min_child_weight': 0.09560678189039701, 'colsample_bytree': 0.9470997853385363, 'subsample': 0.6052931704623356, 'alpha': 0.19887312591820835, 'lambda': 4.775809607851641}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:20:59,089] Trial 26 finished with value: 55.073305665247766 and parameters: {'n_estimators': 116, 'boosting_type': 'gbdt', 'eta': 0.14346910263624463, 'num_leaves': 33, 'max_depth': 10, 'min_child_weight': 0.08552523889187312, 'colsample_bytree': 0.8734242880052995, 'subsample': 0.6640395673959193, 'alpha': 0.04062399918679425, 'lambda': 3.6469728589775685}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:21:20,163] Trial 27 finished with value: 54.45046684534706 and parameters: {'n_estimators': 135, 'boosting_type': 'gbdt', 'eta': 0.04561445234699262, 'num_leaves': 39, 'max_depth': 25, 'min_child_weight': 0.09846178295002798, 'colsample_bytree': 0.9967563499903018, 'subsample': 0.6493954304930064, 'alpha': 0.14535851466514488, 'lambda': 3.0994247884462744}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:21:42,918] Trial 28 finished with value: 53.956654006890005 and parameters: {'n_estimators': 174, 'boosting_type': 'gbdt', 'eta': 0.10059569950871354, 'num_leaves': 45, 'max_depth': 17, 'min_child_weight': 0.054371581986615194, 'colsample_bytree': 0.9447728501281979, 'subsample': 0.5803945230831797, 'alpha': 0.13742231762133028, 'lambda': 4.376924451911433}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:22:01,326] Trial 29 finished with value: 56.83604113227157 and parameters: {'n_estimators': 111, 'boosting_type': 'gbdt', 'eta': 0.06911138807712451, 'num_leaves': 34, 'max_depth': 7, 'min_child_weight': 0.07470524972331292, 'colsample_bytree': 0.792401893887451, 'subsample': 0.6252619417035458, 'alpha': 0.10054018279967217, 'lambda': 2.8699529417997973}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:22:19,325] Trial 30 finished with value: 55.68372244469361 and parameters: {'n_estimators': 130, 'boosting_type': 'gbdt', 'eta': 0.0884445845226749, 'num_leaves': 37, 'max_depth': 14, 'min_child_weight': 0.0357005139483699, 'colsample_bytree': 0.8334924854191612, 'subsample': 0.6523396663299638, 'alpha': 0.09225967031048411, 'lambda': 3.4718374307618847}. Best is trial 15 with value: 53.38855566335648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:22:36,140] Trial 31 finished with value: 53.37360277714729 and parameters: {'n_estimators': 103, 'boosting_type': 'gbdt', 'eta': 0.08000465289174659, 'num_leaves': 40, 'max_depth': 18, 'min_child_weight': 0.06164709979538492, 'colsample_bytree': 0.9009694189863615, 'subsample': 0.6958497841413332, 'alpha': 0.12016359854410269, 'lambda': 4.033485445377736}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:22:53,501] Trial 32 finished with value: 53.57014166843581 and parameters: {'n_estimators': 110, 'boosting_type': 'gbdt', 'eta': 0.09312049613789429, 'num_leaves': 36, 'max_depth': 19, 'min_child_weight': 0.08440472983404611, 'colsample_bytree': 0.9427749325377286, 'subsample': 0.7002144970305618, 'alpha': 0.14077003405750035, 'lambda': 4.0600968320742234}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:23:09,504] Trial 33 finished with value: 54.8134549042781 and parameters: {'n_estimators': 106, 'boosting_type': 'gbdt', 'eta': 0.11363443076646772, 'num_leaves': 35, 'max_depth': 11, 'min_child_weight': 0.07607000618428356, 'colsample_bytree': 0.880479201058452, 'subsample': 0.7082016103259637, 'alpha': 0.1424409155207572, 'lambda': 3.4602675777419343}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:23:25,308] Trial 34 finished with value: 56.58176019272248 and parameters: {'n_estimators': 97, 'boosting_type': 'gbdt', 'eta': 0.14708327246684558, 'num_leaves': 42, 'max_depth': 18, 'min_child_weight': 0.0874320561800505, 'colsample_bytree': 0.9541300306926843, 'subsample': 0.7559983704391322, 'alpha': 0.07722171375593483, 'lambda': 3.791667464812652}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019915 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:23:43,836] Trial 35 finished with value: 54.32661157626336 and parameters: {'n_estimators': 121, 'boosting_type': 'gbdt', 'eta': 0.05760188484681603, 'num_leaves': 31, 'max_depth': 11, 'min_child_weight': 0.08470521916111737, 'colsample_bytree': 0.8887308585399359, 'subsample': 0.6790662904558177, 'alpha': 0.15598278409175198, 'lambda': 5.146800409190466}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:24:03,000] Trial 36 finished with value: 57.73708003967107 and parameters: {'n_estimators': 139, 'boosting_type': 'gbdt', 'eta': 0.09588745036039988, 'num_leaves': 46, 'max_depth': 15, 'min_child_weight': 0.06832261924353422, 'colsample_bytree': 0.7674206878746911, 'subsample': 0.6992037626124492, 'alpha': 0.1972580569326638, 'lambda': 4.092979784999064}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:24:20,019] Trial 37 finished with value: 55.263509934694376 and parameters: {'n_estimators': 125, 'boosting_type': 'gbdt', 'eta': 0.11918262913994936, 'num_leaves': 36, 'max_depth': 20, 'min_child_weight': 0.08813760837999489, 'colsample_bytree': 0.8534521832216213, 'subsample': 0.7319162558740182, 'alpha': 0.04558576449256136, 'lambda': 2.7314703946435195}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:24:37,391] Trial 38 finished with value: 54.75303002087006 and parameters: {'n_estimators': 109, 'boosting_type': 'gbdt', 'eta': 0.0747149116234309, 'num_leaves': 41, 'max_depth': 9, 'min_child_weight': 0.07777542712559175, 'colsample_bytree': 0.9587116194449279, 'subsample': 0.6701784346548181, 'alpha': 0.08032441804033791, 'lambda': 4.492275725734135}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:24:57,257] Trial 39 finished with value: 54.97459110071657 and parameters: {'n_estimators': 136, 'boosting_type': 'gbdt', 'eta': 0.05322609652579089, 'num_leaves': 38, 'max_depth': 13, 'min_child_weight': 0.05191739281147587, 'colsample_bytree': 0.8106748894188585, 'subsample': 0.7158844121618333, 'alpha': 0.1263284754851773, 'lambda': 6.376846014044065}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018564 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:25:10,151] Trial 40 finished with value: 60.99268015592727 and parameters: {'n_estimators': 90, 'boosting_type': 'gbdt', 'eta': 0.15436805009761592, 'num_leaves': 47, 'max_depth': 3, 'min_child_weight': 0.06493132047242965, 'colsample_bytree': 0.9947721773896749, 'subsample': 0.6960524907019222, 'alpha': 0.15483481061222887, 'lambda': 3.2868570062542326}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:25:27,279] Trial 41 finished with value: 55.28461159243982 and parameters: {'n_estimators': 120, 'boosting_type': 'gbdt', 'eta': 0.09507930962024208, 'num_leaves': 38, 'max_depth': 21, 'min_child_weight': 0.058523710897009334, 'colsample_bytree': 0.9349674691575978, 'subsample': 0.6568149851283323, 'alpha': 0.10252344316669748, 'lambda': 4.133981482536074}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:25:45,720] Trial 42 finished with value: 54.60050105368672 and parameters: {'n_estimators': 114, 'boosting_type': 'gbdt', 'eta': 0.07832693104242135, 'num_leaves': 42, 'max_depth': 20, 'min_child_weight': 0.05235515826611064, 'colsample_bytree': 0.904769529195425, 'subsample': 0.6326012879492586, 'alpha': 0.11691119328711776, 'lambda': 4.196938658621599}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:26:03,314] Trial 43 finished with value: 54.365704150302555 and parameters: {'n_estimators': 109, 'boosting_type': 'gbdt', 'eta': 0.06493283653622303, 'num_leaves': 40, 'max_depth': 25, 'min_child_weight': 0.07306932650379364, 'colsample_bytree': 0.8628298995815826, 'subsample': 0.6711822622131701, 'alpha': 0.16937767303537868, 'lambda': 3.6924529914939823}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:26:21,476] Trial 44 finished with value: 54.19350929344373 and parameters: {'n_estimators': 124, 'boosting_type': 'gbdt', 'eta': 0.1174515192515309, 'num_leaves': 47, 'max_depth': 16, 'min_child_weight': 0.09036367641695615, 'colsample_bytree': 0.9556259459995657, 'subsample': 0.734827731278136, 'alpha': 0.08166665442722534, 'lambda': 5.010416823782709}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:26:36,468] Trial 45 finished with value: 54.82462573401651 and parameters: {'n_estimators': 83, 'boosting_type': 'gbdt', 'eta': 0.09223142719650616, 'num_leaves': 36, 'max_depth': 21, 'min_child_weight': 0.09989108866200339, 'colsample_bytree': 0.9050139497642558, 'subsample': 0.6862436912972231, 'alpha': 0.1969658115813101, 'lambda': 4.363174718920495}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:26:54,160] Trial 46 finished with value: 53.48013477053494 and parameters: {'n_estimators': 113, 'boosting_type': 'gbdt', 'eta': 0.07014013003676381, 'num_leaves': 43, 'max_depth': 14, 'min_child_weight': 0.0804755779765577, 'colsample_bytree': 0.9633500380643034, 'subsample': 0.711096138006016, 'alpha': 0.13475299719114825, 'lambda': 4.037910694727659}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:27:15,678] Trial 47 finished with value: 54.010724042008576 and parameters: {'n_estimators': 158, 'boosting_type': 'gbdt', 'eta': 0.06972499158543213, 'num_leaves': 48, 'max_depth': 14, 'min_child_weight': 0.0822007909097802, 'colsample_bytree': 0.9986110879115114, 'subsample': 0.7130212245186466, 'alpha': 0.126683439906139, 'lambda': 3.591548995112063}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037266 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:27:35,281] Trial 48 finished with value: 59.72645515359191 and parameters: {'n_estimators': 130, 'boosting_type': 'gbdt', 'eta': 0.12547834881376227, 'num_leaves': 43, 'max_depth': 6, 'min_child_weight': 0.08147192906254576, 'colsample_bytree': 0.9619976879161345, 'subsample': 0.7474032944704053, 'alpha': 0.1524322753225186, 'lambda': 3.2646555438947873}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:27:54,375] Trial 49 finished with value: 53.93241222873582 and parameters: {'n_estimators': 140, 'boosting_type': 'gbdt', 'eta': 0.10778479070792901, 'num_leaves': 45, 'max_depth': 12, 'min_child_weight': 0.06445721130973946, 'colsample_bytree': 0.8835223663315759, 'subsample': 0.7785295056176044, 'alpha': 0.07273923360721515, 'lambda': 3.8916279859474088}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:28:10,364] Trial 50 finished with value: 54.86090107456139 and parameters: {'n_estimators': 101, 'boosting_type': 'gbdt', 'eta': 0.16270056111635364, 'num_leaves': 43, 'max_depth': 10, 'min_child_weight': 0.07279878803410626, 'colsample_bytree': 0.959722897114774, 'subsample': 0.7058869879366606, 'alpha': 0.045649219271948135, 'lambda': 4.726007358324293}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:28:28,953] Trial 51 finished with value: 54.58282579683612 and parameters: {'n_estimators': 114, 'boosting_type': 'gbdt', 'eta': 0.08217124359747542, 'num_leaves': 39, 'max_depth': 17, 'min_child_weight': 0.09046278204270201, 'colsample_bytree': 0.9377112836384134, 'subsample': 0.6878601203499516, 'alpha': 0.12146540819090107, 'lambda': 3.4739852258327515}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:28:47,470] Trial 52 finished with value: 54.715485572510204 and parameters: {'n_estimators': 119, 'boosting_type': 'gbdt', 'eta': 0.09713274561558057, 'num_leaves': 41, 'max_depth': 21, 'min_child_weight': 0.06939672723955129, 'colsample_bytree': 0.9001119053971995, 'subsample': 0.6720531516874856, 'alpha': 0.10657968513921913, 'lambda': 4.083592656062069}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062728 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:29:04,821] Trial 53 finished with value: 54.05037100418948 and parameters: {'n_estimators': 109, 'boosting_type': 'gbdt', 'eta': 0.060586401406412624, 'num_leaves': 37, 'max_depth': 15, 'min_child_weight': 0.06322114330084894, 'colsample_bytree': 0.9772648518925936, 'subsample': 0.7209723724125231, 'alpha': 0.16804844783561376, 'lambda': 4.452731441710432}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:29:21,728] Trial 54 finished with value: 53.811272722200194 and parameters: {'n_estimators': 94, 'boosting_type': 'gbdt', 'eta': 0.07691496140575506, 'num_leaves': 39, 'max_depth': 25, 'min_child_weight': 0.05522208678306336, 'colsample_bytree': 0.926660854288198, 'subsample': 0.7685047549859642, 'alpha': 0.09294675142646032, 'lambda': 3.9359623071690346}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:29:40,535] Trial 55 finished with value: 54.179433882501655 and parameters: {'n_estimators': 104, 'boosting_type': 'gbdt', 'eta': 0.13774208889830403, 'num_leaves': 50, 'max_depth': 18, 'min_child_weight': 0.07890674212563578, 'colsample_bytree': 0.9740398956333072, 'subsample': 0.8042577967697482, 'alpha': 0.1196181004928442, 'lambda': 3.608827327680729}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:30:00,006] Trial 56 finished with value: 53.58218221637992 and parameters: {'n_estimators': 126, 'boosting_type': 'gbdt', 'eta': 0.08581151788157457, 'num_leaves': 41, 'max_depth': 22, 'min_child_weight': 0.06928687958406912, 'colsample_bytree': 0.9320555291796153, 'subsample': 0.7363732402940804, 'alpha': 0.16806981087186776, 'lambda': 3.2079250753947335}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:30:21,217] Trial 57 finished with value: 53.933348466373495 and parameters: {'n_estimators': 146, 'boosting_type': 'gbdt', 'eta': 0.08890504901143566, 'num_leaves': 44, 'max_depth': 20, 'min_child_weight': 0.06967566292697044, 'colsample_bytree': 0.9077783452938973, 'subsample': 0.7396446620922517, 'alpha': 0.17118028702025773, 'lambda': 3.189737569443678}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:30:40,024] Trial 58 finished with value: 54.31601280502756 and parameters: {'n_estimators': 126, 'boosting_type': 'gbdt', 'eta': 0.1257688926531304, 'num_leaves': 42, 'max_depth': 14, 'min_child_weight': 0.09218223706845767, 'colsample_bytree': 0.9755313747624696, 'subsample': 0.7165578950050946, 'alpha': 0.06404621151209966, 'lambda': 3.0362671079765784}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019493 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:31:00,645] Trial 59 finished with value: 55.39650573234932 and parameters: {'n_estimators': 133, 'boosting_type': 'gbdt', 'eta': 0.10467290438867878, 'num_leaves': 46, 'max_depth': 16, 'min_child_weight': 0.08111217119067882, 'colsample_bytree': 0.9347426309595148, 'subsample': 0.7571666824692189, 'alpha': 0.14886647679223544, 'lambda': 2.6142910158673893}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:31:17,516] Trial 60 finished with value: 56.35882281538258 and parameters: {'n_estimators': 99, 'boosting_type': 'gbdt', 'eta': 0.05287639329529847, 'num_leaves': 34, 'max_depth': 13, 'min_child_weight': 0.060886423272975075, 'colsample_bytree': 0.8650363271877685, 'subsample': 0.726868815198144, 'alpha': 0.17912935286436749, 'lambda': 2.9340948189706406}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041573 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063969 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:31:37,065] Trial 61 finished with value: 53.50357872186222 and parameters: {'n_estimators': 118, 'boosting_type': 'gbdt', 'eta': 0.0832561109314305, 'num_leaves': 41, 'max_depth': 22, 'min_child_weight': 0.07083140259872579, 'colsample_bytree': 0.9245998330173343, 'subsample': 0.6839834253646397, 'alpha': 0.12738275624949086, 'lambda': 3.3930797896499585}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:31:56,640] Trial 62 finished with value: 54.01984717554495 and parameters: {'n_estimators': 122, 'boosting_type': 'gbdt', 'eta': 0.06952275875610855, 'num_leaves': 40, 'max_depth': 22, 'min_child_weight': 0.0695661423653585, 'colsample_bytree': 0.8990052666598197, 'subsample': 0.6953719849249161, 'alpha': 0.1360062658490921, 'lambda': 3.261096514605511}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042549 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:32:15,186] Trial 63 finished with value: 54.26885042125329 and parameters: {'n_estimators': 118, 'boosting_type': 'gbdt', 'eta': 0.08759410043967758, 'num_leaves': 44, 'max_depth': 19, 'min_child_weight': 0.09971383934152794, 'colsample_bytree': 0.9159826562571828, 'subsample': 0.7071334106519557, 'alpha': 0.1930185422458951, 'lambda': 3.446820391336557}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:32:32,677] Trial 64 finished with value: 53.53363422652237 and parameters: {'n_estimators': 112, 'boosting_type': 'gbdt', 'eta': 0.1048701133856117, 'num_leaves': 41, 'max_depth': 22, 'min_child_weight': 0.07612832572634665, 'colsample_bytree': 0.9454587326327308, 'subsample': 0.6801788415464226, 'alpha': 0.09113118347438631, 'lambda': 3.070379556205202}. Best is trial 31 with value: 53.37360277714729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:32:52,285] Trial 65 finished with value: 52.64828710800964 and parameters: {'n_estimators': 112, 'boosting_type': 'gbdt', 'eta': 0.10323888877064927, 'num_leaves': 38, 'max_depth': 17, 'min_child_weight': 0.09108033274194423, 'colsample_bytree': 0.9983901007173711, 'subsample': 0.6859431367461181, 'alpha': 0.0850281852357554, 'lambda': 3.8036388739986937}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:33:10,596] Trial 66 finished with value: 54.00130667438046 and parameters: {'n_estimators': 106, 'boosting_type': 'gbdt', 'eta': 0.11060668092749866, 'num_leaves': 43, 'max_depth': 17, 'min_child_weight': 0.08475943787806262, 'colsample_bytree': 0.9976486306842007, 'subsample': 0.6802704884121166, 'alpha': 0.08689684191163188, 'lambda': 3.8531345201569946}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:33:28,338] Trial 67 finished with value: 54.349158043258264 and parameters: {'n_estimators': 113, 'boosting_type': 'gbdt', 'eta': 0.1249224585092244, 'num_leaves': 40, 'max_depth': 23, 'min_child_weight': 0.07688863056906234, 'colsample_bytree': 0.9710168350205719, 'subsample': 0.6973851379325026, 'alpha': 0.06880156388562951, 'lambda': 3.0772972384731774}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:33:45,313] Trial 68 finished with value: 53.944840709650826 and parameters: {'n_estimators': 103, 'boosting_type': 'gbdt', 'eta': 0.1001073782255279, 'num_leaves': 42, 'max_depth': 19, 'min_child_weight': 0.06255178257931483, 'colsample_bytree': 0.9800863762158512, 'subsample': 0.6855102005171714, 'alpha': 0.08882124013320618, 'lambda': 2.7985769563908396}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:34:02,014] Trial 69 finished with value: 53.82801661939354 and parameters: {'n_estimators': 110, 'boosting_type': 'gbdt', 'eta': 0.13547070563547967, 'num_leaves': 37, 'max_depth': 16, 'min_child_weight': 0.09312846892322282, 'colsample_bytree': 0.9437203154499424, 'subsample': 0.6667647266229392, 'alpha': 0.05714490778312852, 'lambda': 2.462978823194209}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064334 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:34:19,032] Trial 70 finished with value: 53.84245571527557 and parameters: {'n_estimators': 96, 'boosting_type': 'gbdt', 'eta': 0.07885954129025888, 'num_leaves': 49, 'max_depth': 25, 'min_child_weight': 0.08594423021929173, 'colsample_bytree': 0.8775486674344095, 'subsample': 0.7066540202788922, 'alpha': 0.10340497000275, 'lambda': 3.0005579358286174}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:34:36,568] Trial 71 finished with value: 54.185479849759055 and parameters: {'n_estimators': 119, 'boosting_type': 'gbdt', 'eta': 0.10372495925369304, 'num_leaves': 38, 'max_depth': 18, 'min_child_weight': 0.09165788106487975, 'colsample_bytree': 0.9512880619056507, 'subsample': 0.6635150801452189, 'alpha': 0.13381295275198324, 'lambda': 3.646377677672771}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:34:54,899] Trial 72 finished with value: 54.29635689940179 and parameters: {'n_estimators': 114, 'boosting_type': 'gbdt', 'eta': 0.11426243036685374, 'num_leaves': 39, 'max_depth': 15, 'min_child_weight': 0.07435793562991859, 'colsample_bytree': 0.9231813742682955, 'subsample': 0.684261266718575, 'alpha': 0.11061381380393576, 'lambda': 3.3585470214377793}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:35:13,060] Trial 73 finished with value: 54.404342125268926 and parameters: {'n_estimators': 129, 'boosting_type': 'gbdt', 'eta': 0.09391898173083862, 'num_leaves': 36, 'max_depth': 22, 'min_child_weight': 0.08180962586942983, 'colsample_bytree': 0.9987221124422616, 'subsample': 0.7247047954457084, 'alpha': 0.08129161705359356, 'lambda': 3.957066307150263}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022857 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:35:29,994] Trial 74 finished with value: 54.85839792203827 and parameters: {'n_estimators': 101, 'boosting_type': 'gbdt', 'eta': 0.0737201739909199, 'num_leaves': 40, 'max_depth': 19, 'min_child_weight': 0.07765794416376313, 'colsample_bytree': 0.9689502381468136, 'subsample': 0.6434954574491927, 'alpha': 0.1395408032239048, 'lambda': 3.536565118201769}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036220 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:35:47,057] Trial 75 finished with value: 54.700312145000304 and parameters: {'n_estimators': 108, 'boosting_type': 'gbdt', 'eta': 0.1019004857753426, 'num_leaves': 41, 'max_depth': 12, 'min_child_weight': 0.09587948804504919, 'colsample_bytree': 0.9481932099111174, 'subsample': 0.6737376673669867, 'alpha': 0.11083739030882177, 'lambda': 3.746604365998442}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:36:04,857] Trial 76 finished with value: 54.33142788017236 and parameters: {'n_estimators': 117, 'boosting_type': 'gbdt', 'eta': 0.08330812775955468, 'num_leaves': 38, 'max_depth': 23, 'min_child_weight': 0.06487960036755543, 'colsample_bytree': 0.8900022862307988, 'subsample': 0.6588399235473004, 'alpha': 0.09240470050265912, 'lambda': 4.176434547781216}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:36:24,110] Trial 77 finished with value: 53.60736072820802 and parameters: {'n_estimators': 138, 'boosting_type': 'gbdt', 'eta': 0.09371243244638322, 'num_leaves': 43, 'max_depth': 20, 'min_child_weight': 0.07319445216804614, 'colsample_bytree': 0.9141518545289906, 'subsample': 0.701994078526579, 'alpha': 0.07987041956281254, 'lambda': 3.377991412356125}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063213 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:36:40,728] Trial 78 finished with value: 54.33487847978191 and parameters: {'n_estimators': 106, 'boosting_type': 'gbdt', 'eta': 0.11569590718040594, 'num_leaves': 35, 'max_depth': 17, 'min_child_weight': 0.08726466791647236, 'colsample_bytree': 0.982193010905598, 'subsample': 0.6801729908835882, 'alpha': 0.13324040467738552, 'lambda': 3.7526252489673357}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:36:59,314] Trial 79 finished with value: 53.95609949826604 and parameters: {'n_estimators': 124, 'boosting_type': 'gbdt', 'eta': 0.06444219052067726, 'num_leaves': 31, 'max_depth': 14, 'min_child_weight': 0.08374128994863962, 'colsample_bytree': 0.9445224359754414, 'subsample': 0.717191337926506, 'alpha': 0.07064233580233424, 'lambda': 3.1283660782408513}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:37:17,056] Trial 80 finished with value: 54.84088811863466 and parameters: {'n_estimators': 113, 'boosting_type': 'gbdt', 'eta': 0.12916514061569423, 'num_leaves': 44, 'max_depth': 23, 'min_child_weight': 0.050011846857015284, 'colsample_bytree': 0.9630684907316556, 'subsample': 0.6968261383897619, 'alpha': 0.15279265716025123, 'lambda': 3.5621954067608015}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017816 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061895 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:37:35,612] Trial 81 finished with value: 54.3546029087917 and parameters: {'n_estimators': 128, 'boosting_type': 'gbdt', 'eta': 0.08946940101443625, 'num_leaves': 41, 'max_depth': 21, 'min_child_weight': 0.05791365382794126, 'colsample_bytree': 0.926094395127948, 'subsample': 0.7299125582228897, 'alpha': 0.1673386484797328, 'lambda': 3.205318513144467}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:37:55,900] Trial 82 finished with value: 53.76987487723594 and parameters: {'n_estimators': 123, 'boosting_type': 'gbdt', 'eta': 0.08143767541696066, 'num_leaves': 42, 'max_depth': 25, 'min_child_weight': 0.07048151203133911, 'colsample_bytree': 0.9276890107837285, 'subsample': 0.7401280646481011, 'alpha': 0.19936056384865558, 'lambda': 3.3681930806979534}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:38:15,204] Trial 83 finished with value: 54.392111232703265 and parameters: {'n_estimators': 131, 'boosting_type': 'gbdt', 'eta': 0.10871663196993975, 'num_leaves': 41, 'max_depth': 22, 'min_child_weight': 0.066223312567803, 'colsample_bytree': 0.8996078590005115, 'subsample': 0.6892245897672155, 'alpha': 0.11073242032716217, 'lambda': 2.893790297709798}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:38:33,737] Trial 84 finished with value: 54.2254250751377 and parameters: {'n_estimators': 118, 'boosting_type': 'gbdt', 'eta': 0.085387890300918, 'num_leaves': 38, 'max_depth': 19, 'min_child_weight': 0.07837764680037605, 'colsample_bytree': 0.9432652636973101, 'subsample': 0.7141210775580383, 'alpha': 0.17111737372157063, 'lambda': 4.331747223465244}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:38:52,091] Trial 85 finished with value: 53.29074290839147 and parameters: {'n_estimators': 111, 'boosting_type': 'gbdt', 'eta': 0.09912240129146814, 'num_leaves': 37, 'max_depth': 16, 'min_child_weight': 0.09440793048356744, 'colsample_bytree': 0.984815597501698, 'subsample': 0.6766289150250129, 'alpha': 0.12134705395384474, 'lambda': 3.9826805770115428}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:39:09,404] Trial 86 finished with value: 54.095144705644266 and parameters: {'n_estimators': 109, 'boosting_type': 'gbdt', 'eta': 0.0973174298435012, 'num_leaves': 37, 'max_depth': 15, 'min_child_weight': 0.09436346160780246, 'colsample_bytree': 0.9821869134244045, 'subsample': 0.6623297082045934, 'alpha': 0.11864642914611231, 'lambda': 4.080324359805513}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074640 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:39:28,020] Trial 87 finished with value: 53.96032491506666 and parameters: {'n_estimators': 112, 'boosting_type': 'gbdt', 'eta': 0.11761686586153179, 'num_leaves': 39, 'max_depth': 11, 'min_child_weight': 0.08908681941117001, 'colsample_bytree': 0.9649475602802574, 'subsample': 0.6746161030978144, 'alpha': 0.0967931979007591, 'lambda': 3.98406654424739}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:39:44,891] Trial 88 finished with value: 53.70467712891176 and parameters: {'n_estimators': 104, 'boosting_type': 'gbdt', 'eta': 0.0733318222098105, 'num_leaves': 37, 'max_depth': 17, 'min_child_weight': 0.09604256617005302, 'colsample_bytree': 0.9875505105799873, 'subsample': 0.6921588836202118, 'alpha': 0.12920761246663262, 'lambda': 3.824758409767105}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:40:03,639] Trial 89 finished with value: 55.57040168875318 and parameters: {'n_estimators': 134, 'boosting_type': 'gbdt', 'eta': 0.1453032868257315, 'num_leaves': 45, 'max_depth': 13, 'min_child_weight': 0.09980949072166412, 'colsample_bytree': 0.9996679968017654, 'subsample': 0.6514281882234342, 'alpha': 0.09942123333453706, 'lambda': 3.676720581695973}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019549 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:40:21,499] Trial 90 finished with value: 54.25276669960527 and parameters: {'n_estimators': 120, 'boosting_type': 'gbdt', 'eta': 0.10870130137002396, 'num_leaves': 40, 'max_depth': 18, 'min_child_weight': 0.08203497991880582, 'colsample_bytree': 0.9620215172459418, 'subsample': 0.6782801197080212, 'alpha': 0.14883503237948692, 'lambda': 4.55693884401587}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:40:40,906] Trial 91 finished with value: 53.052279213724056 and parameters: {'n_estimators': 126, 'boosting_type': 'gbdt', 'eta': 0.08846285997151576, 'num_leaves': 41, 'max_depth': 21, 'min_child_weight': 0.07471532107002896, 'colsample_bytree': 0.9356239831331191, 'subsample': 0.6988063445611037, 'alpha': 0.17344322657314673, 'lambda': 3.5076425666728315}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036937 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:40:59,108] Trial 92 finished with value: 54.399549238008014 and parameters: {'n_estimators': 117, 'boosting_type': 'gbdt', 'eta': 0.0988676446544286, 'num_leaves': 39, 'max_depth': 16, 'min_child_weight': 0.08810740785453813, 'colsample_bytree': 0.91331053703774, 'subsample': 0.7059749556310153, 'alpha': 0.12803962378072833, 'lambda': 4.249320539843581}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:41:16,331] Trial 93 finished with value: 53.73972088380734 and parameters: {'n_estimators': 111, 'boosting_type': 'gbdt', 'eta': 0.10265941621175875, 'num_leaves': 36, 'max_depth': 20, 'min_child_weight': 0.07407199489761415, 'colsample_bytree': 0.9571137113583791, 'subsample': 0.6680330902241284, 'alpha': 0.15009225685993108, 'lambda': 3.495126890913276}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:41:34,809] Trial 94 finished with value: 54.15848386644124 and parameters: {'n_estimators': 122, 'boosting_type': 'gbdt', 'eta': 0.09141679821107451, 'num_leaves': 38, 'max_depth': 24, 'min_child_weight': 0.0790114202655896, 'colsample_bytree': 0.9347350450547205, 'subsample': 0.689789024564861, 'alpha': 0.08741935478775671, 'lambda': 4.010304714601057}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:41:54,063] Trial 95 finished with value: 54.228685680056145 and parameters: {'n_estimators': 140, 'boosting_type': 'gbdt', 'eta': 0.07917758786039995, 'num_leaves': 40, 'max_depth': 20, 'min_child_weight': 0.0605519791030281, 'colsample_bytree': 0.8910247184093218, 'subsample': 0.6580612004508366, 'alpha': 0.10645285528568352, 'lambda': 3.79931009001843}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:42:13,551] Trial 96 finished with value: 54.02622320494294 and parameters: {'n_estimators': 127, 'boosting_type': 'gbdt', 'eta': 0.06824595026590093, 'num_leaves': 43, 'max_depth': 23, 'min_child_weight': 0.09010540316423282, 'colsample_bytree': 0.9777711026808223, 'subsample': 0.6986182968064764, 'alpha': 0.1798426832886333, 'lambda': 4.222710499619679}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:42:31,750] Trial 97 finished with value: 54.683655777467926 and parameters: {'n_estimators': 116, 'boosting_type': 'gbdt', 'eta': 0.11965596343201605, 'num_leaves': 42, 'max_depth': 15, 'min_child_weight': 0.0846682919551434, 'colsample_bytree': 0.9505592944575407, 'subsample': 0.7201404806358864, 'alpha': 0.1178006052309124, 'lambda': 3.6171548476587025}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:42:48,074] Trial 98 finished with value: 54.897558551442934 and parameters: {'n_estimators': 107, 'boosting_type': 'gbdt', 'eta': 0.09271633527711659, 'num_leaves': 35, 'max_depth': 17, 'min_child_weight': 0.07518551671771148, 'colsample_bytree': 0.8500943412974793, 'subsample': 0.6817373547460969, 'alpha': 0.15761417092787844, 'lambda': 3.8835042628320258}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058669 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 17:43:04,008] Trial 99 finished with value: 55.361201007763164 and parameters: {'n_estimators': 101, 'boosting_type': 'gbdt', 'eta': 0.13143187502796172, 'num_leaves': 39, 'max_depth': 21, 'min_child_weight': 0.06694194952658253, 'colsample_bytree': 0.9096881361949644, 'subsample': 0.6689026778082572, 'alpha': 0.1402891506098696, 'lambda': 3.3222213511693703}. Best is trial 65 with value: 52.64828710800964.\n"
     ]
    }
   ],
   "source": [
    "lgbf = LightGBMForecaster()\n",
    "\n",
    "lgbf.train_model(\n",
    "    train_df=df_train,\n",
    "    target_col=\"target\",\n",
    "    model_name=\"lightgbm_enefit\",\n",
    "    exclude_cols=not_feature_columns+to_drop_cols,\n",
    "    categorical_features=cat_columns\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enefit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
