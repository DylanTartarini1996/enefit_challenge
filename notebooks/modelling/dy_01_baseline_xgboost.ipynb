{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "import joblib\n",
    "\n",
    "from typing import Optional,Dict,Tuple, Literal\n",
    "from pathlib import Path\n",
    "from enefit_challenge.utils.dataset import load_enefit_training_data\n",
    "from enefit_challenge.models.forecaster import Forecaster\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostForecaster(Forecaster):\n",
    "    def __init__(self)-> None:\n",
    "        self.tracking_uri = mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "        pass\n",
    "\n",
    "    def fit_model(\n",
    "        self,  \n",
    "        X:pd.DataFrame,\n",
    "        y:pd.Series,\n",
    "        params:Optional[Dict]=None,\n",
    "    ) -> XGBRegressor:\n",
    "        \"\"\"\n",
    "        Trains a `XGBRegressor` and tracks it using mlflow\n",
    "        \"\"\"\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=100, \n",
    "            objective='reg:squarederror'\n",
    "        )\n",
    "        if params:\n",
    "            model.set_params(**params)\n",
    "\n",
    "        model.fit(X, y)\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def fit_and_test_fold(\n",
    "        self, \n",
    "        params:Dict,\n",
    "        X: pd.DataFrame, \n",
    "        y: pd.Series, \n",
    "        year_month_train, \n",
    "        year_month_test,\n",
    "        experiment_name: str=\"xgboost\",\n",
    "        artifact_path: str=\"xgboost_model\",\n",
    "        metrics: list=[\"mae\"]\n",
    "    ) -> float:\n",
    "        \n",
    "        first_dates_month = pd.to_datetime(X[['year', 'month']].assign(day=1))\n",
    "        train_index = first_dates_month.isin(year_month_train)\n",
    "        test_index = first_dates_month.isin(year_month_test)\n",
    "\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        # fit model on training data\n",
    "        model = self.fit_model(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            params\n",
    "        )\n",
    "        # generate predictions\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        self.signature = infer_signature(X_train, y_test_pred)\n",
    "        mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "        mlflow.xgboost.log_model(\n",
    "            model, \n",
    "            artifact_path=artifact_path,\n",
    "            signature=self.signature\n",
    "        )\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        return mae\n",
    "\n",
    "    def train_model(\n",
    "        self, \n",
    "        train_df: pd.DataFrame, \n",
    "        target_col: str,\n",
    "        model_name: str,\n",
    "        exclude_cols: list=[],\n",
    "        experiment_name: str=\"xgboost\",\n",
    "        artifact_path: str=\"xgboost_model\",\n",
    "        params: Optional[Dict]=None,\n",
    "        metrics: list=[\"MAE\"]\n",
    "    ) -> Dict:\n",
    "        \"\"\" \n",
    "        Takes an instance of `XGBRegressor` model and tracks the hyperparameter tuning\n",
    "        experiment on training set using `mlflow` and `optuna`.  \n",
    "        Registers the best version of the model according to a specified metric\n",
    "        \n",
    "        -------     \n",
    "        params:\n",
    "        -------\n",
    "        `experiment_name`: `str`\n",
    "            the name of the experiment used to store runs in mlflow, \n",
    "            as well as the name of the optuna study\n",
    "        `model_name`: `str`\n",
    "            the name the final model will have in the registry\n",
    "        `train_df`: `pd.DataFrame`\n",
    "            the training data for the model.\n",
    "        `target_col`: `str`\n",
    "            the time-series target column\n",
    "        `exclude_cols`: `list`  \n",
    "            columns in dataset that should not be used\n",
    "        `artifact_path`: `str`\n",
    "            the path pointing to the mlflow artifact\n",
    "        `metrics`: `list`\n",
    "            list of the metrics to track in the mlflow experiment run.\n",
    "        `params`: `Optional[Dict]`\n",
    "            optional dictionary of parameters to use\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        X = train_df.drop([target_col] + exclude_cols, axis=1)\n",
    "        y = train_df[target_col]\n",
    "        # unique year-month combinations -> to be used in cross-validation\n",
    "        timesteps = np.sort(np.array(\n",
    "            pd.to_datetime(X[['year', 'month']].assign(day=1)).unique().tolist()\n",
    "        ))\n",
    "\n",
    "        # define mlflow callback Handler for optuna \n",
    "        mlflc = MLflowCallback(\n",
    "            # tracking_uri=self.tracking_uri,\n",
    "            metric_name=\"MAE\",\n",
    "        )\n",
    "    \n",
    "        @mlflc.track_in_mlflow() # decorator to allow mlflow logging\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 200, log=True),\n",
    "                'eta': trial.suggest_float('eta', 0.01, 0.95,log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 1, 10, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 25, log=True),\n",
    "                'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1, log=True),\n",
    "                'colsample_bylevel': trial.suggest_float(\"colsample_bylevel\", 0.1, 1, log=True),\n",
    "                'colsample_bynode': trial.suggest_float(\"colsample_bynode\", 0.1, 1, log=True),\n",
    "                'subsample': trial.suggest_float(\"subsample\", 0.5, 1, log=True),\n",
    "                'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "                'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True)\n",
    "                # 'l2_leaf_reg': trial.suggest_float('l2_leaf_reg',1e-8,100,log=True),\n",
    "                # 'model_size_reg': trial.suggest_float('model_size_reg',1e-8,100,log=True),\n",
    "            }\n",
    "            cv = TimeSeriesSplit(n_splits=3) # cross validation\n",
    "            cv_mae = [None]*3\n",
    "            for i, (train_index, test_index) in enumerate(cv.split(timesteps)):\n",
    "                cv_mae[i] = self.fit_and_test_fold(\n",
    "                    params,\n",
    "                    X, \n",
    "                    y, \n",
    "                    timesteps[train_index], \n",
    "                    timesteps[test_index]\n",
    "                )\n",
    "            trial.set_user_attr('split_mae', cv_mae)\n",
    "            return np.mean(cv_mae)\n",
    "\n",
    "        \n",
    "        sampler = optuna.samplers.TPESampler(\n",
    "            n_startup_trials=10, \n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        self.study = optuna.create_study(\n",
    "            directions=['minimize'],\n",
    "            sampler=sampler,\n",
    "            study_name=experiment_name\n",
    "        )\n",
    "\n",
    "        self.study.optimize(objective, n_trials=100, timeout= 7200, callbacks=[mlflc]) \n",
    "        \n",
    "        # # search for the best run at the end of the experiment\n",
    "        # best_run = mlflow.search_runs(max_results=1,order_by=[\"metrics.MAE\"]).run_id\n",
    "        # # register new model version in mlflow\n",
    "        # self.result = mlflow.register_model(\n",
    "        #     model_uri=f\"runs:/{best_run}/{artifact_path}\",\n",
    "        #     name=self.model_name\n",
    "        # )\n",
    "\n",
    "    def forecast(\n",
    "        self, \n",
    "        input_data: pd.DataFrame,\n",
    "        use_best_from_run: bool=True,\n",
    "        use_env_model: Literal[\"Staging\", \"Production\", None]=None,\n",
    "        use_version: int=None\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Fetches the latest version of the model from the mlflow backend and uses it\n",
    "        to perform prediction on new input data.\n",
    "        \"\"\"\n",
    "        if use_best_from_run:\n",
    "            # not implemented now bc of callback bug\n",
    "            use_prod_model=None\n",
    "            use_version=None\n",
    "        \n",
    "            # model = mlflow.pyfunc.load_model(\n",
    "            #     model_uri=f\"models:/{self.model_name}/{self.result.version}\"\n",
    "            # )\n",
    "            # y_pred = model.predict(input_data)\n",
    "            # return y_pred\n",
    "        \n",
    "        if use_env_model is not None:\n",
    "            use_version = None\n",
    "\n",
    "            model = mlflow.pyfunc.load_model(\n",
    "                # get registered model in given environment\n",
    "                model_uri=f\"models:/{self.model_name}/{use_env_model}\"\n",
    "            )\n",
    "            y_pred = model.predict(input_data)\n",
    "            return y_pred\n",
    "\n",
    "        if use_version is not None:\n",
    "            # get specific registered version of model\n",
    "            model = mlflow.pyfunc.load_model(\n",
    "                model_uri=f\"models:/{self.model_name}/{use_version}\"\n",
    "            )\n",
    "            y_pred = model.predict(input_data)\n",
    "            return y_pred\n",
    "\n",
    "        \n",
    "        if (not use_best_from_run) & (use_env_model is None) & (use_version is None):\n",
    "            return ValueError(\n",
    "                \"You must specify which kind of XGBoostForecaster you intend to use for prediction\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_enefit_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:33:49,054] A new study created in memory with name: xgboost\n",
      "[I 2023-11-18 13:34:11,100] Trial 0 finished with value: 203.6244684794037 and parameters: {'n_estimators': 84, 'eta': 0.7590145927293601, 'max_depth': 5, 'min_child_weight': 5, 'colsample_bytree': 0.1432249371823025, 'colsample_bylevel': 0.14321698289111517, 'colsample_bynode': 0.1143098387631322, 'subsample': 0.9114125527116832, 'lambda': 0.25378155082656645, 'alpha': 0.6796578090758157}. Best is trial 0 with value: 203.6244684794037.\n",
      "[I 2023-11-18 13:34:28,368] Trial 1 finished with value: 205.82206414006154 and parameters: {'n_estimators': 51, 'eta': 0.8283494908181351, 'max_depth': 6, 'min_child_weight': 1, 'colsample_bytree': 0.1519934830130981, 'colsample_bylevel': 0.15254729458052607, 'colsample_bynode': 0.20148477884158655, 'subsample': 0.7193453335958095, 'lambda': 0.05342937261279776, 'alpha': 0.014618962793704969}. Best is trial 0 with value: 203.6244684794037.\n",
      "[I 2023-11-18 13:34:44,396] Trial 2 finished with value: 310.07644729690816 and parameters: {'n_estimators': 116, 'eta': 0.01887471058056493, 'max_depth': 1, 'min_child_weight': 2, 'colsample_bytree': 0.2858051065806936, 'colsample_bylevel': 0.6097839109531512, 'colsample_bynode': 0.15837031559118753, 'subsample': 0.7141180248258306, 'lambda': 0.23423849847112918, 'alpha': 0.0015339162591163618}. Best is trial 0 with value: 203.6244684794037.\n",
      "[I 2023-11-18 13:35:02,916] Trial 3 finished with value: 291.30115836651413 and parameters: {'n_estimators': 116, 'eta': 0.02173950167319137, 'max_depth': 1, 'min_child_weight': 21, 'colsample_bytree': 0.9239150319627247, 'colsample_bylevel': 0.6432759992849892, 'colsample_bynode': 0.2016572169180859, 'subsample': 0.5350227390366598, 'lambda': 0.5456725485601478, 'alpha': 0.057624872164786005}. Best is trial 0 with value: 203.6244684794037.\n",
      "[I 2023-11-18 13:35:13,046] Trial 4 finished with value: 292.39435580830315 and parameters: {'n_estimators': 59, 'eta': 0.09535051957858227, 'max_depth': 1, 'min_child_weight': 18, 'colsample_bytree': 0.18145961353490248, 'colsample_bylevel': 0.4597505784732165, 'colsample_bynode': 0.2049798052095018, 'subsample': 0.7170114293588699, 'lambda': 0.1537592023548176, 'alpha': 0.0054880470007660455}. Best is trial 0 with value: 203.6244684794037.\n",
      "[I 2023-11-18 13:36:32,990] Trial 5 finished with value: 136.27518879820403 and parameters: {'n_estimators': 192, 'eta': 0.3411917190406207, 'max_depth': 9, 'min_child_weight': 17, 'colsample_bytree': 0.39618677904065835, 'colsample_bylevel': 0.835361075531176, 'colsample_bynode': 0.12260057359187529, 'subsample': 0.5727521453035685, 'lambda': 0.0015167330688076208, 'alpha': 0.02001342062287998}. Best is trial 5 with value: 136.27518879820403.\n",
      "[I 2023-11-18 13:36:58,359] Trial 6 finished with value: 225.90512637665407 and parameters: {'n_estimators': 85, 'eta': 0.03440752114258332, 'max_depth': 6, 'min_child_weight': 2, 'colsample_bytree': 0.19095652801045376, 'colsample_bylevel': 0.3488960745139221, 'colsample_bynode': 0.1383324997521996, 'subsample': 0.8718772746165767, 'lambda': 0.0019870215385428634, 'alpha': 8.862326508576256}. Best is trial 5 with value: 136.27518879820403.\n",
      "[I 2023-11-18 13:37:20,612] Trial 7 finished with value: 281.0475125696339 and parameters: {'n_estimators': 146, 'eta': 0.024717508610145263, 'max_depth': 1, 'min_child_weight': 12, 'colsample_bytree': 0.5091635945818552, 'colsample_bylevel': 0.5358055009231864, 'colsample_bynode': 0.5905685925060635, 'subsample': 0.5263318671916041, 'lambda': 0.02715581955282941, 'alpha': 0.0029072088906598446}. Best is trial 5 with value: 136.27518879820403.\n",
      "[I 2023-11-18 13:37:40,159] Trial 8 finished with value: 189.99077834094646 and parameters: {'n_estimators': 166, 'eta': 0.17088794158649134, 'max_depth': 1, 'min_child_weight': 1, 'colsample_bytree': 0.204636133634816, 'colsample_bylevel': 0.2114381362663436, 'colsample_bynode': 0.5365450324352024, 'subsample': 0.7778465452676808, 'lambda': 3.538758864779242, 'alpha': 0.07742116473996247}. Best is trial 5 with value: 136.27518879820403.\n",
      "[I 2023-11-18 13:37:59,185] Trial 9 finished with value: 141.94249474652207 and parameters: {'n_estimators': 59, 'eta': 0.2573955605115437, 'max_depth': 5, 'min_child_weight': 5, 'colsample_bytree': 0.5901564798023388, 'colsample_bylevel': 0.31174220030046296, 'colsample_bynode': 0.33322135755462345, 'subsample': 0.6724696295065364, 'lambda': 0.0012637946338082875, 'alpha': 0.0027012557725439087}. Best is trial 5 with value: 136.27518879820403.\n",
      "[I 2023-11-18 13:39:21,603] Trial 10 finished with value: 214.68257113427435 and parameters: {'n_estimators': 195, 'eta': 0.01027686781014547, 'max_depth': 10, 'min_child_weight': 10, 'colsample_bytree': 0.36321456121327655, 'colsample_bylevel': 0.8084716496554308, 'colsample_bynode': 0.10357222330387059, 'subsample': 0.590962180200284, 'lambda': 0.007077493671122793, 'alpha': 0.02854006015311615}. Best is trial 5 with value: 136.27518879820403.\n",
      "[I 2023-11-18 13:39:42,895] Trial 11 finished with value: 129.2494366693737 and parameters: {'n_estimators': 73, 'eta': 0.3007039456565225, 'max_depth': 3, 'min_child_weight': 6, 'colsample_bytree': 0.5710136534589846, 'colsample_bylevel': 0.8713537531728947, 'colsample_bynode': 0.33954891134400905, 'subsample': 0.5947479630715633, 'lambda': 0.0010741358214098335, 'alpha': 0.0010473363433053052}. Best is trial 11 with value: 129.2494366693737.\n",
      "[I 2023-11-18 13:40:18,461] Trial 12 finished with value: 136.67970853930277 and parameters: {'n_estimators': 192, 'eta': 0.36648366847799646, 'max_depth': 2, 'min_child_weight': 9, 'colsample_bytree': 0.45206457604868155, 'colsample_bylevel': 0.9859968871861825, 'colsample_bynode': 0.3357562089754236, 'subsample': 0.5852557106469622, 'lambda': 0.0056251036036845305, 'alpha': 0.001051482122690196}. Best is trial 11 with value: 129.2494366693737.\n",
      "[I 2023-11-18 13:41:14,265] Trial 13 finished with value: 110.30640162474356 and parameters: {'n_estimators': 138, 'eta': 0.40686987482012427, 'max_depth': 3, 'min_child_weight': 25, 'colsample_bytree': 0.6982004135446469, 'colsample_bylevel': 0.9970904839307638, 'colsample_bynode': 0.7981143524649661, 'subsample': 0.6170444394390245, 'lambda': 0.00113619921701117, 'alpha': 0.010057220721879475}. Best is trial 13 with value: 110.30640162474356.\n",
      "[I 2023-11-18 13:41:54,556] Trial 14 finished with value: 92.66814959749911 and parameters: {'n_estimators': 79, 'eta': 0.11230950992631686, 'max_depth': 3, 'min_child_weight': 25, 'colsample_bytree': 0.7935659734302277, 'colsample_bylevel': 0.9685804279326053, 'colsample_bynode': 0.9825582354761633, 'subsample': 0.6350973408050398, 'lambda': 0.007427185221494028, 'alpha': 0.006153989951294364}. Best is trial 14 with value: 92.66814959749911.\n",
      "[I 2023-11-18 13:42:29,996] Trial 15 finished with value: 104.54451129727607 and parameters: {'n_estimators': 110, 'eta': 0.08279288876219254, 'max_depth': 2, 'min_child_weight': 23, 'colsample_bytree': 0.9519638090894303, 'colsample_bylevel': 0.672647077243804, 'colsample_bynode': 0.9811075262926868, 'subsample': 0.6463127352264114, 'lambda': 0.010476790916451198, 'alpha': 0.007937074359319748}. Best is trial 14 with value: 92.66814959749911.\n",
      "[I 2023-11-18 13:43:02,983] Trial 16 finished with value: 102.06064073338827 and parameters: {'n_estimators': 98, 'eta': 0.07629023050347963, 'max_depth': 2, 'min_child_weight': 24, 'colsample_bytree': 0.9685122875092715, 'colsample_bylevel': 0.6717121343637893, 'colsample_bynode': 0.9574099497968417, 'subsample': 0.6461864692897633, 'lambda': 0.02041560952010118, 'alpha': 0.006720354777642043}. Best is trial 14 with value: 92.66814959749911.\n",
      "[I 2023-11-18 13:43:26,188] Trial 17 finished with value: 134.99087300160713 and parameters: {'n_estimators': 86, 'eta': 0.055743163835266854, 'max_depth': 2, 'min_child_weight': 13, 'colsample_bytree': 0.7598304867582142, 'colsample_bylevel': 0.4757225629374434, 'colsample_bynode': 0.9489620914279187, 'subsample': 0.6368846328847941, 'lambda': 0.028870185106614595, 'alpha': 0.005470120375211754}. Best is trial 14 with value: 92.66814959749911.\n",
      "[I 2023-11-18 13:44:05,423] Trial 18 finished with value: 97.91495721816186 and parameters: {'n_estimators': 98, 'eta': 0.15336175271843724, 'max_depth': 3, 'min_child_weight': 8, 'colsample_bytree': 0.984401718903173, 'colsample_bylevel': 0.7176417330338838, 'colsample_bynode': 0.7142590773946205, 'subsample': 0.7764570233633827, 'lambda': 0.014059809536878833, 'alpha': 0.13540893485688776}. Best is trial 14 with value: 92.66814959749911.\n",
      "[I 2023-11-18 13:44:32,485] Trial 19 finished with value: 108.3143290217776 and parameters: {'n_estimators': 76, 'eta': 0.13961035901383717, 'max_depth': 3, 'min_child_weight': 8, 'colsample_bytree': 0.7453122397880769, 'colsample_bylevel': 0.7767692394928116, 'colsample_bynode': 0.679939622598087, 'subsample': 0.9871307450225572, 'lambda': 0.00505753461883248, 'alpha': 0.17643093644024868}. Best is trial 14 with value: 92.66814959749911.\n",
      "[I 2023-11-18 13:44:55,940] Trial 20 finished with value: 135.61296726894875 and parameters: {'n_estimators': 98, 'eta': 0.15163147068112817, 'max_depth': 3, 'min_child_weight': 15, 'colsample_bytree': 0.6341026051145422, 'colsample_bylevel': 0.386923788748928, 'colsample_bynode': 0.5108783297667728, 'subsample': 0.8020650271380361, 'lambda': 0.061248423920446056, 'alpha': 0.24043928382921287}. Best is trial 14 with value: 92.66814959749911.\n",
      "[I 2023-11-18 13:45:26,590] Trial 21 finished with value: 113.31507882195542 and parameters: {'n_estimators': 99, 'eta': 0.06378887928321418, 'max_depth': 2, 'min_child_weight': 14, 'colsample_bytree': 0.9580478064897214, 'colsample_bylevel': 0.6607925159930353, 'colsample_bynode': 0.7732298205401457, 'subsample': 0.6683753514889501, 'lambda': 0.015137276046434764, 'alpha': 0.03134543286404605}. Best is trial 14 with value: 92.66814959749911.\n",
      "[I 2023-11-18 13:46:18,368] Trial 22 finished with value: 92.86218800761378 and parameters: {'n_estimators': 98, 'eta': 0.057850956769387035, 'max_depth': 4, 'min_child_weight': 25, 'colsample_bytree': 0.8124661912939943, 'colsample_bylevel': 0.7376485346516424, 'colsample_bynode': 0.9787716525677052, 'subsample': 0.7632942554305902, 'lambda': 0.017620361381515515, 'alpha': 0.037826752420350614}. Best is trial 14 with value: 92.66814959749911.\n",
      "[I 2023-11-18 13:46:58,388] Trial 23 finished with value: 96.30676490610124 and parameters: {'n_estimators': 73, 'eta': 0.11401333963255939, 'max_depth': 4, 'min_child_weight': 16, 'colsample_bytree': 0.7378056522939223, 'colsample_bylevel': 0.999121764891835, 'colsample_bynode': 0.7601161785129296, 'subsample': 0.7706186070180402, 'lambda': 0.0034158221771301775, 'alpha': 0.04227823685719324}. Best is trial 14 with value: 92.66814959749911.\n",
      "[I 2023-11-18 13:47:28,835] Trial 24 finished with value: 109.73718848948057 and parameters: {'n_estimators': 69, 'eta': 0.050575484248233875, 'max_depth': 4, 'min_child_weight': 18, 'colsample_bytree': 0.7654886620238422, 'colsample_bylevel': 0.9573649706668509, 'colsample_bynode': 0.45468073942994963, 'subsample': 0.7550250093226558, 'lambda': 0.003411244587372374, 'alpha': 0.040791282096654224}. Best is trial 14 with value: 92.66814959749911.\n",
      "[I 2023-11-18 13:48:00,384] Trial 25 finished with value: 106.03803456334292 and parameters: {'n_estimators': 79, 'eta': 0.11140984961584753, 'max_depth': 4, 'min_child_weight': 25, 'colsample_bytree': 0.49607289896471285, 'colsample_bylevel': 0.8116336546385928, 'colsample_bynode': 0.7860711751873083, 'subsample': 0.8216470323556311, 'lambda': 0.0032926904069099264, 'alpha': 0.016185455820987626}. Best is trial 14 with value: 92.66814959749911.\n",
      "[I 2023-11-18 13:48:55,824] Trial 26 finished with value: 105.84390766843785 and parameters: {'n_estimators': 89, 'eta': 0.04532710844067205, 'max_depth': 8, 'min_child_weight': 12, 'colsample_bytree': 0.6197644021973854, 'colsample_bylevel': 0.5780998466103449, 'colsample_bynode': 0.6439528079953369, 'subsample': 0.7450106840971261, 'lambda': 0.010022124157262158, 'alpha': 0.04757169275147674}. Best is trial 14 with value: 92.66814959749911.\n",
      "[I 2023-11-18 13:50:03,809] Trial 27 finished with value: 91.76552526620094 and parameters: {'n_estimators': 68, 'eta': 0.09264295644798522, 'max_depth': 7, 'min_child_weight': 17, 'colsample_bytree': 0.7893448612609784, 'colsample_bylevel': 0.9918584779164609, 'colsample_bynode': 0.8396036676387872, 'subsample': 0.6850217425179668, 'lambda': 0.0028070322439791053, 'alpha': 0.01486815241486713}. Best is trial 27 with value: 91.76552526620094.\n",
      "[I 2023-11-18 13:51:01,602] Trial 28 finished with value: 90.79313682948323 and parameters: {'n_estimators': 65, 'eta': 0.07198923395950135, 'max_depth': 7, 'min_child_weight': 19, 'colsample_bytree': 0.83098932476011, 'colsample_bylevel': 0.7619840797547188, 'colsample_bynode': 0.8180064944533818, 'subsample': 0.6920100467284034, 'lambda': 0.006667024615604441, 'alpha': 0.014829115337393747}. Best is trial 28 with value: 90.79313682948323.\n",
      "[I 2023-11-18 13:51:43,765] Trial 29 finished with value: 103.5882990595863 and parameters: {'n_estimators': 65, 'eta': 0.0759423524865768, 'max_depth': 8, 'min_child_weight': 11, 'colsample_bytree': 0.8308267032673903, 'colsample_bylevel': 0.5377516794502206, 'colsample_bynode': 0.4221218969516619, 'subsample': 0.6842399886641901, 'lambda': 0.006870092143791819, 'alpha': 0.011709216752850657}. Best is trial 28 with value: 90.79313682948323.\n",
      "[I 2023-11-18 13:52:07,239] Trial 30 finished with value: 207.89450475262075 and parameters: {'n_estimators': 65, 'eta': 0.10107417754408698, 'max_depth': 7, 'min_child_weight': 19, 'colsample_bytree': 0.10379951973956014, 'colsample_bylevel': 0.767125869069103, 'colsample_bynode': 0.8367738336311608, 'subsample': 0.6905845006229191, 'lambda': 0.0018800551284544364, 'alpha': 0.0030971193306154753}. Best is trial 28 with value: 90.79313682948323.\n",
      "[I 2023-11-18 13:54:12,833] Trial 31 finished with value: 93.48521094470601 and parameters: {'n_estimators': 82, 'eta': 0.06670559242235864, 'max_depth': 10, 'min_child_weight': 19, 'colsample_bytree': 0.8212260986349553, 'colsample_bylevel': 0.8466758997121432, 'colsample_bynode': 0.887834795028101, 'subsample': 0.7318888880586205, 'lambda': 0.010475240338447612, 'alpha': 0.02021016558666938}. Best is trial 28 with value: 90.79313682948323.\n",
      "[I 2023-11-18 13:54:59,355] Trial 32 finished with value: 113.27537868415754 and parameters: {'n_estimators': 78, 'eta': 0.040415296216512034, 'max_depth': 6, 'min_child_weight': 15, 'colsample_bytree': 0.6305768482275662, 'colsample_bylevel': 0.7579172835208681, 'colsample_bynode': 0.6552794152767979, 'subsample': 0.707858829067493, 'lambda': 0.038601312244644956, 'alpha': 0.013849041750726564}. Best is trial 28 with value: 90.79313682948323.\n",
      "[I 2023-11-18 13:55:57,462] Trial 33 finished with value: 89.15644030822084 and parameters: {'n_estimators': 52, 'eta': 0.06073972658889438, 'max_depth': 7, 'min_child_weight': 21, 'colsample_bytree': 0.8226385715647077, 'colsample_bylevel': 0.892972521387558, 'colsample_bynode': 0.871961159541643, 'subsample': 0.6912687768945316, 'lambda': 0.02044690814174585, 'alpha': 0.024368067056049905}. Best is trial 33 with value: 89.15644030822084.\n",
      "[I 2023-11-18 13:56:46,555] Trial 34 finished with value: 107.51069172289642 and parameters: {'n_estimators': 50, 'eta': 0.08854866786159207, 'max_depth': 7, 'min_child_weight': 20, 'colsample_bytree': 0.6769138368226293, 'colsample_bylevel': 0.9143629567161295, 'colsample_bynode': 0.875298360447124, 'subsample': 0.6912943895590387, 'lambda': 0.04757848066708893, 'alpha': 0.019657285056471227}. Best is trial 33 with value: 89.15644030822084.\n",
      "[I 2023-11-18 13:57:18,435] Trial 35 finished with value: 124.95548369438397 and parameters: {'n_estimators': 54, 'eta': 0.20239346511871542, 'max_depth': 5, 'min_child_weight': 14, 'colsample_bytree': 0.572239460696567, 'colsample_bylevel': 0.8912418007090086, 'colsample_bynode': 0.693559168963309, 'subsample': 0.7190757580827984, 'lambda': 0.002685935487197064, 'alpha': 0.009239321745967415}. Best is trial 33 with value: 89.15644030822084.\n",
      "[I 2023-11-18 13:58:17,519] Trial 36 finished with value: 96.12593247964442 and parameters: {'n_estimators': 57, 'eta': 0.11924182697575056, 'max_depth': 8, 'min_child_weight': 20, 'colsample_bytree': 0.8418980089824555, 'colsample_bylevel': 0.6175610696654114, 'colsample_bynode': 0.8270746575080632, 'subsample': 0.6657428668576081, 'lambda': 0.004498938529138367, 'alpha': 0.005135222916375694}. Best is trial 33 with value: 89.15644030822084.\n",
      "[I 2023-11-18 13:59:28,990] Trial 37 finished with value: 109.92662578485697 and parameters: {'n_estimators': 63, 'eta': 0.08601893809978432, 'max_depth': 10, 'min_child_weight': 10, 'colsample_bytree': 0.6787286056005304, 'colsample_bylevel': 0.8617092271086899, 'colsample_bynode': 0.5764825574361224, 'subsample': 0.6220334678620573, 'lambda': 0.007575244791754297, 'alpha': 0.0890294704268687}. Best is trial 33 with value: 89.15644030822084.\n",
      "[I 2023-11-18 14:00:11,560] Trial 38 finished with value: 106.81683136209223 and parameters: {'n_estimators': 55, 'eta': 0.03258907694627188, 'max_depth': 6, 'min_child_weight': 15, 'colsample_bytree': 0.8522746351718864, 'colsample_bylevel': 0.6083440347574145, 'colsample_bynode': 0.8676860325049232, 'subsample': 0.5617261564648018, 'lambda': 0.002176155034059395, 'alpha': 0.02285903379930145}. Best is trial 33 with value: 89.15644030822084.\n",
      "[I 2023-11-18 14:00:51,331] Trial 39 finished with value: 111.66650752917671 and parameters: {'n_estimators': 52, 'eta': 0.06258988481786726, 'max_depth': 7, 'min_child_weight': 18, 'colsample_bytree': 0.5125179484999138, 'colsample_bylevel': 0.7161864778863873, 'colsample_bynode': 0.9938408665924234, 'subsample': 0.7050655189348832, 'lambda': 0.09323307260109841, 'alpha': 0.003957410349416384}. Best is trial 33 with value: 89.15644030822084.\n",
      "[I 2023-11-18 14:01:32,869] Trial 40 finished with value: 98.85008238541134 and parameters: {'n_estimators': 61, 'eta': 0.040328999230265006, 'max_depth': 5, 'min_child_weight': 20, 'colsample_bytree': 0.8577224648288875, 'colsample_bylevel': 0.868602734049864, 'colsample_bynode': 0.6141728016701115, 'subsample': 0.5065670567937175, 'lambda': 0.027988831507256545, 'alpha': 0.001948966004815255}. Best is trial 33 with value: 89.15644030822084.\n",
      "[W 2023-11-18 14:01:53,795] Trial 41 failed with parameters: {'n_estimators': 69, 'eta': 0.05466385445682109, 'max_depth': 9, 'min_child_weight': 24, 'colsample_bytree': 0.864720632160343, 'colsample_bylevel': 0.7164099540601436, 'colsample_bynode': 0.7240196928393346, 'subsample': 0.7153552535474951, 'lambda': 0.018140447167415828, 'alpha': 0.013810391266687816} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/integration/mlflow.py\", line 218, in wrapper\n",
      "    return func(trial)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/var/folders/2c/394jj2r140x4jgq0vzwn32680000gn/T/ipykernel_2396/755658617.py\", line 137, in objective\n",
      "    cv_mae[i] = self.fit_and_test_fold(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/2c/394jj2r140x4jgq0vzwn32680000gn/T/ipykernel_2396/755658617.py\", line 48, in fit_and_test_fold\n",
      "    model = self.fit_model(\n",
      "            ^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/2c/394jj2r140x4jgq0vzwn32680000gn/T/ipykernel_2396/755658617.py\", line 22, in fit_model\n",
      "    model.fit(X, y)\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/sklearn.py\", line 1025, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/training.py\", line 185, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/core.py\", line 1918, in update\n",
      "    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2023-11-18 14:01:53,798] Trial 41 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m not_feature_columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mdatetime\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrow_id\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mprediction_unit_id\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdata_block_id\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mcounty\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mproduct_type\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m xgbf \u001b[39m=\u001b[39m XGBoostForecaster()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m xgbf\u001b[39m.\u001b[39;49mtrain_model(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_df\u001b[39m=\u001b[39;49mdf_train,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     target_col\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtarget\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mxgboost_enefit\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     exclude_cols\u001b[39m=\u001b[39;49mnot_feature_columns,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "\u001b[1;32m/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m sampler \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     n_startup_trials\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstudy \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m     directions\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m     sampler\u001b[39m=\u001b[39msampler,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m     study_name\u001b[39m=\u001b[39mexperiment_name\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m )\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstudy\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, timeout\u001b[39m=\u001b[39;49m \u001b[39m7200\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[mlflc])\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/integration/mlflow.py:218\u001b[0m, in \u001b[0;36mMLflowCallback.track_in_mlflow.<locals>.decorator.<locals>.wrapper\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(trial\u001b[39m.\u001b[39mnumber), nested\u001b[39m=\u001b[39mnested) \u001b[39mas\u001b[39;00m run:\n\u001b[1;32m    214\u001b[0m     trial\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mset_trial_system_attr(\n\u001b[1;32m    215\u001b[0m         trial\u001b[39m.\u001b[39m_trial_id, RUN_ID_ATTRIBUTE_KEY, run\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id\n\u001b[1;32m    216\u001b[0m     )\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m func(trial)\n",
      "\u001b[1;32m/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m cv_mae \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m]\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (train_index, test_index) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(cv\u001b[39m.\u001b[39msplit(timesteps)):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m     cv_mae[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_and_test_fold(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m         params,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m         X, \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m         y, \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m         timesteps[train_index], \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m         timesteps[test_index]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m trial\u001b[39m.\u001b[39mset_user_attr(\u001b[39m'\u001b[39m\u001b[39msplit_mae\u001b[39m\u001b[39m'\u001b[39m, cv_mae)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(cv_mae)\n",
      "\u001b[1;32m/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m y_test \u001b[39m=\u001b[39m y[test_index]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# fit model on training data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     X_train, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     y_train, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     params\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# generate predictions\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m y_test_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;32m/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m params:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     model\u001b[39m.\u001b[39mset_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_01_baseline_xgboost.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m (\n\u001b[1;32m   1017\u001b[0m     model,\n\u001b[1;32m   1018\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1024\u001b[0m )\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1026\u001b[0m     params,\n\u001b[1;32m   1027\u001b[0m     train_dmatrix,\n\u001b[1;32m   1028\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1029\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1030\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1031\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1032\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1033\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1034\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1035\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1036\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1037\u001b[0m )\n\u001b[1;32m   1039\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1040\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "not_feature_columns = ['datetime', 'row_id','prediction_unit_id','date','time', 'data_block_id',\n",
    "                       'county', 'product_type']\n",
    "\n",
    "xgbf = XGBoostForecaster()\n",
    "\n",
    "xgbf.train_model(\n",
    "    train_df=df_train,\n",
    "    target_col=\"target\",\n",
    "    model_name=\"xgboost_enefit\",\n",
    "    exclude_cols=not_feature_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_feature_columns = ['datetime', 'row_id','prediction_unit_id','date','time', 'data_block_id', 'county', 'product_type']\n",
    "\n",
    "# predict on training data is done only to test the forecast method\n",
    "X = df_train.drop(['target'] + not_feature_columns, axis=1).tail(5)\n",
    "y = df_train['target'].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|| 5/5 [00:00<00:00, 60.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([756.55133 ,  27.508097,  49.8362  ,   5.408032, 332.51645 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbf.forecast(\n",
    "    input_data=X,\n",
    "    use_best_from_run=False,\n",
    "    use_env_model=\"Staging\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2017819    702.729\n",
       "2017820     45.864\n",
       "2017821     25.221\n",
       "2017822     12.243\n",
       "2017823    196.240\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueError('You must specify which kind of XGBoostForecaster yoiu intend to use for prediction')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbf = XGBoostForecaster()\n",
    "xgbf.forecast(\n",
    "    input_data=X,\n",
    "    use_best_from_run=False,\n",
    "    use_env_model=None,\n",
    "    use_version=None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enefit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
