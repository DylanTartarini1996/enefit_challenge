{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance assessment\n",
    "in the previous [notebook1](../feature_eng/EDA_feat_imp.ipynb) and [notebook2](../feature_eng/dropping_features.ipynb) we looked at feature importances and decided to drop some of them. Let's look how the winning model behaves now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from enefit_challenge.utils.dataset import load_enefit_training_data\n",
    "from enefit_challenge.models.lightgbm.lightgbm_forecaster import LightGBMForecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_feature_columns = ['datetime', 'row_id','prediction_unit_id','date','time', 'data_block_id']\n",
    "cat_columns = ['county', 'product_type']\n",
    "to_drop_cols = [\n",
    "    '10_metre_u_wind_component_mean_f',\n",
    "    '10_metre_v_wind_component_min_f',\n",
    "    'cloudcover_low_mean_f',\n",
    "    'dayofweek_sine',\n",
    "    'direct_solar_radiation_max_f',\n",
    "    'eic_count',\n",
    "    'euros_per_mwh', # not sure about this one\n",
    "    'hour_sine',\n",
    "    'temperature_min_f',\n",
    "    'total_precipitation_max_f',\n",
    "    'week_sine',\n",
    "    '10_metre_u_wind_component_min_f',\n",
    "    '10_metre_v_wind_component_max_f',\n",
    "    '10_metre_v_wind_component_std_f',\n",
    "    'cloudcover_high_mean_f',\n",
    "    'cloudcover_high_std_f',\n",
    "    'cloudcover_low_min_f',\n",
    "    'cloudcover_low_std_f',\n",
    "    'cloudcover_mid_std_f',\n",
    "    'cloudcover_total_std_f',\n",
    "    'county_12', # find a way to drop\n",
    "    'county_3', # find a way to drop\n",
    "    'county_9', # find a way to drop\n",
    "    'direct_solar_radiation_min_f',\n",
    "    'direct_solar_radiation_std_f',\n",
    "    'highest_price_per_mwh', # not sure about this one\n",
    "    # 'installed_capacity', #this one should alspo be dropped?\n",
    "    'month_cosine',\n",
    "    'product_type_3', # find a way to drop\n",
    "    'snowfall_max_f',\n",
    "    'temperature_std_f',\n",
    "    'total_precipitation_min_f',\n",
    "    'total_precipitation_std_f',\n",
    "    # 'year'\n",
    "]\n",
    "df_train = load_enefit_training_data()\n",
    "# df_train = df_train.drop(dropped_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 14:09:24,775] A new study created in memory with name: lightgbm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 14:09:38,081] Trial 0 finished with value: 85.00109526111159 and parameters: {'n_estimators': 121, 'boosting_type': 'gbdt', 'eta': 0.11957168053633911, 'max_depth': 2, 'min_child_weight': 0.38333321561566636, 'colsample_bytree': 0.2738969595234697, 'subsample': 0.927727492754704, 'lambda': 7.155682161754866, 'alpha': 0.03417952912061012}. Best is trial 0 with value: 85.00109526111159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 14:09:55,023] Trial 1 finished with value: 100.42811331481772 and parameters: {'n_estimators': 179, 'boosting_type': 'dart', 'eta': 0.6769776337422189, 'max_depth': 1, 'min_child_weight': 0.002231090560744304, 'colsample_bytree': 0.10476552591086516, 'subsample': 0.8904582312865974, 'lambda': 1.2960656597279734, 'alpha': 3.020289640158666}. Best is trial 0 with value: 85.00109526111159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 14:10:11,098] Trial 2 finished with value: 93.72367492477021 and parameters: {'n_estimators': 242, 'boosting_type': 'gbdt', 'eta': 0.3496801474075936, 'max_depth': 1, 'min_child_weight': 0.3628140404024382, 'colsample_bytree': 0.1391083782005788, 'subsample': 0.9623735634056996, 'lambda': 0.12229065947034369, 'alpha': 0.045566719139214756}. Best is trial 0 with value: 85.00109526111159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 14:10:22,489] Trial 3 finished with value: 97.94896189232213 and parameters: {'n_estimators': 76, 'boosting_type': 'gbdt', 'eta': 0.13310833122227744, 'max_depth': 1, 'min_child_weight': 0.29548945587266856, 'colsample_bytree': 0.40935087469661635, 'subsample': 0.7668062477401033, 'lambda': 5.9565160187105715, 'alpha': 0.5336803302756714}. Best is trial 0 with value: 85.00109526111159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 14:10:41,579] Trial 4 finished with value: 252.76126621014464 and parameters: {'n_estimators': 89, 'boosting_type': 'dart', 'eta': 0.013155559915552555, 'max_depth': 4, 'min_child_weight': 0.4814503186400563, 'colsample_bytree': 0.1623239345911629, 'subsample': 0.5467397969664458, 'lambda': 0.01826894228153231, 'alpha': 0.028499883436971588}. Best is trial 0 with value: 85.00109526111159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 14:10:57,968] Trial 5 finished with value: 252.7166716460455 and parameters: {'n_estimators': 125, 'boosting_type': 'dart', 'eta': 0.015915358693657216, 'max_depth': 1, 'min_child_weight': 0.004418125737902547, 'colsample_bytree': 0.44989205684622635, 'subsample': 0.5959617329725612, 'lambda': 0.07332348382056167, 'alpha': 0.009499535455183799}. Best is trial 0 with value: 85.00109526111159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-09 14:11:09,514] Trial 6 finished with value: 256.6780480935701 and parameters: {'n_estimators': 64, 'boosting_type': 'dart', 'eta': 0.018762369507155215, 'max_depth': 1, 'min_child_weight': 0.02984699979785086, 'colsample_bytree': 0.662206180587672, 'subsample': 0.5348110858445883, 'lambda': 2.247913678489981, 'alpha': 0.002423224390060893}. Best is trial 0 with value: 85.00109526111159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8120\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-12-09 14:11:36,664] Trial 7 failed with parameters: {'n_estimators': 241, 'boosting_type': 'dart', 'eta': 0.1571148597057821, 'max_depth': 5, 'min_child_weight': 0.0014346671987806323, 'colsample_bytree': 0.19178161105195696, 'subsample': 0.5434414678644279, 'lambda': 0.015295398277813739, 'alpha': 0.002984770033451216} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/integration/mlflow.py\", line 218, in wrapper\n",
      "    return func(trial)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/enefit_challenge-0.1.2-py3.11.egg/enefit_challenge/models/lightgbm/lightgbm_forecaster.py\", line 199, in objective\n",
      "    cv_mae[i], cv_mase[i], cv_mse[i], cv_rmse[i], cv_mape[i] = self.fit_and_test_fold(\n",
      "                                                               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/enefit_challenge-0.1.2-py3.11.egg/enefit_challenge/models/lightgbm/lightgbm_forecaster.py\", line 96, in fit_and_test_fold\n",
      "    model = self.fit_model(\n",
      "            ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/enefit_challenge-0.1.2-py3.11.egg/enefit_challenge/models/lightgbm/lightgbm_forecaster.py\", line 65, in fit_model\n",
      "    model.fit(X, y)\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 571, in safe_patch_function\n",
      "    patch_function(call_original, *args, **kwargs)\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 250, in patch_with_managed_run\n",
      "    result = patch_function(original, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/sklearn/__init__.py\", line 1632, in patched_fit\n",
      "    result = fit_impl(original, self, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/sklearn/__init__.py\", line 1351, in fit_mlflow_xgboost_and_lightgbm\n",
      "    fit_output = original(self, *args, **kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 552, in call_original\n",
      "    return call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 487, in call_original_fn_with_event_logging\n",
      "    original_fn_result = original_fn(*og_args, **og_kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 549, in _original_fn\n",
      "    original_result = original(*_og_args, **_og_kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/lightgbm/sklearn.py\", line 1049, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 571, in safe_patch_function\n",
      "    patch_function(call_original, *args, **kwargs)\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 250, in patch_with_managed_run\n",
      "    result = patch_function(original, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/sklearn/__init__.py\", line 1639, in patched_fit\n",
      "    return original(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 552, in call_original\n",
      "    return call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 487, in call_original_fn_with_event_logging\n",
      "    original_fn_result = original_fn(*og_args, **og_kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 549, in _original_fn\n",
      "    original_result = original(*_og_args, **_og_kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/lightgbm/sklearn.py\", line 842, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 571, in safe_patch_function\n",
      "    patch_function(call_original, *args, **kwargs)\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 250, in patch_with_managed_run\n",
      "    result = patch_function(original, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/lightgbm/__init__.py\", line 876, in train\n",
      "    return train_impl(_log_models, _log_datasets, original, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/lightgbm/__init__.py\", line 789, in train_impl\n",
      "    model = original(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 552, in call_original\n",
      "    return call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 487, in call_original_fn_with_event_logging\n",
      "    original_fn_result = original_fn(*og_args, **og_kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py\", line 549, in _original_fn\n",
      "    original_result = original(*_og_args, **_og_kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/lightgbm/engine.py\", line 276, in train\n",
      "    booster.update(fobj=fobj)\n",
      "  File \"/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/lightgbm/basic.py\", line 3658, in update\n",
      "    _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2023-12-09 14:11:36,672] Trial 7 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_08_drop_features.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_08_drop_features.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lgbf \u001b[39m=\u001b[39m LightGBMForecaster()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_08_drop_features.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lgbf\u001b[39m.\u001b[39;49mtrain_model(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_08_drop_features.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_df\u001b[39m=\u001b[39;49mdf_train,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_08_drop_features.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     target_col\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtarget\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_08_drop_features.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlightgbm_enefit\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_08_drop_features.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     exclude_cols\u001b[39m=\u001b[39;49mnot_feature_columns\u001b[39m+\u001b[39;49mto_drop_cols,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_08_drop_features.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     categorical_features\u001b[39m=\u001b[39;49mcat_columns\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dylantartarini/Desktop/enefit_challenge/notebooks/modelling/dy_08_drop_features.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/enefit_challenge-0.1.2-py3.11.egg/enefit_challenge/models/lightgbm/lightgbm_forecaster.py:235\u001b[0m, in \u001b[0;36mLightGBMForecaster.train_model\u001b[0;34m(self, train_df, target_col, model_name, exclude_cols, categorical_features, experiment_name, artifact_path, params)\u001b[0m\n\u001b[1;32m    224\u001b[0m sampler \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler(\n\u001b[1;32m    225\u001b[0m     n_startup_trials\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, \n\u001b[1;32m    226\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    227\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstudy \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[1;32m    230\u001b[0m     directions\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    231\u001b[0m     sampler\u001b[39m=\u001b[39msampler,\n\u001b[1;32m    232\u001b[0m     study_name\u001b[39m=\u001b[39mexperiment_name\n\u001b[1;32m    233\u001b[0m )\n\u001b[0;32m--> 235\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstudy\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, timeout\u001b[39m=\u001b[39;49m \u001b[39m7200\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[mlflc])\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/optuna/integration/mlflow.py:218\u001b[0m, in \u001b[0;36mMLflowCallback.track_in_mlflow.<locals>.decorator.<locals>.wrapper\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(trial\u001b[39m.\u001b[39mnumber), nested\u001b[39m=\u001b[39mnested) \u001b[39mas\u001b[39;00m run:\n\u001b[1;32m    214\u001b[0m     trial\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mset_trial_system_attr(\n\u001b[1;32m    215\u001b[0m         trial\u001b[39m.\u001b[39m_trial_id, RUN_ID_ATTRIBUTE_KEY, run\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id\n\u001b[1;32m    216\u001b[0m     )\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m func(trial)\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/enefit_challenge-0.1.2-py3.11.egg/enefit_challenge/models/lightgbm/lightgbm_forecaster.py:199\u001b[0m, in \u001b[0;36mLightGBMForecaster.train_model.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    197\u001b[0m cv_mape \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m]\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m\n\u001b[1;32m    198\u001b[0m \u001b[39mfor\u001b[39;00m i, (train_index, test_index) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(cv\u001b[39m.\u001b[39msplit(timesteps)):\n\u001b[0;32m--> 199\u001b[0m     cv_mae[i], cv_mase[i], cv_mse[i], cv_rmse[i], cv_mape[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_and_test_fold(\n\u001b[1;32m    200\u001b[0m         params,\n\u001b[1;32m    201\u001b[0m         X, \n\u001b[1;32m    202\u001b[0m         y, \n\u001b[1;32m    203\u001b[0m         timesteps[train_index], \n\u001b[1;32m    204\u001b[0m         timesteps[test_index]\n\u001b[1;32m    205\u001b[0m     )\n\u001b[1;32m    206\u001b[0m trial\u001b[39m.\u001b[39mset_user_attr(\u001b[39m'\u001b[39m\u001b[39msplit_mae\u001b[39m\u001b[39m'\u001b[39m, cv_mae)\n\u001b[1;32m    207\u001b[0m trial\u001b[39m.\u001b[39mset_user_attr(\u001b[39m'\u001b[39m\u001b[39msplit_mase\u001b[39m\u001b[39m'\u001b[39m, cv_mase)\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/enefit_challenge-0.1.2-py3.11.egg/enefit_challenge/models/lightgbm/lightgbm_forecaster.py:96\u001b[0m, in \u001b[0;36mLightGBMForecaster.fit_and_test_fold\u001b[0;34m(self, params, X, y, year_month_train, year_month_test, experiment_name, artifact_path, metrics)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m# fit model on training data\u001b[39;00m\n\u001b[1;32m     95\u001b[0m mlflow\u001b[39m.\u001b[39mlightgbm\u001b[39m.\u001b[39mautolog(log_models\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, log_datasets\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 96\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_model(\n\u001b[1;32m     97\u001b[0m     X_train, \n\u001b[1;32m     98\u001b[0m     y_train, \n\u001b[1;32m     99\u001b[0m     params\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    102\u001b[0m \u001b[39m# generate predictions\u001b[39;00m\n\u001b[1;32m    103\u001b[0m y_test_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/enefit_challenge-0.1.2-py3.11.egg/enefit_challenge/models/lightgbm/lightgbm_forecaster.py:65\u001b[0m, in \u001b[0;36mLightGBMForecaster.fit_model\u001b[0;34m(self, X, y, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m params:\n\u001b[1;32m     63\u001b[0m     model\u001b[39m.\u001b[39mset_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m---> 65\u001b[0m model\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m     67\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:571\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     patch_function\u001b[39m.\u001b[39mcall(call_original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    570\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 571\u001b[0m     patch_function(call_original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    573\u001b[0m session\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msucceeded\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m try_log_autologging_event(\n\u001b[1;32m    576\u001b[0m     AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_patch_function_success,\n\u001b[1;32m    577\u001b[0m     session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m     kwargs,\n\u001b[1;32m    582\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:250\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001b[0;34m(original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m     managed_run \u001b[39m=\u001b[39m create_managed_run()\n\u001b[1;32m    249\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     result \u001b[39m=\u001b[39m patch_function(original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    251\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m    252\u001b[0m     \u001b[39m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[39m# that runs are terminated if a user prematurely interrupts training execution\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# (e.g. via sigint / ctrl-c)\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m managed_run:\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/sklearn/__init__.py:1632\u001b[0m, in \u001b[0;36m_autolog.<locals>.patched_fit\u001b[0;34m(fit_impl, allow_children_patch, original, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mshould_log():\n\u001b[1;32m   1629\u001b[0m     \u001b[39m# In `fit_mlflow` call, it will also call metric API for computing training metrics\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m     \u001b[39m# so we need temporarily disable the post_training_metrics patching.\u001b[39;00m\n\u001b[1;32m   1631\u001b[0m     \u001b[39mwith\u001b[39;00m _AUTOLOGGING_METRICS_MANAGER\u001b[39m.\u001b[39mdisable_log_post_training_metrics():\n\u001b[0;32m-> 1632\u001b[0m         result \u001b[39m=\u001b[39m fit_impl(original, \u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1633\u001b[0m     \u001b[39mif\u001b[39;00m should_log_post_training_metrics:\n\u001b[1;32m   1634\u001b[0m         _AUTOLOGGING_METRICS_MANAGER\u001b[39m.\u001b[39mregister_model(\n\u001b[1;32m   1635\u001b[0m             \u001b[39mself\u001b[39m, mlflow\u001b[39m.\u001b[39mactive_run()\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id\n\u001b[1;32m   1636\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/sklearn/__init__.py:1351\u001b[0m, in \u001b[0;36m_autolog.<locals>.fit_mlflow_xgboost_and_lightgbm\u001b[0;34m(original, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[39mreturn\u001b[39;00m input_example\n\u001b[1;32m   1349\u001b[0m \u001b[39m# parameter, metric, and non-model artifact logging are done in\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \u001b[39m# `train()` in `mlflow.xgboost.autolog()` and `mlflow.lightgbm.autolog()`\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m fit_output \u001b[39m=\u001b[39m original(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1352\u001b[0m \u001b[39m# log models after training\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m \u001b[39mif\u001b[39;00m log_models:\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:552\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m         original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39m_og_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_og_kwargs)\n\u001b[1;32m    550\u001b[0m         \u001b[39mreturn\u001b[39;00m original_result\n\u001b[0;32m--> 552\u001b[0m \u001b[39mreturn\u001b[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:487\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[0;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    479\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    480\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_start,\n\u001b[1;32m    481\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         og_kwargs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m     original_fn_result \u001b[39m=\u001b[39m original_fn(\u001b[39m*\u001b[39;49mog_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mog_kwargs)\n\u001b[1;32m    489\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    490\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_success,\n\u001b[1;32m    491\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m         og_kwargs,\n\u001b[1;32m    496\u001b[0m     )\n\u001b[1;32m    497\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:549\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[0;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[39m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[39m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[1;32m    546\u001b[0m     disable_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    547\u001b[0m     reroute_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    548\u001b[0m ):\n\u001b[0;32m--> 549\u001b[0m     original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39;49m_og_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_og_kwargs)\n\u001b[1;32m    550\u001b[0m     \u001b[39mreturn\u001b[39;00m original_result\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/lightgbm/sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1034\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1046\u001b[0m     init_model: Optional[Union[\u001b[39mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLGBMRegressor\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1048\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1049\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m   1050\u001b[0m         X,\n\u001b[1;32m   1051\u001b[0m         y,\n\u001b[1;32m   1052\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1053\u001b[0m         init_score\u001b[39m=\u001b[39;49minit_score,\n\u001b[1;32m   1054\u001b[0m         eval_set\u001b[39m=\u001b[39;49meval_set,\n\u001b[1;32m   1055\u001b[0m         eval_names\u001b[39m=\u001b[39;49meval_names,\n\u001b[1;32m   1056\u001b[0m         eval_sample_weight\u001b[39m=\u001b[39;49meval_sample_weight,\n\u001b[1;32m   1057\u001b[0m         eval_init_score\u001b[39m=\u001b[39;49meval_init_score,\n\u001b[1;32m   1058\u001b[0m         eval_metric\u001b[39m=\u001b[39;49meval_metric,\n\u001b[1;32m   1059\u001b[0m         feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[1;32m   1060\u001b[0m         categorical_feature\u001b[39m=\u001b[39;49mcategorical_feature,\n\u001b[1;32m   1061\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1062\u001b[0m         init_model\u001b[39m=\u001b[39;49minit_model\n\u001b[1;32m   1063\u001b[0m     )\n\u001b[1;32m   1064\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:571\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     patch_function\u001b[39m.\u001b[39mcall(call_original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    570\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 571\u001b[0m     patch_function(call_original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    573\u001b[0m session\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msucceeded\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m try_log_autologging_event(\n\u001b[1;32m    576\u001b[0m     AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_patch_function_success,\n\u001b[1;32m    577\u001b[0m     session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m     kwargs,\n\u001b[1;32m    582\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:250\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001b[0;34m(original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m     managed_run \u001b[39m=\u001b[39m create_managed_run()\n\u001b[1;32m    249\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     result \u001b[39m=\u001b[39m patch_function(original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    251\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m    252\u001b[0m     \u001b[39m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[39m# that runs are terminated if a user prematurely interrupts training execution\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# (e.g. via sigint / ctrl-c)\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m managed_run:\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/sklearn/__init__.py:1639\u001b[0m, in \u001b[0;36m_autolog.<locals>.patched_fit\u001b[0;34m(fit_impl, allow_children_patch, original, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m   1638\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1639\u001b[0m     \u001b[39mreturn\u001b[39;00m original(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:552\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m         original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39m_og_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_og_kwargs)\n\u001b[1;32m    550\u001b[0m         \u001b[39mreturn\u001b[39;00m original_result\n\u001b[0;32m--> 552\u001b[0m \u001b[39mreturn\u001b[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:487\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[0;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    479\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    480\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_start,\n\u001b[1;32m    481\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         og_kwargs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m     original_fn_result \u001b[39m=\u001b[39m original_fn(\u001b[39m*\u001b[39;49mog_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mog_kwargs)\n\u001b[1;32m    489\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    490\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_success,\n\u001b[1;32m    491\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m         og_kwargs,\n\u001b[1;32m    496\u001b[0m     )\n\u001b[1;32m    497\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:549\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[0;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[39m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[39m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[1;32m    546\u001b[0m     disable_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    547\u001b[0m     reroute_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    548\u001b[0m ):\n\u001b[0;32m--> 549\u001b[0m     original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39;49m_og_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_og_kwargs)\n\u001b[1;32m    550\u001b[0m     \u001b[39mreturn\u001b[39;00m original_result\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/lightgbm/sklearn.py:842\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    839\u001b[0m evals_result: _EvalResultDict \u001b[39m=\u001b[39m {}\n\u001b[1;32m    840\u001b[0m callbacks\u001b[39m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 842\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m    843\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    844\u001b[0m     train_set\u001b[39m=\u001b[39;49mtrain_set,\n\u001b[1;32m    845\u001b[0m     num_boost_round\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_estimators,\n\u001b[1;32m    846\u001b[0m     valid_sets\u001b[39m=\u001b[39;49mvalid_sets,\n\u001b[1;32m    847\u001b[0m     valid_names\u001b[39m=\u001b[39;49meval_names,\n\u001b[1;32m    848\u001b[0m     feval\u001b[39m=\u001b[39;49meval_metrics_callable,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    849\u001b[0m     init_model\u001b[39m=\u001b[39;49minit_model,\n\u001b[1;32m    850\u001b[0m     feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[1;32m    851\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[1;32m    852\u001b[0m )\n\u001b[1;32m    854\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evals_result \u001b[39m=\u001b[39m evals_result\n\u001b[1;32m    855\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_best_iteration \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster\u001b[39m.\u001b[39mbest_iteration\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:571\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     patch_function\u001b[39m.\u001b[39mcall(call_original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    570\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 571\u001b[0m     patch_function(call_original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    573\u001b[0m session\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msucceeded\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m try_log_autologging_event(\n\u001b[1;32m    576\u001b[0m     AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_patch_function_success,\n\u001b[1;32m    577\u001b[0m     session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m     kwargs,\n\u001b[1;32m    582\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:250\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001b[0;34m(original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m     managed_run \u001b[39m=\u001b[39m create_managed_run()\n\u001b[1;32m    249\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     result \u001b[39m=\u001b[39m patch_function(original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    251\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m    252\u001b[0m     \u001b[39m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[39m# that runs are terminated if a user prematurely interrupts training execution\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# (e.g. via sigint / ctrl-c)\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m managed_run:\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/lightgbm/__init__.py:876\u001b[0m, in \u001b[0;36mautolog.<locals>.train\u001b[0;34m(_log_models, _log_datasets, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[39mwith\u001b[39;00m _SklearnTrainingSession(estimator\u001b[39m=\u001b[39mlightgbm\u001b[39m.\u001b[39mtrain, allow_children\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m t:\n\u001b[1;32m    875\u001b[0m     \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mshould_log():\n\u001b[0;32m--> 876\u001b[0m         \u001b[39mreturn\u001b[39;00m train_impl(_log_models, _log_datasets, original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    877\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    878\u001b[0m         \u001b[39mreturn\u001b[39;00m original(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/lightgbm/__init__.py:789\u001b[0m, in \u001b[0;36mautolog.<locals>.train_impl\u001b[0;34m(_log_models, _log_datasets, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    786\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [callback]\n\u001b[1;32m    788\u001b[0m \u001b[39m# training model\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m model \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    791\u001b[0m \u001b[39m# If early stopping is activated, logging metrics at the best iteration\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[39m# as extra metrics with the max step + 1.\u001b[39;00m\n\u001b[1;32m    793\u001b[0m early_stopping \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mbest_iteration \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:552\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m         original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39m_og_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_og_kwargs)\n\u001b[1;32m    550\u001b[0m         \u001b[39mreturn\u001b[39;00m original_result\n\u001b[0;32m--> 552\u001b[0m \u001b[39mreturn\u001b[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:487\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[0;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    479\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    480\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_start,\n\u001b[1;32m    481\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         og_kwargs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m     original_fn_result \u001b[39m=\u001b[39m original_fn(\u001b[39m*\u001b[39;49mog_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mog_kwargs)\n\u001b[1;32m    489\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    490\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_success,\n\u001b[1;32m    491\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m         og_kwargs,\n\u001b[1;32m    496\u001b[0m     )\n\u001b[1;32m    497\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:549\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[0;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[39m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[39m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[1;32m    546\u001b[0m     disable_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    547\u001b[0m     reroute_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    548\u001b[0m ):\n\u001b[0;32m--> 549\u001b[0m     original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39;49m_og_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_og_kwargs)\n\u001b[1;32m    550\u001b[0m     \u001b[39mreturn\u001b[39;00m original_result\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/lightgbm/engine.py:276\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    269\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[1;32m    270\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    271\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[1;32m    272\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[1;32m    273\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[1;32m    274\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[0;32m--> 276\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[1;32m    278\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[39m=\u001b[39m []\n\u001b[1;32m    279\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/enefit/lib/python3.11/site-packages/lightgbm/basic.py:3658\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3656\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3657\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 3658\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle,\n\u001b[1;32m   3660\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[1;32m   3661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3662\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lgbf = LightGBMForecaster()\n",
    "\n",
    "lgbf.train_model(\n",
    "    train_df=df_train,\n",
    "    target_col=\"target\",\n",
    "    model_name=\"lightgbm_enefit\",\n",
    "    exclude_cols=not_feature_columns+to_drop_cols,\n",
    "    categorical_features=cat_columns\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enefit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
