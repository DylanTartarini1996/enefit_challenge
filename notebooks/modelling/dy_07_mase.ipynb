{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Scaled Error\n",
    "We want to track, during Optuna Trials / mlflow experiments, also the [Mean Absolute Scaled Error](https://en.wikipedia.org/wiki/Mean_absolute_scaled_error) (MASE). Since we are renewing the requirements file including [sktime](https://www.sktime.net/en/stable/index.html), we use it to track all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.performance_metrics.forecasting import MeanAbsoluteScaledError, MeanAbsolutePercentageError, MeanAbsoluteError, MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from enefit_challenge.utils.dataset import load_enefit_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylantartarini/miniconda3/envs/enefit/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "import joblib\n",
    "\n",
    "from typing import Optional, Dict, Tuple, Literal\n",
    "from enefit_challenge.models.forecaster import Forecaster\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "TRACKING_URI = \"http://127.0.0.1:5000/\" # local tracking URI -> launch mlflow before training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMForecaster(Forecaster):\n",
    "    \"\"\"\n",
    "        Implementation of a Forecaster using `LGBMRegressor` as base model, \n",
    "        `optuna` for hyperparameters optimization and `mlflow` as backend to track experiments\n",
    "        and register best-in-class model for time series prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self)-> None:\n",
    "        self.tracking_uri = mlflow.set_tracking_uri(TRACKING_URI)\n",
    "        pass\n",
    "\n",
    "    def fit_model(\n",
    "        self,  \n",
    "        X:pd.DataFrame,\n",
    "        y:pd.Series,\n",
    "        params:Optional[Dict]=None,\n",
    "    ) -> LGBMRegressor:\n",
    "        \"\"\"\n",
    "        Trains a `LGBMRegressor`\n",
    "\n",
    "        -------     \n",
    "        params:\n",
    "        -------\n",
    "        `X`:`pd.DataFrame`\n",
    "            Features to use for fitting\n",
    "        `y`:`pd.Series`\n",
    "            Target variable\n",
    "        `params`: `Optional[Dict]`\n",
    "            optional dictionary of parameters to use\n",
    "        -------     \n",
    "        returns:\n",
    "        -------\n",
    "        fitted `LGBMRegressor`\n",
    "        \"\"\"\n",
    "        model = LGBMRegressor(\n",
    "            n_estimators=100, \n",
    "            objective='regression',\n",
    "        )\n",
    "        if params:\n",
    "            model.set_params(**params)\n",
    "\n",
    "        model.fit(X, y)\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def fit_and_test_fold(\n",
    "        self, \n",
    "        params:Dict,\n",
    "        X: pd.DataFrame, \n",
    "        y: pd.Series, \n",
    "        year_month_train, \n",
    "        year_month_test,\n",
    "        experiment_name: str=\"lightgbm\",\n",
    "        artifact_path: str=\"lightgbm_model\",\n",
    "        metrics: list=[\"mae\"]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Used for cross validation on different time splits; \n",
    "        also in charge of logging every experiment run / study trial into the backend.\n",
    "        \"\"\"\n",
    "        \n",
    "        first_dates_month = pd.to_datetime(X[['year', 'month']].assign(day=1))\n",
    "        train_index = first_dates_month.isin(year_month_train)\n",
    "        test_index = first_dates_month.isin(year_month_test)\n",
    "\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        # fit model on training data\n",
    "        mlflow.lightgbm.autolog(log_models=False, log_datasets=False)\n",
    "        model = self.fit_model(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            params\n",
    "        )\n",
    "        \n",
    "        # generate predictions\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        self.signature = infer_signature(X_train, y_test_pred)\n",
    "        \n",
    "        MAE = MeanAbsoluteError()\n",
    "        mae = MAE(y_test, y_test_pred)\n",
    "        MASE = MeanAbsoluteScaledError()\n",
    "        mase = MASE(y_test, y_test_pred, y_train=y_train)\n",
    "        MAPE = MeanAbsolutePercentageError()\n",
    "        mape = MAPE(y_test, y_test_pred)\n",
    "        MSE = MeanSquaredError()\n",
    "        mse = MSE(y_test, y_test_pred)\n",
    "        RMSE = MeanSquaredError(square_root=True)\n",
    "        rmse = RMSE(y_test, y_test_pred)\n",
    "\n",
    "        mlflow.lightgbm.log_model(\n",
    "            model, \n",
    "            artifact_path=artifact_path,\n",
    "            signature=self.signature\n",
    "        )\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        return mae, mase, mse, rmse, mape\n",
    "\n",
    "    def train_model(\n",
    "        self, \n",
    "        train_df: pd.DataFrame, \n",
    "        target_col: str,\n",
    "        model_name: str,\n",
    "        exclude_cols: list=[],\n",
    "        categorical_features: list=[],\n",
    "        experiment_name: str=\"lightgbm\",\n",
    "        artifact_path: str=\"lightgbm_model\",\n",
    "        params: Optional[Dict]=None\n",
    "    ) -> None:\n",
    "        \"\"\" \n",
    "        Takes an instance of `LGBMRegressor` model and tracks the hyperparameter tuning\n",
    "        experiment on training set using `mlflow` and `optuna`.  \n",
    "        Registers the best version of the model according to a specified metric (to be implemented).\n",
    "        \n",
    "        -------     \n",
    "        params:\n",
    "        -------\n",
    "        `experiment_name`: `str`\n",
    "            the name of the experiment used to store runs in mlflow, \n",
    "            as well as the name of the optuna study\n",
    "        `model_name`: `str`\n",
    "            the name the final model will have in the registry\n",
    "        `train_df`: `pd.DataFrame`\n",
    "            the training data for the model.\n",
    "        `target_col`: `str`\n",
    "            the time-series target column\n",
    "        `exclude_cols`: `list`  \n",
    "            columns in dataset that should not be used\n",
    "        `categorical_features`: `list`\n",
    "            list of categorical features in the dataset\n",
    "        `artifact_path`: `str`\n",
    "            the path pointing to the mlflow artifact\n",
    "        `params`: `Optional[Dict]`\n",
    "            optional dictionary of parameters to use\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "\n",
    "        if len(categorical_features) > 0: \n",
    "           train_df = pd.get_dummies(train_df, columns=categorical_features)\n",
    "\n",
    "        X = train_df.drop([target_col] + exclude_cols, axis=1)\n",
    "        y = train_df[target_col]\n",
    "        # unique year-month combinations -> to be used in cross-validation\n",
    "        timesteps = np.sort(np.array(\n",
    "            pd.to_datetime(X[['year', 'month']].assign(day=1)).unique().tolist()\n",
    "        ))\n",
    "        \n",
    "        # define mlflow callback Handler for optuna \n",
    "        mlflc = MLflowCallback(\n",
    "            metric_name=[\"MAE\"]\n",
    "        )\n",
    "    \n",
    "        @mlflc.track_in_mlflow() # decorator to allow mlflow logging\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 250, log=True),\n",
    "                'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\"]),\n",
    "                'learning_rate': trial.suggest_float('eta', 0.01, 0.95,log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 1, 10, log=True),\n",
    "                'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 10, log=True),\n",
    "                'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1, log=True),\n",
    "                'subsample': trial.suggest_float(\"subsample\", 0.5, 1, log=True),\n",
    "                'reg_alpha': trial.suggest_float('lambda', 1e-3, 10.0, log=True), #L1\n",
    "                'reg_lambda': trial.suggest_float('alpha', 1e-3, 10.0, log=True) #L2\n",
    "            }\n",
    "\n",
    "            cv = TimeSeriesSplit(n_splits=3) # cross validation\n",
    "            cv_mae = [None]*3\n",
    "            cv_mase = [None]*3\n",
    "            cv_mse = [None]*3\n",
    "            cv_rmse = [None]*3\n",
    "            cv_mape = [None]*3\n",
    "            for i, (train_index, test_index) in enumerate(cv.split(timesteps)):\n",
    "                cv_mae[i], cv_mase[i], cv_mse[i], cv_rmse[i], cv_mape[i] = self.fit_and_test_fold(\n",
    "                    params,\n",
    "                    X, \n",
    "                    y, \n",
    "                    timesteps[train_index], \n",
    "                    timesteps[test_index]\n",
    "                )\n",
    "            trial.set_user_attr('split_mae', cv_mae)\n",
    "            trial.set_user_attr('split_mae', cv_mase)\n",
    "            trial.set_user_attr('split_mape', cv_mse)\n",
    "            trial.set_user_attr('split_rmse', cv_rmse)\n",
    "            trial.set_user_attr('split_mae', cv_mape)\n",
    "\n",
    "            mlflow.log_metrics(\n",
    "                {\n",
    "                    \"MAE\":np.mean(cv_mae),\n",
    "                    \"MASE\": np.mean(cv_mase),\n",
    "                    \"MSE\": np.mean(cv_mse),\n",
    "                    \"RMSE\":np.mean(cv_rmse),\n",
    "                    \"MAPE\":np.mean(cv_mape)\n",
    "                }\n",
    "            )\n",
    "            return np.mean(cv_mae) \n",
    "\n",
    "        \n",
    "        sampler = optuna.samplers.TPESampler(\n",
    "            n_startup_trials=10, \n",
    "            seed=0\n",
    "        )\n",
    "\n",
    "        self.study = optuna.create_study(\n",
    "            directions=['minimize'],\n",
    "            sampler=sampler,\n",
    "            study_name=experiment_name\n",
    "        )\n",
    "\n",
    "        self.study.optimize(objective, n_trials=20, timeout= 7200, callbacks=[mlflc]) \n",
    "        \n",
    "        # # search for the best run at the end of the experiment # not implemented now bc of callback bug\n",
    "        # best_run = mlflow.search_runs(max_results=1,order_by=[\"metrics.MAE\"]).run_id\n",
    "        # # register new model version in mlflow\n",
    "        # self.result = mlflow.register_model(\n",
    "        #     model_uri=f\"runs:/{best_run}/{artifact_path}\",\n",
    "        #     name=self.model_name\n",
    "        # )\n",
    "\n",
    "    def forecast(\n",
    "        self, \n",
    "        input_data: pd.DataFrame,\n",
    "        use_best_from_run: bool=True,\n",
    "        use_env_model: Literal[\"Staging\", \"Production\", None]=None,\n",
    "        use_version: int=None\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Fetches a version of the model from the mlflow backend and uses it\n",
    "        to perform prediction on new input data.  \n",
    "        What version is used depends on params settings, \n",
    "        defaults to using the best version from the last experiment run (currently not implemented). \n",
    "        -------     \n",
    "        params:\n",
    "        -------\n",
    "        `input_data`: `pd.DataFrame`\n",
    "            the input data for prediction,\n",
    "              must have the same schema as what's in the model's signature.\n",
    "        `use_best_from_run`: `bool=True`      \n",
    "            use the best model from the current series of iterations, defaults to True\n",
    "        `use_env_model`: `Literal[\"Staging\", \"Production\", None]=None`\n",
    "            use model from a given mlflow environment, defaults to None.  \n",
    "            Said model might come from past iterations, depending on what you decide in the UI\n",
    "        `use_version`: `int=None`\n",
    "            use a previously trained version of the model. \n",
    "            Said version must have been registered from a previous iteration,  \n",
    "            either by the UI or with mlflow's API\n",
    "        \"\"\"\n",
    "        if use_best_from_run:\n",
    "            # not implemented now bc of callback bug\n",
    "            use_prod_model=None\n",
    "            use_version=None\n",
    "        \n",
    "            # model = mlflow.pyfunc.load_model(\n",
    "            #     model_uri=f\"models:/{self.model_name}/{self.result.version}\"\n",
    "            # )\n",
    "            # y_pred = model.predict(input_data)\n",
    "            # return y_pred\n",
    "        \n",
    "        if use_env_model is not None:\n",
    "            use_version = None\n",
    "\n",
    "            model = mlflow.pyfunc.load_model(\n",
    "                # get registered model in given environment\n",
    "                model_uri=f\"models:/{self.model_name}/{use_env_model}\"\n",
    "            )\n",
    "            y_pred = model.predict(input_data)\n",
    "            return y_pred\n",
    "\n",
    "        if use_version is not None:\n",
    "            # get specific registered version of model\n",
    "            model = mlflow.pyfunc.load_model(\n",
    "                model_uri=f\"models:/{self.model_name}/{use_version}\"\n",
    "            )\n",
    "            y_pred = model.predict(input_data)\n",
    "            return y_pred\n",
    "\n",
    "        \n",
    "        if (not use_best_from_run) & (use_env_model is None) & (use_version is None):\n",
    "            return ValueError(\n",
    "                    \"You must specify which kind of LightGBMForecaster you intend to use for prediction\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_enefit_training_data()\n",
    "not_feature_columns = ['datetime', 'row_id','prediction_unit_id','date','time', 'data_block_id']\n",
    "cat_columns = ['county', 'product_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:23:29,661] A new study created in memory with name: lightgbm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044571 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:23:48,756] Trial 0 finished with value: 73.71418821555197 and parameters: {'n_estimators': 121, 'boosting_type': 'gbdt', 'eta': 0.11957168053633911, 'max_depth': 2, 'min_child_weight': 0.38333321561566636, 'colsample_bytree': 0.2738969595234697, 'subsample': 0.927727492754704, 'lambda': 7.155682161754866, 'alpha': 0.03417952912061012}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022143 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:24:10,508] Trial 1 finished with value: 98.57322394762025 and parameters: {'n_estimators': 179, 'boosting_type': 'dart', 'eta': 0.6769776337422189, 'max_depth': 1, 'min_child_weight': 0.002231090560744304, 'colsample_bytree': 0.10476552591086516, 'subsample': 0.8904582312865974, 'lambda': 1.2960656597279734, 'alpha': 3.020289640158666}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:24:28,659] Trial 2 finished with value: 87.6237244780208 and parameters: {'n_estimators': 242, 'boosting_type': 'gbdt', 'eta': 0.3496801474075936, 'max_depth': 1, 'min_child_weight': 0.3628140404024382, 'colsample_bytree': 0.1391083782005788, 'subsample': 0.9623735634056996, 'lambda': 0.12229065947034369, 'alpha': 0.045566719139214756}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:24:44,656] Trial 3 finished with value: 88.16746303987969 and parameters: {'n_estimators': 76, 'boosting_type': 'gbdt', 'eta': 0.13310833122227744, 'max_depth': 1, 'min_child_weight': 0.29548945587266856, 'colsample_bytree': 0.40935087469661635, 'subsample': 0.7668062477401033, 'lambda': 5.9565160187105715, 'alpha': 0.5336803302756714}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:25:07,245] Trial 4 finished with value: 242.43504428625567 and parameters: {'n_estimators': 89, 'boosting_type': 'dart', 'eta': 0.013155559915552555, 'max_depth': 4, 'min_child_weight': 0.4814503186400563, 'colsample_bytree': 0.1623239345911629, 'subsample': 0.5467397969664458, 'lambda': 0.01826894228153231, 'alpha': 0.028499883436971588}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058899 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:25:27,534] Trial 5 finished with value: 254.6439872225435 and parameters: {'n_estimators': 125, 'boosting_type': 'dart', 'eta': 0.015915358693657216, 'max_depth': 1, 'min_child_weight': 0.004418125737902547, 'colsample_bytree': 0.44989205684622635, 'subsample': 0.5959617329725612, 'lambda': 0.07332348382056167, 'alpha': 0.009499535455183799}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:25:43,275] Trial 6 finished with value: 257.37013051511025 and parameters: {'n_estimators': 64, 'boosting_type': 'dart', 'eta': 0.018762369507155215, 'max_depth': 1, 'min_child_weight': 0.02984699979785086, 'colsample_bytree': 0.662206180587672, 'subsample': 0.5348110858445883, 'lambda': 2.247913678489981, 'alpha': 0.002423224390060893}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:27:18,257] Trial 7 finished with value: 84.5991107564766 and parameters: {'n_estimators': 241, 'boosting_type': 'dart', 'eta': 0.1571148597057821, 'max_depth': 5, 'min_child_weight': 0.0014346671987806323, 'colsample_bytree': 0.19178161105195696, 'subsample': 0.5434414678644279, 'lambda': 0.015295398277813739, 'alpha': 0.002984770033451216}. Best is trial 0 with value: 73.71418821555197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031445 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:27:35,455] Trial 8 finished with value: 72.93784069853903 and parameters: {'n_estimators': 83, 'boosting_type': 'gbdt', 'eta': 0.2341630986802013, 'max_depth': 3, 'min_child_weight': 0.01152279840302872, 'colsample_bytree': 0.3336169083954025, 'subsample': 0.533640664000213, 'lambda': 0.20127321428268347, 'alpha': 5.214165242447227}. Best is trial 8 with value: 72.93784069853903.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:27:51,191] Trial 9 finished with value: 81.06194289451936 and parameters: {'n_estimators': 83, 'boosting_type': 'gbdt', 'eta': 0.2610340855190969, 'max_depth': 1, 'min_child_weight': 0.0054046235343238, 'colsample_bytree': 0.3859339058863831, 'subsample': 0.5070175342790754, 'lambda': 2.0689982192171295, 'alpha': 0.0010441957103804478}. Best is trial 8 with value: 72.93784069853903.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:28:08,768] Trial 10 finished with value: 67.10791507725772 and parameters: {'n_estimators': 51, 'boosting_type': 'gbdt', 'eta': 0.0545470197466688, 'max_depth': 8, 'min_child_weight': 6.267878593618888, 'colsample_bytree': 0.945515609149866, 'subsample': 0.6346624474453691, 'lambda': 0.3341080705306471, 'alpha': 9.48084849128898}. Best is trial 10 with value: 67.10791507725772.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064667 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:28:26,624] Trial 11 finished with value: 77.63159139966186 and parameters: {'n_estimators': 51, 'boosting_type': 'gbdt', 'eta': 0.04441475551169314, 'max_depth': 10, 'min_child_weight': 8.190089323058663, 'colsample_bytree': 0.8267355890798979, 'subsample': 0.6352820489447292, 'lambda': 0.23484188448733628, 'alpha': 7.891776929031949}. Best is trial 10 with value: 67.10791507725772.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035621 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110243 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:28:45,184] Trial 12 finished with value: 63.866171813828466 and parameters: {'n_estimators': 53, 'boosting_type': 'gbdt', 'eta': 0.05840872722237289, 'max_depth': 9, 'min_child_weight': 8.356746083342582, 'colsample_bytree': 0.9436570799603321, 'subsample': 0.6481649533517366, 'lambda': 0.0013616188957842135, 'alpha': 1.2971461679559289}. Best is trial 12 with value: 63.866171813828466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:29:04,176] Trial 13 finished with value: 66.36963611460509 and parameters: {'n_estimators': 50, 'boosting_type': 'gbdt', 'eta': 0.056827751246046346, 'max_depth': 10, 'min_child_weight': 9.47878547995462, 'colsample_bytree': 0.949070608285757, 'subsample': 0.6810372491253096, 'lambda': 0.001549938455049924, 'alpha': 0.9314720204285705}. Best is trial 12 with value: 63.866171813828466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:29:22,787] Trial 14 finished with value: 66.25380578817602 and parameters: {'n_estimators': 60, 'boosting_type': 'gbdt', 'eta': 0.05585884795696196, 'max_depth': 7, 'min_child_weight': 2.502157372304177, 'colsample_bytree': 0.6209612611540423, 'subsample': 0.7224866208392954, 'lambda': 0.0015440363518525759, 'alpha': 0.4188697223880712}. Best is trial 12 with value: 63.866171813828466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:29:42,107] Trial 15 finished with value: 85.44508520966731 and parameters: {'n_estimators': 62, 'boosting_type': 'gbdt', 'eta': 0.03477254734313827, 'max_depth': 6, 'min_child_weight': 2.54274229401728, 'colsample_bytree': 0.6063059818872701, 'subsample': 0.7515718987335854, 'lambda': 0.0011910979048357791, 'alpha': 0.3011586428951019}. Best is trial 12 with value: 63.866171813828466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:30:10,412] Trial 16 finished with value: 58.224452640637615 and parameters: {'n_estimators': 104, 'boosting_type': 'gbdt', 'eta': 0.07809853687079434, 'max_depth': 7, 'min_child_weight': 1.4931356529138706, 'colsample_bytree': 0.6210095923909388, 'subsample': 0.738204036523041, 'lambda': 0.0034798424169360503, 'alpha': 0.20944441627266694}. Best is trial 16 with value: 58.224452640637615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:30:34,659] Trial 17 finished with value: 60.544727458003685 and parameters: {'n_estimators': 106, 'boosting_type': 'gbdt', 'eta': 0.08393905717296767, 'max_depth': 5, 'min_child_weight': 1.4281764499281888, 'colsample_bytree': 0.7373959768498988, 'subsample': 0.8106765540527653, 'lambda': 0.004575862849554225, 'alpha': 0.12256203967901345}. Best is trial 16 with value: 58.224452640637615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:30:58,200] Trial 18 finished with value: 62.91727561699176 and parameters: {'n_estimators': 105, 'boosting_type': 'gbdt', 'eta': 0.09665959649135596, 'max_depth': 5, 'min_child_weight': 0.07062985958192339, 'colsample_bytree': 0.4964142243275759, 'subsample': 0.8147110605200386, 'lambda': 0.005802947889603856, 'alpha': 0.14347812786996073}. Best is trial 16 with value: 58.224452640637615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14082\n",
      "[LightGBM] [Info] Number of data points in the train set: 551778, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 242.074828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042444, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 247.723599\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14544\n",
      "[LightGBM] [Info] Number of data points in the train set: 1537188, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 255.469345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 18:31:21,396] Trial 19 finished with value: 64.91670455839802 and parameters: {'n_estimators': 145, 'boosting_type': 'gbdt', 'eta': 0.08764215980697401, 'max_depth': 3, 'min_child_weight': 1.1809009933574914, 'colsample_bytree': 0.686038316280143, 'subsample': 0.8343042856666202, 'lambda': 0.0059918731204437825, 'alpha': 0.11973116630505895}. Best is trial 16 with value: 58.224452640637615.\n"
     ]
    }
   ],
   "source": [
    "lgbf = LightGBMForecaster()\n",
    "\n",
    "lgbf.train_model(\n",
    "   train_df=train_df,\n",
    "   target_col=\"target\",\n",
    "   model_name=\"lightgbm_enefit\",\n",
    "   exclude_cols=not_feature_columns,\n",
    "   categorical_features=cat_columns\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enefit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
